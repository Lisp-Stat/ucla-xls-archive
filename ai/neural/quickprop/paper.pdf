%PDF-1.3
%‚„œ”
2 0 obj
<<
/Length 2262
>>
stream
BT
/TT2 1 Tf
13 0 0 13 197.83 614.41 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(An Empirical Study of Learning Speed)Tj
1.7092 -2.42 TD
(in Back-Propagation Networks)Tj
10 0 0 10 268.91 550.34 Tm
(Scott E. Fahlman)Tj
0.279 -2.16 TD
(September 1988)Tj
-0.153 -2.16 TD
(CMU-CS-88-162)Tj
1.722 -11.988 TD
(Abstract)Tj
/TT4 1 Tf
-20.539 -2.572 TD
0.2086 Tw
(Most connectionist or "neural network" learning systems use some form of the back-propagation algorithm.)Tj
-1 -1.286 TD
0.1476 Tw
(However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become)Tj
T*
0.0435 Tw
[(larger and more complex.  The factors governing learning speed are poorly understood.  I have begun a systematic,)]TJ
T*
0.0696 Tw
(empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems.)Tj
T*
0.0881 Tw
(The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology)Tj
T*
0 Tw
(that will be of value in future studies of this kind.)Tj
1 -2.388 TD
0.0409 Tw
[(This paper is a progress report describing the results obtained during the first six months of this study.  To date I)]TJ
-1 -1.286 TD
0.0191 Tw
(have looked only at a limited set of benchmark problems, but the results on these are encouraging: I have developed)Tj
T*
0.0266 Tw
(a new learning algorithm that is faster than standard backprop by an order of magnitude or more and that appears to)Tj
T*
0 Tw
(scale up very well as the problem size increases.)Tj
1 -6.164 TD
0.0894 Tw
(This research was sponsored in part by the National Science Foundation under Contract Number EET-8716324)Tj
-1 -1.105 TD
0.004 Tw
(and by the Defense Advanced Research Projects Agency \(DOD\), ARPA Order No. 4976 under Contract F33615-87-)Tj
T*
0.2201 Tw
(C-1499 and monitored by the Avionics Laboratory, Air Force Wright Aeronautical Laboratories, Aeronautical)Tj
T*
0 Tw
(Systems Division \(AFSC\), Wright-Patterson AFB, OH 45433-6543.)Tj
1 -2.207 TD
0.0756 Tw
(The views and conclusions contained in this document are those of the authors and should not be interpreted as)Tj
-1 -1.105 TD
0 Tw
(representing the official policies, either expressed or implied, of these agencies or of the U.S.  Government.)Tj
ET
endstream
endobj
3 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
9 0 obj
<<
/Length 4653
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(1)Tj
/TT2 1 Tf
12 0 0 12 72 711.96 Tm
(1. Introduction)Tj
/TT6 1 Tf
10 0 0 10 82 699.1 Tm
0.0863 Tw
(Note: In this paper I will not attempt to review the basic ideas of connectionism or back-propagation learning.)Tj
-1 -1.286 TD
0.0032 Tw
(See [3] for a brief overview of this area and [10], chapters 1 - 8, for a detailed treatment.  When I refer to "standard)Tj
T*
0 Tw
(back-propagation" in this paper, I mean the back-propagation algorithm with momentum, as described in [9].)Tj
/TT4 1 Tf
1 -2.388 TD
0.0186 Tw
(The greatest single obstacle to the widespread use of connectionist learning networks in real-world applications is)Tj
-1 -1.286 TD
0.0263 Tw
[(the slow speed at which the current algorithms learn.  At present, the fastest learning algorithm for most purposes is)]TJ
T*
0.1953 Tw
[(the algorithm that is generally known as "back-propagation" or "backprop" )195.3([6, )195.3(7, )195.3(9, )195.3(18]. )-444.7(The back-propagation)]TJ
T*
0.0208 Tw
[(learning algorithm runs faster than earlier learning methods, but it is still much slower than we would like.  Even on)]TJ
T*
0.1271 Tw
(relatively simple problems, standard back-propagation often requires the complete set of training examples to be)Tj
T*
0.1061 Tw
[(presented hundreds or thousands of times.  This means that we are limited to investigating rather small networks)]TJ
T*
0.0017 Tw
(with only a few thousand trainable weights.  Some problems of real-world importance can be tackled using networks)Tj
T*
0.0687 Tw
(of this size, but most of the tasks for which connectionist technology might be appropriate are much too large and)Tj
T*
0 Tw
(complex to be handled by our current learning-network technology.)Tj
1 -2.388 TD
0.0162 Tw
(One solution is to run our network simulations on faster computers or to implement the network elements directly)Tj
-1 -1.286 TD
0.0263 Tw
[(in VLSI chips.  A number of groups are working on faster implementations, including a group at CMU that is using)]TJ
T*
0.0186 Tw
[(the 10-processor Warp machine )18.6([13]. )-269.4(This work is important, but even if we had a network implemented directly in)]TJ
T*
0.201 Tw
[(hardware our slow learning algorithms would still limit the range of problems we could attack.  Advances in)]TJ
T*
0.0835 Tw
[(learning algorithms and in implementation technology are complementary. )-333.5(If we can combine hardware that runs)]TJ
T*
0.0307 Tw
(several orders of magnitude faster and learning algorithms that scale up well to very large networks, we will be in a)Tj
T*
0 Tw
(position to tackle a much larger universe of possible applications.)Tj
1 -2.388 TD
0.0133 Tw
[(Since January of 1988 I have been conducting an empirical study of learning speed in simulated networks.  I have)]TJ
-1 -1.286 TD
0.0168 Tw
(studied the standard backprop algorithm and a number of variations on standard back-propagation, applying these to)Tj
T*
0.0156 Tw
[(a set of moderate-sized benchmark problems.  Many of the variations that I have investigated were first proposed by)]TJ
T*
0.0098 Tw
(other researchers, but until now there have been no systematic studies to compare these methods, individually and in)Tj
T*
0.1163 Tw
[(various combinations, against a standard set of learning problems.  Only through such systematic studies can we)]TJ
T*
0 Tw
(hope to understand which methods work best in which situations.)Tj
1 -2.388 TD
0.1016 Tw
[(This paper is a report on the results obtained in the first six months of this study.  Perhaps the most important)]TJ
-1 -1.286 TD
0.0583 Tw
(result is the identification of a new learning method -- actually a combination of several ideas -- that on a range of)Tj
T*
0.1114 Tw
[(encoder/decoder problems is faster than standard back-propagation by an order of magnitude or more.  This new)]TJ
T*
0.0411 Tw
(method also appears to scale up much better than standard backprop as the size and complexity of the learning task)Tj
T*
0 Tw
(grows.)Tj
1 -2.388 TD
0.0344 Tw
[(I must emphasize that this is a progress report.  The learning-speed study is far from complete.  Until now I have)]TJ
-1 -1.286 TD
0.0852 Tw
[(concentrated most of my effort on a single class of benchmarks, namely the encoder/decoder problems.  Like any)]TJ
T*
0.1787 Tw
(family of benchmarks taken in isolation, encoder/decoder problems have certain peculiarities that may bias the)Tj
T*
0.0001 Tw
[(results of the study. )-249.9(Until a more comprehensive set of benchmarks has been run, it would be premature to draw any)]TJ
T*
0 Tw
(sweeping conclusions or make any strong claims about the widespread applicability of these techniques.)Tj
ET
endstream
endobj
10 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
13 0 obj
<<
/Length 5165
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(2)Tj
/TT2 1 Tf
12 0 0 12 72 711.96 Tm
(2. Methodology)Tj
11 0 0 11 72 675.79 Tm
(2.1. What Makes a Good Benchmark?)Tj
/TT4 1 Tf
10 0 0 10 82 662.93 Tm
0.3112 Tw
(At present there is no widely accepted methodology for measuring and comparing the speed of various)Tj
-1 -1.286 TD
0.1549 Tw
[(connectionist learning algorithms.  Some researchers have proposed new algorithms based only on a theoretical)]TJ
T*
0.1127 Tw
[(analysis of the problem.  It is sometimes hard to determine how well these theoretical models fit actual practice.)]TJ
T*
0.0884 Tw
(Other researchers implement their ideas and run one or two benchmarks to demonstrate the speed of the resulting)Tj
T*
0.2024 Tw
[(system. )-451.6(Unfortunately, no two researchers ever seem to choose the same benchmark or, if they do, they use)]TJ
T*
0.028 Tw
[(different parameters or adopt different criteria for success.  This makes it very hard to determine which algorithm is)]TJ
T*
0 Tw
(best for a given application.)Tj
1 -2.388 TD
0.372 Tw
(The measurement problem is compounded by widespread confusion about the speed of standard back-)Tj
-1 -1.286 TD
0.2926 Tw
[(propagation. )-543.4(Selection of the back-propagation learning parameters is something of a black art, and small)]TJ
T*
0.066 Tw
[(differences in these parameters can lead to large differences in learning times.  It is not uncommon to see learning)]TJ
T*
0.2311 Tw
(times reported in the literature that differ by an order of magnitude or more on the same problem and with)Tj
T*
0 Tw
(essentially the same learning method.)Tj
1 -2.388 TD
0.2326 Tw
(The net effect of all this confusion is that we are faced with a vast, uncharted space of possible learning)Tj
-1 -1.286 TD
0.0611 Tw
(algorithms in which only a few isolated points have been explored, and even for those points it is hard to compare)Tj
T*
0.0569 Tw
[(the claims of the various explorers.  What we need now is a careful, systematic effort to fill in the rest of the map.)]TJ
T*
0.0218 Tw
(The primary goal of this study is to develop faster learning algorithms for connectionist networks, but I also hope to)Tj
T*
0 Tw
(contribute to the development of a new, more coherent methodology for studies of this kind.)Tj
1 -2.388 TD
0.0587 Tw
(One good way to begin is to select a family of benchmark problems that learning-speed researchers can use in a)Tj
-1 -1.286 TD
0.054 Tw
[(standard way to evaluate their algorithms.  We should try to choose benchmarks that will give us good insight into)]TJ
T*
0 Tw
(how various learning algorithms will perform on the real-world tasks we eventually want to tackle.)Tj
1 -2.388 TD
0.1859 Tw
(At present, the benchmark that appears most often in the literature is the exclusive-or problem, often called)Tj
-1 -1.286 TD
0.0133 Tw
("XOR" : we are given a network with two input units, one or two hidden units, and one output unit, and the problem)Tj
T*
0.0553 Tw
(is to train the weights in this network so that the output unit will turn on if one or the other of the inputs is on, but)Tj
T*
0.0851 Tw
[(not both.  Some researchers, notably Tesauro and Janssens )85.1([16], generalize this to the N-input parity problem: the)]TJ
T*
0 Tw
(output is to be on if an odd number of inputs are on.)Tj
1 -2.388 TD
0.0243 Tw
[(The XOR/parity problem looms large in the history and theory of connectionist models \(see )24.3([11] for an important)]TJ
-1 -1.286 TD
0.1389 Tw
(piece of this history\), but if our goal is to develop good learning algorithms for real-world pattern-classification)Tj
T*
0.2274 Tw
[(tasks, XOR/parity is the wrong problem to concentrate on. )-478.6(Classification tasks take advantage of a learning)]TJ
T*
0.0585 Tw
(networkís ability to generalize from the input patterns it has seen during training to nearby patterns in the space of)Tj
T*
0.059 Tw
[(possible inputs.  Such networks must occasionally make sharp distinctions between very similar input patterns, but)]TJ
T*
0.0779 Tw
[(this is the exception rather than the rule.  XOR and parity, on the other hand, have exactly the opposite character:)]TJ
T*
0.0105 Tw
(generalization is actually punished, since the nearest neighbors of an input pattern must produce the opposite answer)Tj
T*
0 Tw
(from the pattern itself.)Tj
1 -2.388 TD
0.2179 Tw
(Other popular benchmark problems, such as the "Penzias" or cluster-counting task have some of this same)Tj
-1 -1.286 TD
0.0002 Tw
(anti-generalizing quality.  Here again, the change of any one bit will almost always make a big change in the answer.)Tj
T*
0.0371 Tw
(As part of a suite of benchmarks, such tasks are valuable, but if used in isolation they may encourage us to develop)Tj
T*
0 Tw
(learning algorithms that do not generalize well.)Tj
1 -2.388 TD
0.0782 Tw
(In my view, we would do better to concentrate on what I will call "noisy associative memory" benchmarks: we)Tj
-1 -1.286 TD
0.0988 Tw
(select a set of input patterns, perhaps binary strings chosen at random, and expand each of these into a cluster of)Tj
T*
0.104 Tw
(similar inputs by adding random noise or flipping some of the input bits; we then train the network to produce a)Tj
ET
endstream
endobj
14 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
16 0 obj
<<
/Length 5661
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(3)Tj
-23.15 -3.6 TD
0.1328 Tw
[(different output pattern for each of these input clusters.  We might occasionally ask the network to map several)]TJ
0 -1.286 TD
0.0092 Tw
[(different clusters into the same output.  Some of the noise-modified input patterns are not used during training; these)]TJ
T*
0.2671 Tw
(can be used later to determine whether the network is capturing the "central idea" of the cluster or is just)Tj
T*
0.014 Tw
[(memorizing the specific input patterns it has seen.  I believe that performance of a learning algorithm on this type of)]TJ
T*
0 Tw
(problem will correlate very well with performance on real-world pattern classification tasks.)Tj
1 -2.388 TD
0.1715 Tw
[(This family of benchmarks can be scaled in various ways. )-421.5(We can vary the number of pattern-clusters, the)]TJ
-1 -1.286 TD
0.1653 Tw
[(number of input and output units, and the amount of noise added to the inputs. )-415.7(We can treat this as a digital)]TJ
T*
0.1076 Tw
[(problem, in which the inputs are either 0 or 1, or we can use analog input patterns.  We can vary the number of)]TJ
T*
0.0291 Tw
[(hidden units and the number of layers.  Obviously, it will take some time before we have accumulated a solid set of)]TJ
T*
0 Tw
(baseline results against which new algorithms can be measured.)Tj
/TT2 1 Tf
11 0 0 11 72 550.21 Tm
(2.2. The Encoder/Decoder Task)Tj
/TT4 1 Tf
10 0 0 10 82 537.35 Tm
0.0289 Tw
(In the coming months I intend to concentrate on "noisy associative memory" benchmarks, plus some benchmarks)Tj
-1 -1.286 TD
0.089 Tw
[(adapted from real-world applications in domains such as speech understanding and road following. )-340(However, for)]TJ
T*
0.0775 Tw
[(the early stages of the study it seemed wise to stick with encoder/decoder problems.  This family of problems has)]TJ
T*
0.0897 Tw
(been popular in the connectionist community for some years, so we have a base of shared experience in applying)Tj
T*
0 Tw
(older learning algorithms to them.)Tj
1 -2.388 TD
0.1291 Tw
(The encoder/decoder problems, often called simply "encoders", will be familiar to most readers of this report.)Tj
-1 -1.286 TD
0.1067 Tw
(When I speak of an "N-M-N encoder", I mean a network with three layers of units and two layers of modifiable)Tj
T*
0.015 Tw
[(weights. )-266(There are N units in the input layer, M hidden units, and N units in the output layer.  There is a connection)]TJ
T*
0.0026 Tw
(from every input unit to every hidden unit and a connection from every hidden unit to every output unit.  In addition,)Tj
T*
0.1607 Tw
[(each unit has a modifiable threshold.  There are no direct connections from the input units to the output units.)]TJ
T*
0 Tw
(Normally, M is smaller than than N, so the network has a bottleneck through which information must flow.)Tj
1 -2.388 TD
0.0527 Tw
(This network is presented with N distinct input patterns, each of which has only one of the input units turned on)Tj
-1 -1.286 TD
0.0855 Tw
[(\(set to 1.0\); the other input bits are turned off \(set to 0.0\).  The task is to duplicate the input pattern in the output)]TJ
T*
0.0848 Tw
[(units. )-333.2(Since all information must flow through the hidden units, the network must develop a unique encoding for)]TJ
T*
0.1042 Tw
(each of the N patterns in the M hidden units and a set of connection weights that, working together, perform the)Tj
T*
0 Tw
(encoding and decoding operations.)Tj
1 -2.388 TD
0.038 Tw
(If an encoder network has)Tj
/TT6 1 Tf
10.825 0 TD
0 Tw
(M)Tj
/TT4 1 Tf
0.983 0 TD
[(=)-150(log)]TJ
/TT6 1 Tf
2.542 0 TD
(N)Tj
/TT4 1 Tf
0.667 0 TD
0.0381 Tw
[(, I will speak of it as a "tight" encoder.  For example, the 8-3-8 and 16-4-16)]TJ
8 0 0 8 220 295.08 Tm
0 Tw
(2)Tj
10 0 0 10 72 285.67 Tm
0.0504 Tw
[(encoders are tight in this sense.  If the hidden units assume only two values, 1 and 0, then the network must assign)]TJ
0 -1.286 TD
0.0127 Tw
[(every input pattern to one of the N possible binary codes in the hidden layer, wasting none of the codes.  In practice,)]TJ
T*
0.169 Tw
(a backprop network will often use three or more distinct analog values in some of the hidden units, so "tight")Tj
T*
0.0002 Tw
(encoder networks are not really forced to search for optimal binary encodings.  It is even possible to learn to perform)Tj
T*
0.055 Tw
(the encoder/decoder task in an "ultra-tight" network such as 8-2-8, though this takes much longer than learning the)Tj
T*
0 Tw
(same task in an 8-3-8 network.)Tj
1 -2.388 TD
0.0899 Tw
(In a real-world pattern classification task, we will usually give the network enough hidden units to perform the)Tj
-1 -1.286 TD
0.0647 Tw
[(task easily.  We do not want to provide too many hidden units -- that allows the network to memorize the training)]TJ
T*
0.044 Tw
(examples rather than extracting the general features that will allow it to handle cases it has not seen during training)Tj
T*
0.0556 Tw
(-- but neither do we want to force the network to spend a lot of extra time trying to find an optimal representation.)Tj
T*
0.1417 Tw
(This suggests that relatively "loose" encoder problems, such as 10-5-10 or 64-8-64, might be more realistic and)Tj
T*
0.0125 Tw
[(informative benchmarks than tight or ultra-tight encoders.  Much of the work reported here was done on the 10-5-10)]TJ
T*
0.0491 Tw
[(encoder, which is not too tight and not too small.  This made it possible to run a large number of experiments with)]TJ
T*
0 Tw
(this same problem, including some using very slow learning algorithms.)Tj
1 -2.388 TD
0.0948 Tw
(The standard encoder problem has one unusual feature that is not seen in typical pattern classification tasks: in)Tj
ET
endstream
endobj
17 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
19 0 obj
<<
/Length 5809
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(4)Tj
-23.15 -3.6 TD
0.1206 Tw
(each learning example, only one of the connections on the input side carries a non-zero activation for any given)Tj
0 -1.286 TD
0.0676 Tw
[(training pattern.  In the standard back-propagation algorithm, only this one input-side weight is modified by errors)]TJ
T*
0.0346 Tw
[(due to that pattern.  This separation of effects makes the standard encoder/decoder somewhat easier than the typical)]TJ
T*
0.0167 Tw
[(pattern-classification task, and therefore perhaps unrealistic as a benchmark.  In an effort to understand what kind of)]TJ
T*
0.106 Tw
(bias this may be introducing, I have also looked at)Tj
/TT6 1 Tf
21.419 0 TD
0.107 Tw
(complement encoder)Tj
/TT4 1 Tf
8.767 0 TD
(problems, in which the input and output)Tj
-30.186 -1.286 TD
0.0571 Tw
[(patterns are all ones except for a single zero in some position.  As we will see, complement encoders require more)]TJ
T*
0 Tw
(learning time than standard encoders, but the difference can be minimized if the right learning techniques are used.)Tj
1 -2.388 TD
0.0499 Tw
(In all of the encoder and complement encoder problems, learning times are reported in)Tj
/TT6 1 Tf
35.552 0 TD
0 Tw
(epochs)Tj
/TT4 1 Tf
2.7769 0 TD
0.05 Tw
[(. )-300(An epoch is one)]TJ
-39.3289 -1.286 TD
0.1336 Tw
[(presentation of the entire set of N training patterns.  All of the algorithms that I have studied to date perform a)]TJ
T*
0.0781 Tw
(forward pass and a backward pass on each pattern, collecting error data; the weights are updated at the end of the)Tj
T*
0 Tw
(epoch, after the entire set of N patterns has been seen.)Tj
/TT2 1 Tf
11 0 0 11 72 537.35 Tm
(2.3. When is the Learning Complete?)Tj
/TT4 1 Tf
10 0 0 10 82 524.49 Tm
0.0605 Tw
(One source of confusion in the learning-speed studies done to date is that each researcher has chosen a different)Tj
-1 -1.286 TD
0.1711 Tw
[(criterion for "successful" learning of a task.  In complex pattern-classification tasks, this is a hard problem for)]TJ
T*
0.0066 Tw
[(several reasons.  First, if the inputs are noisy, it may be impossible to perform a perfect classification.  Second, if the)]TJ
T*
0.0001 Tw
[(outputs are analog in nature, the accuracy required will depend on the demands of the specific task. )-250.9(Finally, in many)]TJ
T*
0.049 Tw
(tasks one can manipulate the learning parameters to trade off speed and accuracy against generalization; again, any)Tj
T*
0.0435 Tw
[(reasonable criterion for success will depend on the demands of the particular problem.  I believe that researchers in)]TJ
T*
0.0522 Tw
(this field need to discuss these issues and arrive at some consensus about how various benchmark problems should)Tj
T*
0 Tw
(be run and evaluated.)Tj
1 -2.388 TD
0.1927 Tw
(On something like an encoder/decoder problem there should be little difficulty in agreeing upon criteria for)Tj
-1 -1.286 TD
0.0086 Tw
[(success, but no such agreement exists at present.  Some researchers accept any output over 0.50000 as a one and any)]TJ
T*
0.0475 Tw
[(output below that threshold as a zero.  However, if we were to apply this criterion to real analog hardware devices,)]TJ
T*
0.0056 Tw
(the slightest bit of additional noise could drive some bits to the other side of the threshold.  Other researchers require)Tj
T*
0.0125 Tw
[(that each output be very close to the specified target value.  For networks performing a task with binary outputs, this)]TJ
T*
0.0758 Tw
(seems unnecessarily strict; some learning algorithms may produce useful outputs quickly, but take much longer to)Tj
T*
0.0618 Tw
[(adjust the output values to within the specified tolerances.  Still other researchers declare success when the sum of)]TJ
T*
0.135 Tw
[(the squared error for all the outputs falls below some fixed value.  This seems an odd choice, since in a binary)]TJ
T*
0.0793 Tw
(application we want each of the individual outputs to be correct; we donít want to trade more error on one output)Tj
T*
0 Tw
(against less error on another.)Tj
1 -2.388 TD
0.0478 Tw
(Another possibility is to consider a network to have learned the problem when, for each input pattern, the output)Tj
-1 -1.286 TD
0.0048 Tw
(of the "correct" unit is larger than any other output.  This criterion is sometimes met much earlier in the training than)Tj
T*
0.0156 Tw
[(any criterion based upon a fixed threshold.  However, if we were to use this criterion for training an actual hardware)]TJ
T*
0.1327 Tw
(network, we would need some additional circuitry to select the largest output; it seems better to let the learning)Tj
T*
0.0304 Tw
[(network itself do this job, if possible.  In addition, this criterion is not applicable in problems with multiple one-bits)]TJ
T*
0 Tw
(in the output pattern.)Tj
1 -2.388 TD
0.0626 Tw
(I suggest that for problems with binary outputs, we adopt a "threshold and margin" criterion similar to that used)Tj
-1 -1.286 TD
0.0057 Tw
(by digital logic designers: if the total range of the output units is 0.0 to 1.0, any value below 0.4 is considered to be a)Tj
T*
0.0249 Tw
(zero and any value above 0.6 is considered to be a one; values between 0.4 and 0.6 are considered to be "marginal",)Tj
T*
0.1844 Tw
[(and are not counted as correct during training. )-434.6(If the output range is different, we scale these two thresholds)]TJ
T*
0.0947 Tw
[(appropriately. )-343.3(By creating a "no manís land" between the two classes of outputs, we produce a network that can)]TJ
T*
0 Tw
(tolerate a small amount of noise in the outputs.)Tj
1 -2.388 TD
0.0555 Tw
[(All of the examples in this paper use this 0.4 - 0.6 criterion to measure success.  Training continues until, for an)]TJ
-1 -1.286 TD
0.113 Tw
(entire epoch, we observe that every output is correct; then we stop and declare learning to have been successful.)Tj
ET
endstream
endobj
20 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
22 0 obj
<<
/Length 5510
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(5)Tj
-23.15 -3.6 TD
0.0148 Tw
(Since back-propagation networks are deterministic, we will get the same successful results in all future trials as long)Tj
0 -1.286 TD
0 Tw
(as we do not change the weights.)Tj
/TT2 1 Tf
11 0 0 11 72 664.11 Tm
(2.4. How Should We Report Learning Times?)Tj
/TT4 1 Tf
10 0 0 10 82 651.25 Tm
(The)Tj
/TT6 1 Tf
1.855 0 TD
(epoch)Tj
/TT4 1 Tf
2.3877 0 TD
0.0504 Tw
(, defined as a single presentation of each of the I/O patterns in the training set, is a convenient unit by)Tj
-5.2427 -1.286 TD
0.0465 Tw
[(which to measure learning time for the benchmarks reported here.  An alternative is to use the)]TJ
/TT6 1 Tf
38.615 0 TD
0.046 Tw
(pattern presentation)Tj
/TT4 1 Tf
-38.615 -1.286 TD
0.157 Tw
[(as the basic unit of learning time.  This is the presentation of a single I/O pattern, with the associated forward)]TJ
T*
0.0002 Tw
(propagation of results and back-propagation of error, and \(sometimes\) the modification of weights.  The presentation)Tj
T*
0.0616 Tw
(is a more natural measure to use in problems that do not have a finite training set, or that do not cycle through the)Tj
T*
0.132 Tw
[(entire training set between weight-updates.  As long as it is made clear what units are being reported, it is easy)]TJ
T*
0.0574 Tw
(enough to convert from epochs to presentations, so researchers can choose whatever units they like without fear of)Tj
T*
0 Tw
(confusion.)Tj
1 -2.388 TD
0.1619 Tw
(In measuring a new learning algorithm against a particular benchmark problem, it is desirable to run a large)Tj
-1 -1.286 TD
0.1711 Tw
[(number of trials with the weights initialized to different random values in each case.  Any single trial may be)]TJ
T*
0.1211 Tw
(misleading: the choice of initial weights might have a greater influence on the time required than any difference)Tj
T*
0.0252 Tw
[(between two algorithms.  Most of the results reported in this paper are averages over 25 or 100 trials; for some very)]TJ
T*
0 Tw
(large or very slow problems, I have been forced to use fewer trials.)Tj
1 -2.388 TD
0.0085 Tw
[(Reporting of learning times is a simple enough matter when all of the trials succeed.  That is the case for all of the)]TJ
-1 -1.286 TD
0.0328 Tw
[(encoder examples I have run. )-282.2(In this case, it is useful to report the average learning time over all the trials, the best)]TJ
T*
0.0989 Tw
[(and worst results, and the standard deviation or some other measure of the variation in results. )-347.1(Unfortunately, in)]TJ
T*
0.0277 Tw
(some problems such as XOR, the network will occasionally become stuck in a local minimum from which it cannot)Tj
T*
0.1751 Tw
[(escape. )-426.9(These learning trials never converge, so the learning time is infinite.  A few other trials may take an)]TJ
T*
0 Tw
(anomalously long time; mixing these long trials into an average may give a distorted picture of the data.)Tj
1 -2.388 TD
0.0716 Tw
[(How should such results be reported?  One option, used by Robert Jacobs )71.6([5], is simply to report the failures in)]TJ
-1 -1.286 TD
0.1071 Tw
[(one column and the average of the successful trials in another.  The problem with this is that it becomes hard to)]TJ
T*
0.0032 Tw
[(choose between a learning method with fewer failures and one with a better average. )-253.8(In addition, it becomes unclear)]TJ
T*
0 Tw
(whether the average has been polluted by a few very long near-failures.)Tj
1 -2.388 TD
0.0292 Tw
[(A second option is adopted by Tesauro and Janssens )29.2([16]. )-278.8(Instead of averaging the times for N trials in the usual)]TJ
-1 -1.286 TD
0.017 Tw
[(way, they define the "average" training time to be the inverse of the average training rate.  The training rate for each)]TJ
T*
0.0227 Tw
[(trial is defined as the inverse of the time required for that trial.  This method gives us a single, well defined number,)]TJ
T*
0.0947 Tw
(even if the set of trials includes some trials that are very long or infinite; the learning rate for these trials goes to)Tj
T*
0.0474 Tw
[(zero. )-298.6(However, this kind of average emphasizes short trials and de-emphasizes long ones, so results computed this)]TJ
T*
0.0891 Tw
[(way look much faster than if a conventional average were used.  Note also that if two algorithms are equally fast)]TJ
T*
0.2295 Tw
(when measured by a conventional average, the training-rate average will rate the more consistent of the two)Tj
T*
0.038 Tw
[(algorithms as slower.  Algorithms that take risky, unsound steps to get very short convergence times on a few trials)]TJ
T*
0 Tw
(are favored, even if these algorithms do very poorly by other measures.)Tj
1 -2.388 TD
0.0708 Tw
(The option I favor, for lack of a better idea, is to allow the learning program to restart a trial, with new random)Tj
-1 -1.286 TD
0.053 Tw
[(weights, whenever the network has failed to converge after a certain number of epochs.  The time reported for that)]TJ
T*
0.1315 Tw
[(trial must include the time spent before the restart and after it. )-380.5(We can consider these restarts to be part of the)]TJ
T*
0.0869 Tw
(learning algorithm; the restart threshold is just another parameter that the experimenter can adjust for best results.)Tj
T*
0.0957 Tw
(This seems realistic: when faced with a method that usually converges in 20 epochs or less, but that occasionally)Tj
T*
0.0618 Tw
[(gets stuck, it seems only natural to give up and start over at some point.  The XOR results reported below use this)]TJ
T*
0 Tw
(method of reporting.)Tj
ET
endstream
endobj
23 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
25 0 obj
<<
/Length 3009
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(6)Tj
/TT2 1 Tf
11 0 0 11 72 712.63 Tm
(2.5. Implementation)Tj
/TT4 1 Tf
10 0 0 10 82 699.77 Tm
0.0819 Tw
(All of the experiments reported here were run on a back-propagation simulator that I developed specifically for)Tj
-1 -1.286 TD
0.0331 Tw
[(this purpose.  The simulator was written in CMU Common Lisp and runs on the IBM RT PC workstation under the)]TJ
T*
0.1836 Tw
[(Mach operating system and the X10.4 window system.  The machines that were used in this study have been)]TJ
T*
0.054 Tw
(provided by IBM as part of a joint research agreement with the Computer Science Department of Carnegie-Mellon)Tj
T*
0 Tw
(University.)Tj
1 -2.388 TD
0.2061 Tw
[(The simulator will soon be converted to use the X11 window system via the CLX interface.  It should be)]TJ
-1 -1.286 TD
0.0582 Tw
(relatively easy to port this code to any other implementation of Common Lisp, though I have not taken the time to)Tj
T*
0 Tw
(make the code 100% portable.)Tj
1 -2.388 TD
0.2068 Tw
[(Because it is coded in Common Lisp, the simulator is very flexible.  It is easy to try out several program)]TJ
-1 -1.286 TD
0.125 Tw
[(variations in a few hours.  With the displays turned off, the simulator runs the back-propagation algorithm for a)]TJ
T*
0.1836 Tw
(10-5-10 encoder at roughly 3 epochs per second, a processing rate of about 3500 connection-presentations per)Tj
T*
0.0197 Tw
[(second. )-268.3(This is fast enough for experimentation on small benchmarks, especially with the new algorithms that learn)]TJ
T*
0.0473 Tw
[(such tasks in relatively few epochs.  For larger problems and real applications, we will need a faster simulator.  By)]TJ
T*
0.062 Tw
(hand-coding the inner loops in assembler, it should be possible to speed up my simulator considerably, perhaps by)Tj
T*
0.1412 Tw
[(as much as a factor of ten. )-390.8(Beyond that point, we will have to move the inner loops to a faster machine.  For)]TJ
T*
0.2205 Tw
(comparison, the Warp machine is now running standard back-propagation at a rate of 17 million connection-)Tj
T*
0.1682 Tw
[(presentations per second )168.2([13], almost 5000 times faster than my simulator, but it is a very difficult machine to)]TJ
T*
0 Tw
(program.)Tj
1 -2.388 TD
0.0605 Tw
[(My simulator is designed to make it easy for the experimenter to see what is going on inside of the network.  A)]TJ
-1 -1.286 TD
0.0013 Tw
(set of windows displays the changing values of the unit outputs and other per-unit statistics.  Another set of windows)Tj
T*
0.0147 Tw
[(displays the weights, weight changes, and other per-weight statistics.  A control panel is provided through which the)]TJ
T*
0.0119 Tw
(operator can alter the learning parameters in real time or single-step the processing to see more clearly what is going)Tj
T*
0.0093 Tw
[(on. )-258.7(These displays have been of immense value in helping me to understand what problems were developing during)]TJ
T*
0 Tw
(the learning procedure and what might be done about them.)Tj
ET
endstream
endobj
26 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
28 0 obj
<<
/Length 8634
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(7)Tj
/TT2 1 Tf
12 0 0 12 72 711.96 Tm
(3. Experiments and Results)Tj
11 0 0 11 72 675.79 Tm
(3.1. Tuning of Backprop Learning Parameters)Tj
/TT4 1 Tf
10 0 0 10 82 662.93 Tm
0.048 Tw
(The first task undertaken in this study was to understand how the learning parameters affected learning time in a)Tj
-1 -1.286 TD
0.044 Tw
[(relatively small benchmark, the 10-5-10 encoder.  There are three parameters of interest:)]TJ
/TT7 1 Tf
36.257 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.439 0 TD
0.045 Tw
(, the learning rate;)Tj
/TT7 1 Tf
7.706 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
0.045 Tw
(, the)Tj
-45.0329 -1.286 TD
0 Tw
(momentum factor; and)Tj
/TT6 1 Tf
9.36 0 TD
(r)Tj
/TT4 1 Tf
0.3892 0 TD
(, the range of the random initial weights.)Tj
-8.7492 -2.388 TD
0.1139 Tw
(At the start of each learning trial, each of the weights and thresholds in the network is initialized to a random)Tj
-1 -1.286 TD
0 Tw
(value chosen from the range -)Tj
/TT6 1 Tf
11.9116 0 TD
(r)Tj
/TT4 1 Tf
0.6404 0 TD
(to +)Tj
/TT6 1 Tf
1.5918 0 TD
(r)Tj
/TT4 1 Tf
0.3892 0 TD
(.)Tj
-13.533 -2.388 TD
0.0706 Tw
(The formula for updating the weights is)Tj
/TT7 1 Tf
16.632 0 TD
0 Tw
<0036>Tj
/TT6 1 Tf
0.6118 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
0.15 Tc
(\)=)Tj
/TT7 1 Tf
1.1974 0 TD
0 Tc
[<003c00a1>-150<002c>]TJ
/TT6 1 Tf
1.6319 0 TD
(E)Tj
/TT4 1 Tf
0.7611 0 TD
(/)Tj
/TT7 1 Tf
0.428 0 TD
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
0.15 Tc
(\)+)Tj
/TT7 1 Tf
1.197 0 TD
<005f0036>Tj
/TT6 1 Tf
1.3927 0 TD
0 Tc
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT7 1 Tf
0.2778 0 TD
<003c>Tj
/TT4 1 Tf
0.5488 0 TD
0.071 Tw
(1\), where)Tj
/TT7 1 Tf
4.1687 0 TD
0 Tw
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(E)Tj
/TT4 1 Tf
0.7609 0 TD
(/)Tj
/TT7 1 Tf
0.428 0 TD
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
0.071 Tw
(\) is the error derivative)Tj
-37.352 -1.286 TD
0 Tw
(for that weight, accumulated over the whole epoch.  Sometimes I refer to this derivative the "slope" of that weight.)Tj
1 -2.388 TD
0.0915 Tw
(Because standard back-propagation learning is rather slow, I did not exhaustively scan the 3-dimensional space)Tj
-1 -1.286 TD
0.048 Tw
(defined by)Tj
/TT6 1 Tf
4.595 0 TD
0 Tw
(r)Tj
/TT4 1 Tf
0.3892 0 TD
(,)Tj
/TT7 1 Tf
0.5478 0 TD
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
0.048 Tw
(, and)Tj
/TT7 1 Tf
2.2901 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.439 0 TD
0.0481 Tw
[(. )-297.9(A cursory exploration was done, with only a few trials at each point tested, in order to find)]TJ
-8.892 -1.286 TD
0.021 Tw
[(the regions that were most promising.  This suggested that an)]TJ
/TT6 1 Tf
25.007 0 TD
0 Tw
(r)Tj
/TT4 1 Tf
0.66 0 TD
0.021 Tw
(value of 1.0, an)Tj
/TT7 1 Tf
6.527 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.902 0 TD
0.0206 Tw
(value between 0.0 and 0.5, and an)Tj
/TT7 1 Tf
-33.096 -1.286 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.709 0 TD
0.0202 Tw
[(value somewhere around 1.0 would produce the fastest learning for this problem. )-271.8(Many researchers have reported)]TJ
-0.709 -1.286 TD
0.048 Tw
(good results at)Tj
/TT7 1 Tf
6.227 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.929 0 TD
0.0471 Tw
(values of 0.8 or 0.9, but on this problem I found that such a high)Tj
/TT7 1 Tf
26.705 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.928 0 TD
0.047 Tw
(led to very poor results, often)Tj
-34.789 -1.286 TD
0 Tw
(requiring a thousand epochs or more.)Tj
1 -2.388 TD
0.0308 Tw
[(The promising area was then explored more intensively, with 25 trials at each point tested.  Surprisingly, the best)]TJ
-1 -1.286 TD
0 Tw
(result was obtained with)Tj
/TT7 1 Tf
9.999 0 TD
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
(=0.0, or no momentum at all:)Tj
-6.5169 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
90.09 412.54 m
521.91 412.54 l
170.07 412.54 m
170.07 429.57 l
214.05 412.54 m
214.05 429.57 l
258.03 412.54 m
258.03 429.57 l
302.01 412.54 m
302.01 429.57 l
345.99 412.54 m
345.99 429.57 l
389.97 412.54 m
389.97 429.57 l
433.95 412.54 m
433.95 429.57 l
477.93 412.54 m
477.93 429.57 l
S
BT
10 0 0 10 114.25 401.69 Tm
[(10-5-10)-4115(25)-3273(1.7)-3148(0.0)-3148(1.0)-3023(265)-3148(80)-3148(129)-3148(46)]TJ
ET
90.09 395.51 431.82 34.06 re
170.07 395.51 m
170.07 412.54 l
214.05 395.51 m
214.05 412.54 l
258.03 395.51 m
258.03 412.54 l
302.01 395.51 m
302.01 412.54 l
345.99 395.51 m
345.99 412.54 l
389.97 395.51 m
389.97 412.54 l
433.95 395.51 m
433.95 412.54 l
477.93 395.51 m
477.93 412.54 l
S
BT
10 0 0 10 82 371.63 Tm
0.1487 Tw
(This notation says that, with 25 trials and the parameters specified, the longest trial required 265 epochs, the)Tj
-1 -1.286 TD
0.053 Tw
[(shortest required 80 epochs, the average over all the runs was 129 epochs, and the standard deviation was 46.  The)]TJ
T*
0.0098 Tw
(standard deviations are included only to give the reader a crude idea of the variation in the values; the distribution of)Tj
T*
0 Tw
(learning times seldom looks like a normal distribution, often exhibiting multiple humps, for example.)Tj
1 -2.388 TD
(The same average learning time was obtained for an)Tj
/TT7 1 Tf
21.133 0 TD
<005f>Tj
/TT4 1 Tf
0.881 0 TD
(of 0.5 and a smaller)Tj
/TT7 1 Tf
8.165 0 TD
<00a1>Tj
/TT4 1 Tf
0.689 0 TD
(value:)Tj
-27.755 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
90.09 282.9 m
521.91 282.9 l
170.07 282.9 m
170.07 299.93 l
214.05 282.9 m
214.05 299.93 l
258.03 282.9 m
258.03 299.93 l
302.01 282.9 m
302.01 299.93 l
345.99 282.9 m
345.99 299.93 l
389.97 282.9 m
389.97 299.93 l
433.95 282.9 m
433.95 299.93 l
477.93 282.9 m
477.93 299.93 l
S
BT
10 0 0 10 114.25 272.05 Tm
[(10-5-10)-4115(25)-3273(1.1)-3148(0.5)-3148(1.0)-3023(275)-3148(85)-3148(129)-3148(40)]TJ
ET
90.09 265.87 431.82 34.06 re
170.07 265.87 m
170.07 282.9 l
214.05 265.87 m
214.05 282.9 l
258.03 265.87 m
258.03 282.9 l
302.01 265.87 m
302.01 282.9 l
345.99 265.87 m
345.99 282.9 l
389.97 265.87 m
389.97 282.9 l
433.95 265.87 m
433.95 282.9 l
477.93 265.87 m
477.93 282.9 l
S
BT
10 0 0 10 82 241.99 Tm
0.065 Tw
(If we hold)Tj
/TT7 1 Tf
4.555 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.946 0 TD
0.065 Tw
(at 0.0 and vary)Tj
/TT7 1 Tf
6.453 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.439 0 TD
0.0656 Tw
(, we see a U-shaped curve, rising to an average learning time of 242 at)Tj
/TT7 1 Tf
29.394 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.439 0 TD
0.066 Tw
(=0.5 and)Tj
-43.226 -1.286 TD
0.092 Tw
(rising more steeply to a value of 241 at)Tj
/TT7 1 Tf
16.687 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.439 0 TD
0.092 Tw
[(=1.3. )-342(If we increase)]TJ
/TT7 1 Tf
8.882 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.973 0 TD
0.0917 Tw
(up to 0.5 or so, we see that we are actually in a)Tj
-26.981 -1.286 TD
0.054 Tw
[(U-shaped valley whose lowest point is always close to 130 epochs.  Above)]TJ
/TT7 1 Tf
30.92 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
0.0544 Tw
(=0.5, the floor of the valley begins to)Tj
-31.5509 -1.286 TD
0 Tw
(rise steeply.)Tj
1 -2.388 TD
0.01 Tw
(Varying the)Tj
/TT6 1 Tf
5.019 0 TD
0 Tw
(r)Tj
/TT4 1 Tf
0.649 0 TD
0.0093 Tw
[(parameter by small amounts made very little difference in any of these trials.  Changing)]TJ
/TT6 1 Tf
35.523 0 TD
0 Tw
(r)Tj
/TT4 1 Tf
0.648 0 TD
0.009 Tw
(by a large)Tj
-42.839 -1.286 TD
0.0935 Tw
[(amount, either up or down, led to greatly increased learning times. )-344.5(A value of 1.0 or 2.0 seemed as good as any)]TJ
T*
0 Tw
(other and better than most.)Tj
1 -2.388 TD
0.0416 Tw
[(Plaut, Nowlan, and Hinton )41.6([12] present some analysis suggesting that it may be beneficial to use different values)]TJ
-1 -1.286 TD
0 Tw
(of)Tj
/TT7 1 Tf
1.207 0 TD
<00a1>Tj
/TT4 1 Tf
0.813 0 TD
0.124 Tw
[(for different weights in the network.  Specifically, they suggest that the)]TJ
/TT7 1 Tf
30.206 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.813 0 TD
0.1248 Tw
(value used in tuning each weight)Tj
-33.039 -1.286 TD
0.1194 Tw
[(should be inversely proportional to the fan-in of the unit receiving activation via that weight.  I tried this with a)]TJ
T*
0.0743 Tw
(variety of parameter values, but for the 10-5-10 network using standard backprop, it consistently performed worse)Tj
T*
0.019 Tw
(than using a single, constant)Tj
/TT7 1 Tf
11.65 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.708 0 TD
0.0185 Tw
[(value for all of the weights.  As we will see below, this "split epsilon" technique does)]TJ
ET
endstream
endobj
29 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
32 0 obj
<<
/Length 7455
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(8)Tj
-23.15 -3.6 TD
(turn out to be useful on networks such as the 128-7-128, where the variation in fan-in is very large.)Tj
1 -2.388 TD
0.0136 Tw
(The same paper suggests that the parameter values should be varied as the network learns:)Tj
/TT7 1 Tf
36.558 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.895 0 TD
0.014 Tw
(should be very small)Tj
-38.453 -1.286 TD
0.1229 Tw
[(at first, and should be increased after the network has chosen a direction.  The best schedule for increasing)]TJ
/TT7 1 Tf
45.13 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
1.003 0 TD
(is)Tj
-46.133 -1.286 TD
0.2206 Tw
[(apparently different for each problem.  I did not explore this idea extensively since the quickprop algorithm,)]TJ
T*
0.1639 Tw
(described below, seems to do roughly the same job, but in a way that adapts itself to the problem, rather than)Tj
T*
0 Tw
(requiring the human operator to do this job.)Tj
/TT2 1 Tf
11 0 0 11 72 601.65 Tm
(3.2. Eliminating the "Flat Spot")Tj
/TT4 1 Tf
10 0 0 10 82 588.79 Tm
0.0694 Tw
(Once I was able to display the weights and unit outputs during the course of the learning, one problem with the)Tj
-1 -1.286 TD
0.1506 Tw
(standard backprop algorithm became very clear: units were being turned off hard during the early stages of the)Tj
T*
0.057 Tw
[(learning, and they were getting stuck in the zero state.  The more hidden units there were in the network, the more)]TJ
T*
0 Tw
(likely it was that some output units would get stuck.)Tj
1 -2.388 TD
0.1486 Tw
[(This problem is due to the "flat spots" where the derivative of the sigmoid function approaches zero.  In the)]TJ
-1 -1.286 TD
0.1332 Tw
(standard back-propagation algorithm, as we back-propagate the error through the network, we multiply the error)Tj
T*
0.086 Tw
(seen by each unit)Tj
/TT6 1 Tf
7.509 0 TD
0 Tw
(j)Tj
/TT4 1 Tf
0.614 0 TD
0.085 Tw
(by the derivative of the sigmoid function at)Tj
/TT6 1 Tf
18.234 0 TD
0 Tw
(o)Tj
/TT4 1 Tf
0.872 0 TD
0.085 Tw
(, the current output of unit)Tj
/TT6 1 Tf
11.259 0 TD
0 Tw
(j)Tj
/TT4 1 Tf
0.2778 0 TD
0.085 Tw
[(. )-335(This derivative is)]TJ
/TT6 1 Tf
8 0 0 8 340.57 497.16 Tm
0 Tw
(j)Tj
/TT4 1 Tf
10 0 0 10 72 487.75 Tm
0.036 Tw
(equal to)Tj
/TT6 1 Tf
3.516 0 TD
0 Tw
(o)Tj
/TT4 1 Tf
0.872 0 TD
(\(1)Tj
/TT7 1 Tf
0.983 0 TD
<003c>Tj
/TT6 1 Tf
0.699 0 TD
(o)Tj
/TT4 1 Tf
0.722 0 TD
0.036 Tw
(\); I call this the)Tj
/TT6 1 Tf
6.485 0 TD
(sigmoid-prime function)Tj
/TT4 1 Tf
9.3964 0 TD
0.037 Tw
[(. )-286(Note that the value of the sigmoid-prime function goes to)]TJ
/TT6 1 Tf
8 0 0 8 112.16 484.3 Tm
2.9147 Tc
0 Tw
(jj)Tj
/TT4 1 Tf
10 0 0 10 72 474.89 Tm
0 Tc
0.0778 Tw
[(zero as the unitís output approaches 0.0 or to 1.0.  Even if such an output value represents the maximum possible)]TJ
0 -1.286 TD
0.1243 Tw
(error, a unit whose output is close to 0.0 or 1.0 will pass back only a tiny fraction of this error to the incoming)Tj
T*
0.0652 Tw
[(weights and to units in earlier layers.  Such a unit will theoretically recover, but it may take a very long time; in a)]TJ
T*
0 Tw
(machine with roundoff error and the potential for truncating very small values, such units may never recover.)Tj
1 -2.388 TD
0.0277 Tw
[(What can we do about this?  One possibility, suggested by James McClelland and tested by Michael Franzini )27.7([4],)]TJ
-1 -1.286 TD
0.0638 Tw
[(is to use an error measure that goes to infinity as the sigmoid-prime function goes to zero. )-313.2(This is mathematically)]TJ
T*
0.0011 Tw
(elegant, and it seemed to work fairly well, but it is hard to implement.  I chose to explore a simpler solution: alter the)Tj
T*
0.025 Tw
[(sigmoid-prime function so that it does not go to zero for any output value.  The first modification I tried worked the)]TJ
T*
0.0483 Tw
[(best: I simply added a constant 0.1 to the sigmoid prime value before using it to scale the error.  Instead of a curve)]TJ
T*
0 Tw
(that goes from 0.0 up to 0.25 and back down to 0.0, we now have a curve that goes from 0.1 to 0.35 and back to 0.1.)Tj
1 -2.388 TD
0.1406 Tw
[(This modification made a dramatic difference, cutting the learning time almost in half.  Once again we see a)]TJ
-1 -1.286 TD
0.176 Tw
(valley in)Tj
/TT7 1 Tf
4.074 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
(-)Tj
/TT7 1 Tf
0.333 0 TD
<00a1>Tj
/TT4 1 Tf
0.8651 0 TD
0.176 Tw
(space running from)Tj
/TT7 1 Tf
8.554 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
0.176 Tw
(=0.0 up to about)Tj
/TT7 1 Tf
7.5181 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
0.1762 Tw
(=0.5, and once again the best learning speed is roughly)Tj
-23.2369 -1.286 TD
0 Tw
(constant as)Tj
/TT7 1 Tf
4.666 0 TD
<005f>Tj
/TT4 1 Tf
0.881 0 TD
(increases over this range.  The two best values obtained were the following:)Tj
-1.434 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
90.09 272.26 m
521.91 272.26 l
170.07 272.26 m
170.07 289.29 l
214.05 272.26 m
214.05 289.29 l
258.03 272.26 m
258.03 289.29 l
302.01 272.26 m
302.01 289.29 l
345.99 272.26 m
345.99 289.29 l
389.97 272.26 m
389.97 289.29 l
433.95 272.26 m
433.95 289.29 l
477.93 272.26 m
477.93 289.29 l
S
BT
10 0 0 10 114.25 261.41 Tm
[(10-5-10)-4115(25)-3273(1.5)-3148(0.0)-3148(1.0)-3023(115)-3148(50)-3398(75)-3398(14)]TJ
ET
90.09 255.23 m
521.91 255.23 l
S
BT
10 0 0 10 114.25 244.38 Tm
[(10-5-10)-4115(25)-3273(0.9)-3148(0.4)-3148(1.0)-3023(120)-3148(50)-3398(74)-3398(14)]TJ
ET
90.09 238.2 431.82 51.09 re
170.07 238.2 m
170.07 272.26 l
214.05 238.2 m
214.05 272.26 l
258.03 238.2 m
258.03 272.26 l
302.01 238.2 m
302.01 272.26 l
345.99 238.2 m
345.99 272.26 l
389.97 238.2 m
389.97 272.26 l
433.95 238.2 m
433.95 272.26 l
477.93 238.2 m
477.93 272.26 l
S
BT
10 0 0 10 82 214.32 Tm
0.1067 Tw
[(I tried other ways of altering the sigmoid-prime function as well.  The most radical was simply to replace this)]TJ
-1 -1.286 TD
0.094 Tw
(function with a constant value of 0.5; in effect, this eliminates the multiplication by the derivative of the sigmoid)Tj
T*
0 Tw
[(altogether. )-250(The best performance obtained with this variation was the following:)]TJ
4.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
90.09 162.33 m
521.91 162.33 l
170.07 162.33 m
170.07 179.36 l
214.05 162.33 m
214.05 179.36 l
258.03 162.33 m
258.03 179.36 l
302.01 162.33 m
302.01 179.36 l
345.99 162.33 m
345.99 179.36 l
389.97 162.33 m
389.97 179.36 l
433.95 162.33 m
433.95 179.36 l
477.93 162.33 m
477.93 179.36 l
S
BT
10 0 0 10 114.25 151.48 Tm
[(10-5-10)-4115(25)-3273(0.5)-3148(0.4)-3148(1.0)-3023(170)-3148(50)-3398(89)-3398(34)]TJ
ET
90.09 145.3 431.82 34.06 re
170.07 145.3 m
170.07 162.33 l
214.05 145.3 m
214.05 162.33 l
258.03 145.3 m
258.03 162.33 l
302.01 145.3 m
302.01 162.33 l
345.99 145.3 m
345.99 162.33 l
389.97 145.3 m
389.97 162.33 l
433.95 145.3 m
433.95 162.33 l
477.93 145.3 m
477.93 162.33 l
S
BT
10 0 0 10 82 121.42 Tm
0.0417 Tw
[(I next tried replacing sigmoid-prime with a function that returned a random number in the range 0.0 to 1.0.  This)]TJ
-1 -1.286 TD
0.0016 Tw
(did not do as well, probably because some learning trials were wasted when the random number happened to be very)Tj
T*
0 Tw
[(small. )-250(The best result obtained with this scheme was the following:)]TJ
ET
endstream
endobj
33 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
35 0 obj
<<
/Length 6832
>>
stream
BT
/TT4 1 Tf
10 0 0 10 303.5 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(9)Tj
-19.037 -3.999 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
90.09 702.97 m
521.91 702.97 l
170.07 702.97 m
170.07 720 l
214.05 702.97 m
214.05 720 l
258.03 702.97 m
258.03 720 l
302.01 702.97 m
302.01 720 l
345.99 702.97 m
345.99 720 l
389.97 702.97 m
389.97 720 l
433.95 702.97 m
433.95 720 l
477.93 702.97 m
477.93 720 l
S
BT
10 0 0 10 114.25 692.12 Tm
[(10-5-10)-4115(25)-3273(0.4)-3148(0.7)-3148(1.0)-3023(340)-3148(75)-3148(163)-3148(75)]TJ
ET
90.09 685.94 431.82 34.06 re
170.07 685.94 m
170.07 702.97 l
214.05 685.94 m
214.05 702.97 l
258.03 685.94 m
258.03 702.97 l
302.01 685.94 m
302.01 702.97 l
345.99 685.94 m
345.99 702.97 l
389.97 685.94 m
389.97 702.97 l
433.95 685.94 m
433.95 702.97 l
477.93 685.94 m
477.93 702.97 l
S
BT
10 0 0 10 82 662.06 Tm
0.0788 Tw
(Finally, I tried replacing the sigmoid-prime function with the sum of a constant 0.25 and a random value in the)Tj
-1 -1.286 TD
0 Tw
(range 0.0 to 0.5.  This did about as well as the constant alone:)Tj
4.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
90.09 622.93 m
521.91 622.93 l
170.07 622.93 m
170.07 639.96 l
214.05 622.93 m
214.05 639.96 l
258.03 622.93 m
258.03 639.96 l
302.01 622.93 m
302.01 639.96 l
345.99 622.93 m
345.99 639.96 l
389.97 622.93 m
389.97 639.96 l
433.95 622.93 m
433.95 639.96 l
477.93 622.93 m
477.93 639.96 l
S
BT
10 0 0 10 114.25 612.08 Tm
[(10-5-10)-4115(25)-3273(0.5)-3148(0.6)-3148(1.0)-3023(135)-3148(50)-3398(90)-3398(27)]TJ
ET
90.09 605.9 431.82 34.06 re
170.07 605.9 m
170.07 622.93 l
214.05 605.9 m
214.05 622.93 l
258.03 605.9 m
258.03 622.93 l
302.01 605.9 m
302.01 622.93 l
345.99 605.9 m
345.99 622.93 l
389.97 605.9 m
389.97 622.93 l
433.95 605.9 m
433.95 622.93 l
477.93 605.9 m
477.93 622.93 l
S
BT
10 0 0 10 82 582.02 Tm
0.0146 Tw
(In all cases, the same kind of valley in)Tj
/TT7 1 Tf
15.685 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
(-)Tj
/TT7 1 Tf
0.333 0 TD
<00a1>Tj
/TT4 1 Tf
0.7041 0 TD
0.015 Tw
(space was observed, and I was always able to find an)Tj
/TT7 1 Tf
21.659 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.704 0 TD
0.015 Tw
(value that gave)Tj
-40.716 -1.286 TD
0 Tw
(a near-optimal performance with)Tj
/TT7 1 Tf
13.385 0 TD
<005f>Tj
/TT4 1 Tf
0.6309 0 TD
(=0.0.)Tj
-13.0159 -2.388 TD
0.1069 Tw
(The primary lesson from these experiments is that it is very useful to eliminate the flat spots by one means or)Tj
-1 -1.286 TD
0.1342 Tw
[(another. )-383.8(In standard backprop, we must carefully choose the learning parameters to avoid the problem of stuck)]TJ
T*
0.0782 Tw
[(units; with the modified sigmoid-prime function, we can optimize the parameters for best performance overall.  A)]TJ
T*
0.0522 Tw
(slight modification of the classic sigmoid-prime function did the job best, but replacing this step with a constant or)Tj
T*
0.0527 Tw
[(with a constant plus noise only reduces the learning speed by about 20%.  This suggests that this general family of)]TJ
T*
0.0098 Tw
(learning algorithms is very robust, and will give you decent results however you scale the error, as long as you donít)Tj
T*
0.1316 Tw
[(change the sign or eliminate the error signal by letting the sigmoid-prime function go to zero.  Of course, these)]TJ
T*
0 Tw
(results may not hold for other problems or for multi-layer networks.)Tj
/TT2 1 Tf
11 0 0 11 72 419.09 Tm
(3.3. Using a Non-Linear Error Function)Tj
/TT4 1 Tf
10 0 0 10 82 406.23 Tm
0.15 Tw
(As mentioned in the previous section, several researchers have eliminated the flat spots in the sigmoid-prime)Tj
-1 -1.286 TD
0.0365 Tw
(function, at least for the units in the output layer of the network, by using an error function that grows to infinity as)Tj
T*
0.0398 Tw
[(the difference between the desired and observed outputs goes to 1.0 or -1.0.  As the error approaches these extreme)]TJ
T*
0.1406 Tw
(values, the product of this non-linear error function and sigmoid-prime remains finite, so some error signal gets)Tj
T*
0 Tw
(through and the unit does not get stuck.)Tj
1 -2.388 TD
0.0522 Tw
(David Plaut suggested to me that the non-linear error function might speed up learning even though the problem)Tj
-1 -1.286 TD
0.0226 Tw
[(of stuck units had been handled by other means.  The idea was that for small differences between the output and the)]TJ
T*
0.002 Tw
(desired output, the error should behave linearly, but as the difference increased, the error function should grow faster)Tj
T*
0 Tw
(than linearly, heading toward infinity as errors approach their maximum values.)Tj
1 -2.388 TD
0.0917 Tw
[(One function that meets these requirements is hyperbolic arctangent of the difference.  I tried using that, rather)]TJ
-1 -1.286 TD
0.0952 Tw
[(than the difference itself, as the error signal fed into the output units in the network. )-345.8(Since this function was not)]TJ
T*
0.0551 Tw
(competing against one going to zero, I did not let it grow arbitrarily large; I cut it off at -.9999999 and +.9999999,)Tj
T*
0 Tw
(and used values of -17.0 and +17.0 for more extreme differences.)Tj
1 -2.388 TD
0.1145 Tw
[(This did indeed have a modest beneficial effect. )-365.5(On the 10-5-10 encoder, using backprop with the hyperbolic)]TJ
-1 -1.286 TD
0.2562 Tw
(arctan error function and adding 0.1 to sigmoid-prime, I was able to get the following result, about a 25%)Tj
T*
0 Tw
(improvement:)Tj
4.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
3.863 Tc
<00a1005f>Tj
/TT6 1 Tf
8.821 0 TD
0 Tc
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
90.09 154 m
521.91 154 l
170.07 154 m
170.07 171.03 l
214.05 154 m
214.05 171.03 l
258.03 154 m
258.03 171.03 l
302.01 154 m
302.01 171.03 l
345.99 154 m
345.99 171.03 l
389.97 154 m
389.97 171.03 l
433.95 154 m
433.95 171.03 l
477.93 154 m
477.93 171.03 l
S
BT
10 0 0 10 114.25 143.15 Tm
[(10-5-10)-4115(25)-3273(0.6)-3148(0.0)-3148(1.0)-3273(85)-3398(40)-3023(58.7)-3023(13)]TJ
ET
90.09 136.97 431.82 34.06 re
170.07 136.97 m
170.07 154 l
214.05 136.97 m
214.05 154 l
258.03 136.97 m
258.03 154 l
302.01 136.97 m
302.01 154 l
345.99 136.97 m
345.99 154 l
389.97 136.97 m
389.97 154 l
433.95 136.97 m
433.95 154 l
477.93 136.97 m
477.93 154 l
S
BT
10 0 0 10 82 113.09 Tm
0.0131 Tw
(I have not tried applying nonlinear error functions to units in interior layers of the network, but plan to investigate)Tj
-1 -1.286 TD
0.0755 Tw
[(this in the near future.  Some sort of non-linear error function may be of value in increasing the learning speed of)]TJ
T*
0 Tw
(networks with multiple hidden layers.)Tj
ET
endstream
endobj
36 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
38 0 obj
<<
/Length 7161
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(10)Tj
/TT2 1 Tf
11 0 0 11 72 712.63 Tm
(3.4. The Quickprop Algorithm)Tj
/TT4 1 Tf
10 0 0 10 82 699.77 Tm
0.0401 Tw
(Back-propagation and its relatives work by calculating the partial first derivative of the overall error with respect)Tj
-1 -1.286 TD
0.0626 Tw
[(to each weight.  Given this information we can do gradient descent in weight space.  If we take infinitesimal steps)]TJ
T*
0.119 Tw
(down the gradient, we are guaranteed to reach a local minimum, and it has been empirically determined that for)Tj
T*
0.1981 Tw
(many problems this local minimum will be a global minimum, or at least a "good enough" solution for most)Tj
T*
0 Tw
(purposes.)Tj
1 -2.388 TD
0.0607 Tw
(Of course, if we want to find a solution in the shortest possible time, we do not want to take infinitesimal steps;)Tj
-1 -1.286 TD
0.1218 Tw
[(we want to take the largest steps possible without overshooting the solution.  Unfortunately, a set of partial first)]TJ
T*
0.041 Tw
(derivatives collected at a single point tells us very little about how large a step we may safely take in weight space.)Tj
T*
0.265 Tw
(If we knew something about the higher-order derivatives -- the curvature of the error function -- we could)Tj
T*
0 Tw
(presumably do much better.)Tj
1 -2.388 TD
0.1482 Tw
[(Two kinds of approaches to this problem have been tried.  The first approach tries to dynamically adjust the)]TJ
-1 -1.286 TD
0.1961 Tw
(learning rate, either globally or separately for each weight, based in some heuristic way on the history of the)Tj
T*
0.108 Tw
[(computation. )-358(The momentum term used in standard back-propagation is a form of this strategy; so are the fixed)]TJ
T*
0.047 Tw
[(schedules for parameter adjustment that are recommended in )47([12], though in this case the adjustment is based upon)]TJ
T*
0.1244 Tw
[(the experience of the programmer rather than that of the network.  Franzini )124.4([4] has investigated a technique that)]TJ
T*
0.103 Tw
(heuristically adjusts the global)Tj
/TT7 1 Tf
12.856 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.792 0 TD
0.1023 Tw
(parameter, increasing it whenever two successive gradient vectors are nearly the)Tj
-13.648 -1.286 TD
0.0669 Tw
[(same and decreasing it otherwise.  Jacobs )66.9([5] has conducted an empirical study comparing standard backprop with)]TJ
T*
0.0852 Tw
[(momentum to a rule that dynamically adjusts a separate learning-rate parameter for each weight.  Cater )85.2([2] uses a)]TJ
T*
0.0587 Tw
[(more complex heuristic for adjusting the learning rate.  All of these methods improve the overall learning speed to)]TJ
T*
0 Tw
(some degree.)Tj
1 -2.388 TD
0.066 Tw
(The other kind of approach makes explicit use of the second derivative of the error with respect to each weight.)Tj
-1 -1.286 TD
0.1421 Tw
(Given this information, we can select a new set of weights using Newtonís method or some more sophisticated)Tj
T*
0.2185 Tw
[(optimization technique.  Unfortunately, it requires a very costly global computation to derive the true second)]TJ
T*
0.0072 Tw
[(derivative, so some approximation is used.  Parker )7.2([8], Watrous )7.2([17], and Becker and LeCun )7.2([1] have all been active)]TJ
T*
0.0513 Tw
[(in this area.  Watrous has implemented two such algorithms and tried them on the XOR problem.  He claims some)]TJ
T*
0.1715 Tw
(improvement over back-propagation, but it does not appear that his methods will scale up well to much larger)Tj
T*
0 Tw
(problems.)Tj
1 -2.388 TD
0.0339 Tw
[(I have developed an algorithm that I call "quickprop" that has some connection to both of these traditions.  It is a)]TJ
-1 -1.286 TD
0.0505 Tw
[(second-order method, based loosely on Newtonís method, but in spirit it is more heuristic than formal.  Everything)]TJ
T*
0.0327 Tw
(proceeds as in standard back-propagation, but for each weight I keep a copy of the)Tj
/TT7 1 Tf
33.703 0 TD
0 Tw
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(E)Tj
/TT4 1 Tf
0.7609 0 TD
(/)Tj
/TT7 1 Tf
0.428 0 TD
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT7 1 Tf
0.2778 0 TD
<003c>Tj
/TT4 1 Tf
0.5488 0 TD
0.032 Tw
(1\), the error derivative)Tj
-37.7068 -1.286 TD
0.0558 Tw
(computed during the previous training epoch, along with the difference between the current and previous values of)Tj
T*
0 Tw
(this weight.  The)Tj
/TT7 1 Tf
6.972 0 TD
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(E)Tj
/TT4 1 Tf
0.7609 0 TD
(/)Tj
/TT7 1 Tf
0.428 0 TD
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
(\) value for the current training epoch is also available at weight-update time.)Tj
-9.427 -2.388 TD
0.0126 Tw
(I then make two risky assumptions: first, that the error vs. weight curve for each weight can be approximated by a)Tj
-1 -1.286 TD
0.0069 Tw
(parabola whose arms open upward; second, that the change in the slope of the error curve, as seen by each weight, is)Tj
T*
0.02 Tw
[(not affected by all the other weights that are changing at the same time.  For each weight, independently, we use the)]TJ
T*
0.033 Tw
(previous and current error slopes and the weight-change between the points at which these slopes were measured to)Tj
T*
0.0028 Tw
(determine a parabola; we then jump directly to the minimum point of this parabola.  The computation is very simple,)Tj
T*
0 Tw
(and it uses only the information local to the weight being updated:)Tj
/TT6 1 Tf
6.958 -1.52 TD
(S)Tj
/TT4 1 Tf
0.5 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
(\))Tj
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
124.87 149.21 m
172.73 149.21 l
S
BT
/TT7 1 Tf
10 0 0 10 92 147.01 Tm
<0036>Tj
/TT6 1 Tf
0.6118 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
(\) =)Tj
/TT7 1 Tf
6.4334 0 TD
<0036>Tj
/TT6 1 Tf
0.6118 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT7 1 Tf
0.2778 0 TD
<003c>Tj
/TT4 1 Tf
0.5488 0 TD
(1\))Tj
/TT6 1 Tf
-7.4745 -0.66 TD
(S)Tj
/TT4 1 Tf
0.5 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT7 1 Tf
0.2778 0 TD
<003c>Tj
/TT4 1 Tf
0.5488 0 TD
(1\))Tj
/TT7 1 Tf
0.9833 0 TD
<003c>Tj
/TT6 1 Tf
0.699 0 TD
(S)Tj
/TT4 1 Tf
0.5 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
(\))Tj
-8.7398 -2.388 TD
(where)Tj
/TT6 1 Tf
2.777 0 TD
(S)Tj
/TT4 1 Tf
0.5 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT4 1 Tf
0.2778 0 TD
0.084 Tw
(\) and)Tj
/TT6 1 Tf
2.4452 0 TD
0 Tw
(S)Tj
/TT4 1 Tf
0.5 0 TD
(\()Tj
/TT6 1 Tf
0.333 0 TD
(t)Tj
/TT7 1 Tf
0.2778 0 TD
<003c>Tj
/TT4 1 Tf
0.5488 0 TD
0.0845 Tw
(1\) are the current and previous values of)Tj
/TT7 1 Tf
17.0613 0 TD
0 Tw
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(E)Tj
/TT4 1 Tf
0.7609 0 TD
(/)Tj
/TT7 1 Tf
0.428 0 TD
<002c>Tj
/TT6 1 Tf
0.4941 0 TD
(w)Tj
/TT4 1 Tf
0.667 0 TD
0.085 Tw
[(. )-335(Of course, this new value is only a crude)]TJ
-28.8981 -1.286 TD
0.2497 Tw
(approximation to the optimum value for the weight, but when applied iteratively this method is surprisingly)Tj
T*
0 Tw
[(effective. )-250(Notice that the old)]TJ
/TT7 1 Tf
11.914 0 TD
<005f>Tj
/TT4 1 Tf
0.881 0 TD
(parameter is gone, though we will need to keep)Tj
/TT7 1 Tf
19.163 0 TD
<00a1>Tj
/TT4 1 Tf
0.689 0 TD
(\(see below\).)Tj
ET
endstream
endobj
39 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
43 0 obj
<<
/Length 6688
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(11)Tj
-21.9 -3.6 TD
0.1959 Tw
(Using this update formula, if the current slope is somewhat smaller than the previous one, but in the same)Tj
-1 -1.286 TD
0.0834 Tw
[(direction, the weight will change again in the same direction.  The step may be large or small, depending on how)]TJ
T*
0.0331 Tw
[(much the slope was reduced by the previous step.  If the current slope is in the opposite direction from the previous)]TJ
T*
0.0435 Tw
[(one, that means that we have crossed over the minimum and that we are now on the opposite side of the valley.  In)]TJ
T*
0.0557 Tw
[(this case, the next step will place us somewhere between the current and previous positions.  The third case occurs)]TJ
T*
0.0314 Tw
[(when the current slope is in the same direction as the previous slope, but is the same size or larger in magnitude.  If)]TJ
T*
0.1388 Tw
(we were to blindly follow the formula in this case, we would end up taking an infinite step or actually moving)Tj
T*
0 Tw
(backwards, up the current slope and toward a local maximum.)Tj
1 -2.388 TD
0.036 Tw
[(I have experimented with several ways of handling this third situation.  The method that seems to work best is to)]TJ
-1 -1.286 TD
0.075 Tw
(create a new parameter, which I call)Tj
/TT8 1 Tf
15.241 0 TD
0 Tw
(µ)Tj
/TT4 1 Tf
0.5762 0 TD
0.0756 Tw
[(, the "maximum growth factor". )-326.4(No weight step is allowed to be greater in)]TJ
-15.8172 -1.286 TD
0.061 Tw
(magnitude than)Tj
/TT8 1 Tf
6.566 0 TD
0 Tw
(µ)Tj
/TT4 1 Tf
0.887 0 TD
0.0601 Tw
(times the previous step for that weight; if the step computed by the quickprop formula would be)Tj
-7.453 -1.286 TD
0.011 Tw
(too large, infinite, or uphill on the current slope, we instead use)Tj
/TT8 1 Tf
25.712 0 TD
0 Tw
(µ)Tj
/TT4 1 Tf
0.837 0 TD
0.0115 Tw
(times the previous step as the size of the new step.)Tj
-26.549 -1.286 TD
0.0438 Tw
(The idea is that if, instead of flattening out, the error curve actually becomes steeper as you move down it, you can)Tj
T*
0.0858 Tw
[(afford to accelerate, but within limits.  Since there is some "noise" coming from the simultaneous update of other)]TJ
T*
0.154 Tw
[(units, we donít want to extrapolate too far from a finite baseline.  Experiments show that if)]TJ
/TT8 1 Tf
39.196 0 TD
0 Tw
(µ)Tj
/TT4 1 Tf
0.979 0 TD
0.153 Tw
(is too large, the)Tj
-40.175 -1.286 TD
0.024 Tw
[(network behaves chaotically and fails to converge.  The optimal value of)]TJ
/TT8 1 Tf
29.643 0 TD
0 Tw
(µ)Tj
/TT4 1 Tf
0.85 0 TD
0.0242 Tw
(depends to some extent upon the type of)Tj
-30.493 -1.286 TD
0 Tw
(problem, but a value of 1.75 works well for a wide range of problems.)Tj
1 -2.388 TD
0.0245 Tw
(Since quickprop changes weights based on what happened during the previous weight update, we need some way)Tj
-1 -1.286 TD
0.0424 Tw
[(to bootstrap the process.  In addition, we need a way to restart the learning process for a weight that has previously)]TJ
T*
0.0821 Tw
(taken a step of size zero but that now is seeing a non-zero-slope because something has changed elsewhere in the)Tj
T*
0.0387 Tw
[(network. )-287.3(The obvious move is to use gradient descent, based on the current slope and some learning rate)]TJ
/TT7 1 Tf
43.033 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.439 0 TD
0.039 Tw
(, to start)Tj
-43.472 -1.286 TD
0 Tw
(the process and to restart the process for any weight that has a previous step size of zero.)Tj
1 -2.388 TD
0.1221 Tw
[(It took me several tries to get this "ignition" process working well.  Originally I picked a small threshold and)]TJ
-1 -1.286 TD
0.2481 Tw
(switched from the quadratic approximation to gradient descent whenever the previous weight fell below this)Tj
T*
0.1401 Tw
[(threshold. )-391.9(This worked fairly well, but I came to suspect that odd things were happening in the vicinity of the)]TJ
T*
0.144 Tw
[(threshold, especially for very large encoder problems.  I replaced this mechanism with one that always added a)]TJ
T*
0.0395 Tw
[(gradient descent term to the step computed by the quadratic method. )-288.5(This worked well when a weight was moving)]TJ
T*
0.0582 Tw
(down a slope, but it led to oscillation when the weight overshot the minimum and had to come back: the quadratic)Tj
T*
0.1553 Tw
(method would accurately locate the bottom of the parabola, and the gradient descent term would then push the)Tj
T*
0 Tw
(weight past this point.)Tj
1 -2.388 TD
0.0148 Tw
(My current version of quickprop always adds)Tj
/TT7 1 Tf
18.517 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.704 0 TD
0.015 Tw
(times the current slope to the)Tj
/TT7 1 Tf
11.922 0 TD
0 Tw
<0036>Tj
/TT6 1 Tf
0.6118 0 TD
(w)Tj
/TT4 1 Tf
0.9322 0 TD
0.015 Tw
(value computed by the quadratic)Tj
-33.687 -1.286 TD
0.0347 Tw
(formula, unless the current slope is opposite in sign from the previous slope; in that case, the quadratic term is used)Tj
T*
0 Tw
(alone.)Tj
1 -2.388 TD
0.1126 Tw
[(One final refinement is required.  For some problems, quickprop will allow some of the weights to grow very)]TJ
-1 -1.286 TD
0.0668 Tw
[(large. )-317.2(This leads to floating-point overflow errors in the middle of a training session.  I fix this by adding a small)]TJ
T*
0 Tw
(weight-decay term to the slope computed for each weight.  This keeps the weights within an acceptable range.)Tj
1 -2.388 TD
0.1924 Tw
(Quickprop can suffer from the same "flat spot" problems as standard backprop, so I always run it with the)Tj
-1 -1.286 TD
0 Tw
(sigmoid-prime function modified by the addition of 0.1, as described in the previous section.)Tj
1 -2.388 TD
(With the normal linear error function, the following result was the best one obtained using quickprop:)Tj
3.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
<00a1>Tj
/TT8 1 Tf
4.33 0 TD
(µ)Tj
/TT6 1 Tf
4.491 0 TD
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
90.09 121.05 m
521.91 121.05 l
170.07 121.05 m
170.07 138.08 l
214.05 121.05 m
214.05 138.08 l
258.03 121.05 m
258.03 138.08 l
302.01 121.05 m
302.01 138.08 l
345.99 121.05 m
345.99 138.08 l
389.97 121.05 m
389.97 138.08 l
433.95 121.05 m
433.95 138.08 l
477.93 121.05 m
477.93 138.08 l
S
BT
10 0 0 10 114.25 110.2 Tm
[(10-5-10)-3865(100)-3023(1.5)-2898(1.75)-2898(2.0)-3273(72)-3398(13)-3023(22.1)-2898(8.9)]TJ
ET
90.09 104.02 431.82 34.06 re
170.07 104.02 m
170.07 121.05 l
214.05 104.02 m
214.05 121.05 l
258.03 104.02 m
258.03 121.05 l
302.01 104.02 m
302.01 121.05 l
345.99 104.02 m
345.99 121.05 l
389.97 104.02 m
389.97 121.05 l
433.95 104.02 m
433.95 121.05 l
477.93 104.02 m
477.93 121.05 l
S
BT
10 0 0 10 82 80.14 Tm
(With the addition of the hyperbolic arctan error function, quickprop did better still:)Tj
ET
endstream
endobj
44 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
/TT8 45 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
47 0 obj
<<
/Length 6752
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(12)Tj
-18.787 -3.999 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
<00a1>Tj
/TT8 1 Tf
4.33 0 TD
(µ)Tj
/TT6 1 Tf
4.491 0 TD
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
90.09 702.97 m
521.91 702.97 l
170.07 702.97 m
170.07 720 l
214.05 702.97 m
214.05 720 l
258.03 702.97 m
258.03 720 l
302.01 702.97 m
302.01 720 l
345.99 702.97 m
345.99 720 l
389.97 702.97 m
389.97 720 l
433.95 702.97 m
433.95 720 l
477.93 702.97 m
477.93 720 l
S
BT
10 0 0 10 114.25 692.12 Tm
[(10-5-10)-3865(100)-2773(0.35)-2648(1.75)-2898(2.0)-3273(21)-3648(9)-3023(14.01)-2648(2.1)]TJ
ET
90.09 685.94 431.82 34.06 re
170.07 685.94 m
170.07 702.97 l
214.05 685.94 m
214.05 702.97 l
258.03 685.94 m
258.03 702.97 l
302.01 685.94 m
302.01 702.97 l
345.99 685.94 m
345.99 702.97 l
389.97 685.94 m
389.97 702.97 l
433.95 685.94 m
433.95 702.97 l
477.93 685.94 m
477.93 702.97 l
S
BT
10 0 0 10 82 662.06 Tm
0.0529 Tw
(This result is better by about a factor of 4 than any time I obtained with a modified but non-quadratic version of)Tj
-1 -1.286 TD
0.0054 Tw
[(backprop, and it is almost an order of magnitude better than the value of 129 I obtained for standard backprop.  With)]TJ
T*
0.036 Tw
(quickprop, only the)Tj
/TT7 1 Tf
8.163 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.725 0 TD
0.036 Tw
(parameter seems to require problem-specific tuning, and even)Tj
/TT7 1 Tf
25.254 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.725 0 TD
0.0353 Tw
(does not have to be tuned too)Tj
-34.867 -1.286 TD
0 Tw
(carefully for reasonably good results.)Tj
/TT2 1 Tf
11 0 0 11 72 587.31 Tm
(3.5. Scaling Experiments)Tj
/TT4 1 Tf
10 0 0 10 82 574.45 Tm
0.0963 Tw
(The next step was to see how well the combination of quickprop, adding 0.1 to sigmoid-prime, and hyperbolic)Tj
-1 -1.286 TD
0.0623 Tw
[(arctan error would scale up to larger encoder problems.  I decided to run a series of "tight" encoders: 4-2-4, 8-3-8,)]TJ
T*
0.0411 Tw
[(and so on.  For the larger problems in the series, the fan-in for the hidden units was much greater than the fan-in to)]TJ
T*
0.028 Tw
(the output units, and it proved beneficial to divide the value of)Tj
/TT7 1 Tf
25.528 0 TD
0 Tw
<00a1>Tj
/TT4 1 Tf
0.717 0 TD
0.028 Tw
(by the fan-in of the unit that is receiving activation)Tj
-26.245 -1.286 TD
0 Tw
(from the weight being updated.  It also proved useful to gradually reduce)Tj
/TT7 1 Tf
29.439 0 TD
<00a1>Tj
/TT4 1 Tf
0.689 0 TD
(as the problem size increased.)Tj
-29.128 -2.388 TD
(The results obtained for this series were as follows:)Tj
3.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
<00a1>Tj
/TT8 1 Tf
4.33 0 TD
(µ)Tj
/TT6 1 Tf
4.491 0 TD
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
90.09 472.86 m
521.91 472.86 l
170.07 472.86 m
170.07 489.89 l
214.05 472.86 m
214.05 489.89 l
258.03 472.86 m
258.03 489.89 l
302.01 472.86 m
302.01 489.89 l
345.99 472.86 m
345.99 489.89 l
389.97 472.86 m
389.97 489.89 l
433.95 472.86 m
433.95 489.89 l
477.93 472.86 m
477.93 489.89 l
S
BT
10 0 0 10 119.25 462.01 Tm
[(4-2-4)-4365(100)-3023(2.0)-2898(1.75)-2898(2.0)-3273(36)-3648(8)-3023(15.93)-2648(6.2)]TJ
ET
90.09 455.83 m
521.91 455.83 l
S
BT
10 0 0 10 119.25 444.98 Tm
[(8-3-8)-4365(100)-3023(2.0)-2898(1.75)-2898(2.0)-3273(42)-3398(12)-2773(21.99)-2648(5.6)]TJ
ET
90.09 438.8 m
521.91 438.8 l
S
BT
10 0 0 10 114.25 427.95 Tm
[(16-4-16)-3865(100)-3023(1.2)-2898(1.75)-2898(2.0)-3273(48)-3398(20)-2773(28.93)-2648(5.4)]TJ
ET
90.09 421.77 m
521.91 421.77 l
S
BT
10 0 0 10 114.25 410.92 Tm
[(32-5-32)-4115(25)-3273(1.0)-2898(1.75)-2898(2.0)-3273(38)-3398(26)-2773(30.48)-2648(3.1)]TJ
ET
90.09 404.74 m
521.91 404.74 l
S
BT
10 0 0 10 114.25 393.89 Tm
[(64-6-64)-4115(10)-3023(0.75)-2648(1.75)-2898(2.0)-3273(37)-3398(28)-2773(33.90)-2648(2.9)]TJ
ET
90.09 387.71 m
521.91 387.71 l
S
BT
10 0 0 10 109.25 376.86 Tm
[(128-7-128)-3865(5)-3523(0.5)-2898(1.75)-2898(2.0)-3273(43)-3398(36)-2773(40.20)-2648(2.8)]TJ
ET
90.09 370.68 m
521.91 370.68 l
S
BT
10 0 0 10 109.25 359.83 Tm
[(256-8-256)-3865(5)-3523(0.3)-2898(1.75)-2898(2.0)-3273(44)-3398(40)-2773(42.00)-2648(1.6)]TJ
ET
90.09 353.65 431.82 136.24 re
170.07 353.65 m
170.07 472.86 l
214.05 353.65 m
214.05 472.86 l
258.03 353.65 m
258.03 472.86 l
302.01 353.65 m
302.01 472.86 l
345.99 353.65 m
345.99 472.86 l
389.97 353.65 m
389.97 472.86 l
433.95 353.65 m
433.95 472.86 l
477.93 353.65 m
477.93 472.86 l
S
BT
10 0 0 10 82 329.77 Tm
0.0543 Tw
[(These times are significantly better than any others I have seen for tight encoder problems.  The literature of the)]TJ
-1 -1.286 TD
0.0238 Tw
[(field gives very few specific timings for such problems, especially for large ones.  The best time I have obtained for)]TJ
T*
0.0426 Tw
[(the 16-4-16 encoder with standard backprop is 794.6 epochs \(average time over 10 trials\).  With the sigmoid-prime)]TJ
T*
0 Tw
(function modified to add 0.1, the time goes down to 539.2.)Tj
1 -2.388 TD
0.0817 Tw
(David Plaut, who has run many backprop simulations during his graduate student career at CMU, is able to get)Tj
-1 -1.286 TD
0.0088 Tw
[(times "generally in the low 40ís" on the 16-4-16 encoder using backprop with a non-linear error function.  However,)]TJ
T*
0.0671 Tw
(he accomplishes this by watching the progress of each learning trial on the display and adjusting)Tj
/TT7 1 Tf
39.902 0 TD
0 Tw
<005f>Tj
/TT4 1 Tf
0.948 0 TD
0.067 Tw
(by hand as the)Tj
-40.85 -1.286 TD
0.1313 Tw
[(learning progresses.  This method is hard to replicate, and it is unclear how well it scales up.  I suspect that an)]TJ
T*
0.1438 Tw
(analysis of Plaitís real-time adjustments would show that he is doing something very similar to what quickprop)Tj
T*
0 Tw
(does.)Tj
1 -2.388 TD
0.012 Tw
[(Juergen Schmidhuber )12([14] has investigated this same class of problems up to 64-6-64 using two methods: first, he)]TJ
-1 -1.286 TD
0.0096 Tw
(used standard backprop, but he adjusted the weights after every presentation of a training example rather than after a)Tj
T*
0.1873 Tw
(full epoch; second, he used a learning technique of his own that measures the total error, rather than the first)Tj
T*
0.0047 Tw
(derivative, and tries to converge toward a zero of the error function.  On the 16-4-16 encoder, Schmidhuber reports a)Tj
T*
0.0834 Tw
(learning time of 239 epochs for backprop and 146 for his own method; on 64-6-64, he gets 750 for backprop and)Tj
T*
0 Tw
(220 for his own method.)Tj
1 -2.388 TD
0.0821 Tw
(The most exciting aspect of the learning times in the table above are the way they scale up as the problem size)Tj
-1 -1.286 TD
0.0902 Tw
[(increases. )-339.8(If we take N as the number of patterns to be learned, the learning time measured in epochs is actually)]TJ
ET
endstream
endobj
48 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
/TT8 45 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
50 0 obj
<<
/Length 6196
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(13)Tj
-22.9 -3.6 TD
0.115 Tw
(growing more slowly than log)Tj
/TT6 1 Tf
12.665 0 TD
0 Tw
(N)Tj
/TT4 1 Tf
0.667 0 TD
0.1147 Tw
[(. )-365.3(In the past, it was generally believed that for tight encoder problems this time)]TJ
-13.332 -1.286 TD
0 Tw
(would grow exponentially with the problem size, or at least linearly.)Tj
1 -2.388 TD
0.1211 Tw
(Of course, the measurement of learning time in epochs can be deceiving. The number of training examples in)Tj
-1 -1.286 TD
0.0387 Tw
(each epoch grows by a factor of N, and the time required to run each forward-backward pass on a serial machine is)Tj
T*
0.0487 Tw
(proportional to the number of connections -- also roughly a factor of N. This means that on a serial machine, using)Tj
8 0 0 8 482.06 641.27 Tm
3.3112 Tc
0 Tw
(22)Tj
10 0 0 10 72 637.82 Tm
0 Tc
0.0197 Tw
(the techniques described here, the actual clock time required grows by a factor somewhere between)Tj
/TT6 1 Tf
40.339 0 TD
0 Tw
(N)Tj
/TT4 1 Tf
1.336 0 TD
(and)Tj
/TT6 1 Tf
1.713 0 TD
(N)Tj
/TT4 1 Tf
1.067 0 TD
(log)Tj
/TT6 1 Tf
1.428 0 TD
(N)Tj
/TT4 1 Tf
0.667 0 TD
(.)Tj
-46.55 -1.286 TD
0.0402 Tw
(On a parallel network, the clock time required grows by a factor between)Tj
/TT6 1 Tf
30.016 0 TD
0 Tw
(N)Tj
/TT4 1 Tf
0.958 0 TD
(and)Tj
/TT6 1 Tf
1.735 0 TD
(N)Tj
/TT4 1 Tf
0.667 0 TD
(log)Tj
/TT6 1 Tf
1.428 0 TD
(N)Tj
/TT4 1 Tf
0.667 0 TD
0.041 Tw
[(. )-291(If this scaling result holds)]TJ
-35.471 -1.286 TD
0.1618 Tw
(larger networks and for other kinds of problems, that is good news for the future applicability of connectionist)Tj
T*
0 Tw
(techniques.)Tj
1 -2.388 TD
0.0662 Tw
(In order to get a feeling for how the learning time was affected by the number of units in the single hidden-unit)Tj
-1 -1.286 TD
0.1253 Tw
[(layer, I ran the 8-M-8 problem for different M values.  Again, these results are for quickprop, hyperbolic arctan)]TJ
T*
0 Tw
(error, 0.1 added to sigmoid-prime, and epsilon divided by the fan-in.)Tj
4.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
<00a1>Tj
/TT8 1 Tf
4.33 0 TD
(µ)Tj
/TT6 1 Tf
4.491 0 TD
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
90.09 523.37 m
521.91 523.37 l
170.07 523.37 m
170.07 540.4 l
214.05 523.37 m
214.05 540.4 l
258.03 523.37 m
258.03 540.4 l
302.01 523.37 m
302.01 540.4 l
345.99 523.37 m
345.99 540.4 l
389.97 523.37 m
389.97 540.4 l
433.95 523.37 m
433.95 540.4 l
477.93 523.37 m
477.93 540.4 l
S
BT
10 0 0 10 119.25 512.52 Tm
[(8-2-8)-4615(25)-3273(1.0)-2898(1.75)-2898(4.0)-3023(155)-3148(26)-2773(102.8)-2398(37.7)]TJ
ET
90.09 506.34 m
521.91 506.34 l
S
BT
10 0 0 10 119.25 495.49 Tm
[(8-3-8)-4365(100)-3023(2.0)-2898(1.75)-2898(2.0)-3273(42)-3398(12)-2773(21.99)-2648(5.6)]TJ
ET
90.09 489.31 m
521.91 489.31 l
S
BT
10 0 0 10 119.25 478.46 Tm
[(8-4-8)-4365(100)-3023(3.0)-2898(1.75)-2898(2.0)-3273(23)-3398(10)-2773(14.88)-2648(2.8)]TJ
ET
90.09 472.28 m
521.91 472.28 l
S
BT
10 0 0 10 119.25 461.43 Tm
[(8-5-8)-4365(100)-3023(3.0)-2898(1.75)-2898(2.0)-3273(19)-3648(9)-3023(12.29)-2648(2.0)]TJ
ET
90.09 455.25 m
521.91 455.25 l
S
BT
10 0 0 10 119.25 444.4 Tm
[(8-6-8)-4365(100)-3023(3.0)-2898(1.75)-2898(2.0)-3273(15)-3648(7)-3023(10.13)-2648(1.6)]TJ
ET
90.09 438.22 m
521.91 438.22 l
S
BT
10 0 0 10 119.25 427.37 Tm
[(8-8-8)-4365(100)-3023(3.0)-2898(1.75)-2898(2.0)-3273(12)-3648(6)-3273(8.79)-2898(1.3)]TJ
ET
90.09 421.19 m
521.91 421.19 l
S
BT
10 0 0 10 116.75 410.34 Tm
[(8-16-8)-4115(100)-3023(3.0)-2898(1.75)-2898(2.0)-3273(10)-3648(4)-3273(6.28)-2898(1.0)]TJ
ET
90.09 404.16 431.82 136.24 re
170.07 404.16 m
170.07 523.37 l
214.05 404.16 m
214.05 523.37 l
258.03 404.16 m
258.03 523.37 l
302.01 404.16 m
302.01 523.37 l
345.99 404.16 m
345.99 523.37 l
389.97 404.16 m
389.97 523.37 l
433.95 404.16 m
433.95 523.37 l
477.93 404.16 m
477.93 523.37 l
S
BT
10 0 0 10 82 380.28 Tm
0.031 Tw
(The most interesting result here is that the learning time goes down monotonically with increasing M, even when)Tj
-1 -1.286 TD
0.0403 Tw
(M is much greater than N. Some researchers have suggested that, beyond a certain point, it actually makes learning)Tj
T*
0.0356 Tw
[(slower if you add more hidden units. )-285.4(This belief probably came about because in standard backprop, the additional)]TJ
T*
0.0301 Tw
[(hidden units tend to push the output units deeper into the flat spot.  Of course, on a serial simulation, the clock time)]TJ
T*
0 Tw
(may increase as more units are added because of the extra connections that must be simulated.)Tj
/TT2 1 Tf
11 0 0 11 72 292.67 Tm
(3.6. The Complement Encoder Problem)Tj
/TT4 1 Tf
10 0 0 10 82 279.81 Tm
0.0342 Tw
(As I mentioned earlier, the standard encoder problem has the peculiar feature that only one of the connections on)Tj
-1 -1.286 TD
0.0145 Tw
[(the input side is active for each of the training patterns.  Since the quickprop scheme is based on the assumption that)]TJ
T*
0.0463 Tw
(the weight changes are not strongly coupled to one another, we might guess that quickprop looks better on encoder)Tj
T*
0.1345 Tw
[(problems than on anything else.  To test this, I ran a series of experiments on the 10-5-10 complement encoder)]TJ
T*
0.1565 Tw
[(problem, in which each of the input and output patterns is a string of one-bits, with only a single zero.  If the)]TJ
T*
0 Tw
(standard encoder is unusually easy for quickprop, then the complement encoder should be unusually hard.)Tj
1 -2.388 TD
0.2204 Tw
(The 10-5-10 complement encoder problem was run for each of the following learning algorithms: standard)Tj
-1 -1.286 TD
0.1205 Tw
(backprop, backprop with 0.1 added to the sigmoid-prime function, the same with the hyperbolic arctangent error)Tj
T*
0.0254 Tw
[(function, and quickprop with hyperbolic arctan error.  In each case 25 trials were run, and a quick search was run to)]TJ
T*
0.0403 Tw
[(determine the best learning parameters for each method.  Epsilon values marked with an asterisk are divided by the)]TJ
T*
0.147 Tw
[(fan-in. )-398(These results are summarized in the table below; for comparison, the rightmost column shows the time)]TJ
T*
0 Tw
(required by each method for the normal encoder problem.)Tj
ET
endstream
endobj
51 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
/TT8 45 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
53 0 obj
<<
/Length 6488
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(14)Tj
-16.449 -3.999 TD
(Method)Tj
/TT7 1 Tf
11.134 0 TD
<00a1>Tj
/TT8 1 Tf
3.348 0 TD
(µ)Tj
/TT4 1 Tf
0.826 0 TD
(or)Tj
/TT7 1 Tf
1.083 0 TD
<005f>Tj
/TT6 1 Tf
3.564 0 TD
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1177.3(Normal)]TJ
ET
0 G
0 J 0 j 0.5 w 10 M []0 d
1 i 
76.08 702.97 m
535.92 702.97 l
228.06 702.97 m
228.06 720 l
272.04 702.97 m
272.04 720 l
316.02 702.97 m
316.02 720 l
360 702.97 m
360 720 l
403.98 702.97 m
403.98 720 l
447.96 702.97 m
447.96 720 l
491.94 702.97 m
491.94 720 l
S
BT
10 0 0 10 80.07 692.12 Tm
[(Standard. BP)-11095.2(0.9)-3148(0.1)-3148(1.0)-3023(656)-2898(140)-2523(334.4)-2148(129.0)]TJ
ET
76.08 685.94 m
535.92 685.94 l
S
BT
10 0 0 10 80.07 675.09 Tm
[(BP, Sig-Prime+0.1)-8780.2(1.1)-3148(0.0)-3148(1.0)-3023(462)-2898(111)-2523(196.2)-2398(74.0)]TJ
ET
76.08 668.91 m
535.92 668.91 l
S
BT
10 0 0 10 80.07 658.06 Tm
[(BP, Sig-Prime+0.1, Hyper Err)-4004.3(0.15)-2898(0.6)-3148(1.0)-3023(267)-3148(73)-2773(134.9)-2398(58.7)]TJ
ET
76.08 651.88 m
535.92 651.88 l
S
BT
10 0 0 10 80.07 641.03 Tm
[(QP, Hyper Err)-10068.8(0.01*)-2648(2.5)-3148(2.0)-3023(103)-3148(36)-3023(63.6)-2398(14.01)]TJ
ET
76.08 634.85 m
535.92 634.85 l
S
BT
10 0 0 10 80.07 624 Tm
[(QP, Hyper Err, Symmetric)-5430.6(5.0*)-2648(1.15)-2898(0.5)-3273(31)-3398(15)-3023(20.8)-2648(20.8)]TJ
ET
76.08 617.82 459.84 102.18 re
228.06 617.82 m
228.06 702.97 l
272.04 617.82 m
272.04 702.97 l
316.02 617.82 m
316.02 702.97 l
360 617.82 m
360 702.97 l
403.98 617.82 m
403.98 702.97 l
447.96 617.82 m
447.96 702.97 l
491.94 617.82 m
491.94 702.97 l
S
BT
10 0 0 10 82 593.94 Tm
0.0615 Tw
[(For each method tried, learning the complement encoder took more than twice as long as the normal encoder.  I)]TJ
-1 -1.286 TD
0.011 Tw
(believe that this is because, in the complement encoder, the information gathered during each trial is unable to affect)Tj
T*
0.0556 Tw
[(the one weight to which it is most relevant, so such information can have only an indirect effect.  In the quickprop)]TJ
T*
0 Tw
(case, it takes over 4 times as long to learn the complement encoder as to learn the normal encoder.)Tj
1 -2.388 TD
0.1732 Tw
(The last row in the table shows the time required using quickprop, hyperbolic arctan error, and units whose)Tj
-1 -1.286 TD
0.0565 Tw
[(activation function is symmetric around zero, ranging from -0.5 to +0.5 rather than from 0.0 to 1.0.  The input and)]TJ
T*
0 Tw
(output patterns, and the thresholds used to detect successful learning are shifted down by 0.5 as well.)Tj
1 -2.388 TD
0.1338 Tw
[(Stornetta and Huberman )133.8([15] advocate the use of such symmetric units.  The empirical results they report are)]TJ
-1 -1.286 TD
0.0152 Tw
[(sketchy, but they claim speedups ranging from 10% to 50%, depending on the problem. )-265.8(It occurred to me that, with)]TJ
T*
0.0643 Tw
(symmetric units, the encoder and complement encoder problems would be equivalent, but it was not clear whether)Tj
T*
0.119 Tw
(this meant that the complement encoder would be learned four times faster or that the regular encoder would be)Tj
T*
0.0102 Tw
[(learned four times slower.  As the table shows, the result is between the two extremes, but closer to the speed for the)]TJ
T*
0 Tw
(normal encoder.)Tj
1 -2.388 TD
0.0204 Tw
(It seems likely that the normal encoder is one of the few problems that is well-suited to the asymmetric activation)Tj
-1 -1.286 TD
0 Tw
(functions, and that symmetric activation should be used for most problems.)Tj
/TT2 1 Tf
11 0 0 11 72 344.67 Tm
(3.7. The Exclusive-Or Problem)Tj
/TT4 1 Tf
10 0 0 10 82 331.81 Tm
0.0864 Tw
(I have argued that the XOR problem receives too much attention in learning-speed studies, but since this is the)Tj
-1 -1.286 TD
0.025 Tw
[(most popular benchmark at present, I felt that I must try XOR using these new methods.  In this problem, even with)]TJ
T*
0.034 Tw
(very conservative parameter settings, there were still some trials that got caught in a local minimum and showed no)Tj
T*
0 Tw
(signs of convergence, so I used the restart method described in section 2.4.)Tj
1 -2.388 TD
0.0722 Tw
(Using quickprop with hyperbolic arctan error, epsilon divided by fan-in, and symmetric activation, and with the)Tj
-1 -1.286 TD
0 Tw
(restart limit set at 40 epochs, I got the following result for XOR with two hidden units:)Tj
4.113 -2.009 TD
[(Problem)-3337.3(Trials)]TJ
/TT7 1 Tf
12.071 0 TD
<00a1>Tj
/TT8 1 Tf
4.33 0 TD
(µ)Tj
/TT6 1 Tf
4.491 0 TD
(r)Tj
/TT4 1 Tf
3.676 0 TD
[(Max)-2648(Min)-1871(Average)-1691.3(S. D.)]TJ
ET
90.09 230.22 m
521.91 230.22 l
170.07 230.22 m
170.07 247.25 l
214.05 230.22 m
214.05 247.25 l
258.03 230.22 m
258.03 247.25 l
302.01 230.22 m
302.01 247.25 l
345.99 230.22 m
345.99 247.25 l
389.97 230.22 m
389.97 247.25 l
433.95 230.22 m
433.95 247.25 l
477.93 230.22 m
477.93 247.25 l
S
BT
10 0 0 10 119.52 219.37 Tm
[(XOR)-4392.7(100)-3023(4.0)-2898(2.25)-2898(1.0)-3273(66)-3398(10)-2773(24.22)-2398(16.3)]TJ
ET
90.09 213.19 431.82 34.06 re
170.07 213.19 m
170.07 230.22 l
214.05 213.19 m
214.05 230.22 l
258.03 213.19 m
258.03 230.22 l
302.01 213.19 m
302.01 230.22 l
345.99 213.19 m
345.99 230.22 l
389.97 213.19 m
389.97 230.22 l
433.95 213.19 m
433.95 230.22 l
477.93 213.19 m
477.93 230.22 l
S
BT
10 0 0 10 82 189.31 Tm
(In 100 trials there were 14 restarts.  The median learning time was 19 epochs.)Tj
0 -2.388 TD
0.0181 Tw
(The time of 24.22 epochs compares very favorably with other times reported for this problem using backprop and)Tj
-1 -1.286 TD
0.1373 Tw
[(backprop-like algorithms.  Jacobs )137.3([5] reports a time of 538.9 \(plus one failure\) for standard backprop and 250.4)]TJ
T*
0.0257 Tw
[(\(plus two failures\) for his "delta-bar-delta" algorithm.  Tesauro and Janssens )25.7([16] report an time of 95 epochs, using)]TJ
T*
0.1305 Tw
[(their rate-based averaging function.  Watrous )130.5([17] reports a learning time of 3495 epochs for a slightly different)]TJ
T*
0.1544 Tw
(version of XOR that uses a single hidden unit and some additional connections; his "BFGS" method learns the)Tj
T*
0.031 Tw
[(problem in 36 epochs, but this involves a very expensive non-local computation for each update epoch. )-283(Rumelhart,)]TJ
T*
0.0914 Tw
[(Hinton, and Williams )91.4([9] report that Yves Chauvin got learning times around 245 for the XOR problem with two)]TJ
T*
0.0247 Tw
[(hidden units.  Of course, all of these studies used slightly different success criteria, usually stricter than the criterion)]TJ
ET
endstream
endobj
54 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
/TT7 30 0 R
/TT8 45 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
56 0 obj
<<
/Length 2864
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(15)Tj
-22.9 -3.6 TD
(I used.)Tj
/TT2 1 Tf
12 0 0 12 72 676.3 Tm
(4. Conclusions and Future Work)Tj
/TT4 1 Tf
10 0 0 10 82 663.44 Tm
0.0887 Tw
(On the problems tested to date, the learning algorithms described here run much faster than standard backprop,)Tj
-1 -1.286 TD
0.0967 Tw
[(and scale up much better. )-347.3(These results are encouraging, but are not conclusive because we have only tested the)]TJ
T*
0.0217 Tw
[(new techniques against a very small, and perhaps atypical, set of benchmark problems. )-271.3(The most important task for)]TJ
T*
0.1208 Tw
(the immediate future is to apply the new algorithms, perhaps with some additional modifications, to a variety of)Tj
T*
0.0278 Tw
[(additional benchmarks and then to some real-world applications.  If the speedup and scaling results hold up in these)]TJ
T*
0.0779 Tw
(tests, then we will have achieved something of a breakthrough, especially when these algorithms are implemented)Tj
T*
0 Tw
(on the fastest available machines.)Tj
1 -2.388 TD
0.0016 Tw
(The development of these new algorithms was driven not by a theoretical analysis, but by observing problems that)Tj
-1 -1.286 TD
0.0433 Tw
[(occurred in standard back-propagation learning and by attempting to cure these problems one by one.  The result is)]TJ
T*
0.0487 Tw
[(admittedly something of a patchwork.  Now that we have seen what can be accomplished, it would be useful to try)]TJ
T*
0.0208 Tw
[(to develop a theoretical understanding of some of these tricks.  For example, it might be possible to develop a better)]TJ
T*
0.0071 Tw
[(understanding of the expected performance of quickprop for various classes of problems.  This sort of understanding)]TJ
T*
0 Tw
(should ultimately lead us to more elegant and faster learning algorithms.)Tj
1 -2.388 TD
0.2761 Tw
(Among the issues that will become more important in the future are incremental learning -- adding new)Tj
-1 -1.286 TD
0.0265 Tw
(knowledge to a network that has already been trained -- and the development of networks that handle recognize and)Tj
T*
0.0589 Tw
[(produce time-varying sequences of inputs, rather than individual patterns.  It will be interesting to see whether any)]TJ
T*
0 Tw
(of these learning techniques are applicable in these more complex domains.)Tj
/TT2 1 Tf
12 0 0 12 72 398.8 Tm
(Acknowledgments)Tj
/TT4 1 Tf
10 0 0 10 82 385.94 Tm
0.0514 Tw
(I would like to thank Geoff Hinton for sparking my interest in networks of this kind and Dave Touretzky for his)Tj
-1 -1.286 TD
0.0468 Tw
[(persistent efforts to promote an active interchange of ideas within the local connectionist community.  David Plaut,)]TJ
T*
0.0123 Tw
(Mark Derthick, Barak Pearlmutter, and Kevin Lang offered valuable suggestions and helped to acquaint me with the)Tj
T*
0 Tw
(folklore of back-propagation learning systems.)Tj
ET
endstream
endobj
57 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
59 0 obj
<<
/Length 3990
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(16)Tj
/TT2 1 Tf
12 0 0 12 72 711.96 Tm
(References)Tj
/TT4 1 Tf
10 0 0 10 72 694.05 Tm
[([1])-1834(Becker, S. and LeCun, Y.)]TJ
3 -1.105 TD
(The Feasibility of Applying Numerical Optimization Techniques to Back-Propagation.)Tj
T*
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings, 1988 Connectionist Models Summer School)Tj
/TT4 1 Tf
22.9966 0 TD
0.25 Tw
[(. Morgan-Kaufman, )250(1988.)]TJ
-24.0796 -1.105 TD
0 Tw
(\(to appear\).)Tj
-3 -1.791 TD
[([2])-1834(Cater, J. P.)]TJ
3 -1.105 TD
(Successfully Using Peak Learning Rates of 10 \(and greater\) in Back-Propagation Networks with the)Tj
1.5 -1.105 TD
(Heuristic Learning Algorithm.)Tj
-1.5 -1.105 TD
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings of the IEEE International Conference on Neural Networks)Tj
/TT4 1 Tf
28.606 0 TD
(, pages 645-651.  San Diego, CA,)Tj
-28.189 -1.105 TD
(1987.)Tj
-4.5 -1.791 TD
[([3])-1834(Fahlman, S. E. and Hinton, G. E.)]TJ
3 -1.105 TD
(Connectionist Architectures for Artificial Intelligence.)Tj
/TT6 1 Tf
T*
(IEEE Computer)Tj
/TT4 1 Tf
6.666 0 TD
(20\(1\):100-109, January, 1987.)Tj
-9.666 -1.791 TD
[([4])-1834(Franzini, M. A.)]TJ
3 -1.105 TD
(Speech Recognition with Back Propagation.)Tj
T*
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings, Ninth Annual Conference of IEEE Engineering in Medicine and Biology Society)Tj
/TT4 1 Tf
37.6016 0 TD
0.25 Tw
(. 1987.)Tj
-41.6846 -1.791 TD
0 Tw
[([5])-1834(Jacobs, R. A.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Increased rates of Convergence Through Learning Rate Adaptation)Tj
/TT4 1 Tf
27.1353 0 TD
(.)Tj
-27.1353 -1.105 TD
(Technical Report COINS TR 87-117, University of Massachusetts at Amherst, Dept. of Computer and)Tj
1.5 -1.105 TD
(Information Science, Amherst, MA, 1987.)Tj
-4.5 -1.791 TD
[([6])-1834(LeCun, Y.)]TJ
3 -1.105 TD
(Une procedure díapprentissage pour reseau a seauil assymetrique \(A learning procedure for asymmetric)Tj
1.5 -1.105 TD
(threshold network\).)Tj
-1.5 -1.105 TD
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings of Cognitiva 85)Tj
/TT4 1 Tf
11.4707 0 TD
(, pages 599-604.  Paris, 1985.)Tj
-15.5537 -1.791 TD
[([7])-1834(Parker, D. B.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Learning-Logic)Tj
/TT4 1 Tf
6.2778 0 TD
(.)Tj
-6.2778 -1.105 TD
(Technical Report TR-47, Massachusetts Institute of Technology, Center for Computational Research in)Tj
1.5 -1.105 TD
(Economics and Management Science, Cambridge, MA, 1985.)Tj
-4.5 -1.791 TD
[([8])-1834(Parker, D. B.)]TJ
3 -1.105 TD
(Optimal ALgorithms for Adaptive Networks: Second Order Back Propagation, Second Order Direct)Tj
1.5 -1.105 TD
(Propagation, and Second Order Hebbian Learning.)Tj
-1.5 -1.105 TD
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings of the IEEE International Conference on Neural Networks)Tj
/TT4 1 Tf
28.606 0 TD
(, pages 593-600.  San Diego, CA,)Tj
-28.189 -1.105 TD
(1987.)Tj
-4.5 -1.791 TD
[([9])-1834(Rumelhart, D. E., Hinton, G. E., and Williams, R. J.)]TJ
3 -1.105 TD
(Learning Internal Representations by Error Propagation.)Tj
T*
(In Rumelhart, D. E. and McClelland, J. L. \(editor\),)Tj
/TT6 1 Tf
20.553 0 TD
(Parallel Distributed Processing: Explorations in the)Tj
-19.053 -1.105 TD
(Microstructure of Cognition)Tj
/TT4 1 Tf
11.333 0 TD
(, chapter 8. MIT Press, Cambridge, MA, and London, England, 1986.)Tj
-15.833 -1.791 TD
[([10])-1334(Rumelhart, D. E. and McClelland, J. L.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Parallel Distributed Processing: Explorations in the Microstructure of Cognition.)Tj
/TT4 1 Tf
T*
(MIT Press, Cambridge, MA, and London, England, 1986.)Tj
-3 -1.791 TD
[([11])-1334(Minsky, M. L. and Papert, S.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Perceptrons.)Tj
/TT4 1 Tf
T*
(MIT Press, Cambridge, MA, and London, England, 1969.)Tj
-3 -1.791 TD
[([12])-1334(Plaut, D. C., Nowlan, S. J., and Hinton, G. E.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Experiments on Learning by Back-Propagation)Tj
/TT4 1 Tf
18.9966 0 TD
(.)Tj
-18.9966 -1.105 TD
(Technical Report CMU-CS-86-126, Carnegie-Mellon University, Computer Science Dept., Pittsburgh, PA,)Tj
1.5 -1.105 TD
(1986.)Tj
ET
endstream
endobj
60 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
/TT6 11 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
62 0 obj
<<
/Length 1914
>>
stream
BT
/TT4 1 Tf
10 0 0 10 301 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(17)Tj
-22.9 -3.6 TD
[([13])-1334(Pomerleau, D. A., Gusciora, G. L., Touretzky, D. S, and Kung, H. T.)]TJ
3 -1.105 TD
(Neural network simulation at Warp speed:  How we got 17 million connections per second.)Tj
T*
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings of the IEEE International Conference on Neural Networks)Tj
/TT4 1 Tf
28.606 0 TD
[(. )-250(San Diego, CA, 1988.)]TJ
-29.689 -1.105 TD
(\(to appear\).)Tj
-3 -1.791 TD
[([14])-1334(Schmidhuber, J.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Accelerated Learning in back-Propagation Nets)Tj
/TT4 1 Tf
19.2739 0 TD
(.)Tj
-19.2739 -1.105 TD
(Technical Report , Technische Universitaet Muenchen, Institut Fuer Informatik, Munich, Federal Republic)Tj
1.5 -1.105 TD
(of Germany, 1988.)Tj
-4.5 -1.791 TD
[([15])-1334(Stornetta, W. S., and Huberman, B. A.)]TJ
3 -1.105 TD
(An Improved Three-Layer Back-Propagation Algorithm.)Tj
T*
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings of the IEEE International Conference on Neural Networks)Tj
/TT4 1 Tf
28.606 0 TD
(, pages 637-644.  San Diego, CA,)Tj
-28.189 -1.105 TD
(1987.)Tj
-4.5 -1.791 TD
[([16])-1334(Tesauro, G. and Janssens, B.)]TJ
3 -1.105 TD
(Scaling Relationships in Back-Propagation Learning.)Tj
/TT6 1 Tf
T*
(Complex Systems 2)Tj
/TT4 1 Tf
7.971 0 TD
(:39-44, 1988.)Tj
-10.971 -1.791 TD
[([17])-1334(Watrous, R. L.)]TJ
3 -1.105 TD
(Learning Algorithms for Connectionist Networks: Applied Gradient Methods for Non-Linear Optimizaiton.)Tj
T*
(In)Tj
/TT6 1 Tf
1.083 0 TD
(Proceedings of the IEEE International Conference on Neural Networks)Tj
/TT4 1 Tf
28.606 0 TD
(, pages 619-627.  San Diego, CA,)Tj
-28.189 -1.105 TD
(1987.)Tj
-4.5 -1.791 TD
[([18])-1334(Werbos, P. J.)]TJ
/TT6 1 Tf
3 -1.105 TD
(Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences)Tj
/TT4 1 Tf
34.6885 0 TD
(.)Tj
-34.6885 -1.105 TD
(PhD thesis, Harvard University, 1984.)Tj
ET
endstream
endobj
63 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT4 5 0 R
/TT6 11 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
65 0 obj
<<
/Length 1199
>>
stream
BT
/TT4 1 Tf
10 0 0 10 304.61 749.14 Tm
0 g
/GS1 gs
0 Tc
0 Tw
(i)Tj
/TT2 1 Tf
12 0 0 12 260.33 711.96 Tm
(Table of Contents)Tj
11 0 0 11 88.5 700.28 Tm
(1. Introduction)Tj
40.5455 0 TD
(1)Tj
-40.5455 -1.0618 TD
(2. Methodology)Tj
40.5455 0 TD
(2)Tj
10 0 0 10 107 677.8 Tm
[(2.1. What Makes a Good Benchmark?)-26493.8(2)]TJ
0 -1.08 TD
[(2.2. The Encoder/Decoder Task)-29328.3(3)]TJ
T*
[(2.3. When is the Learning Complete?)-26939.2(4)]TJ
T*
[(2.4. How Should We Report Learning Times?)-23299(5)]TJ
T*
(2.5. Implementation)Tj
42.8 0 TD
(6)Tj
11 0 0 11 88.5 622.92 Tm
[(3. Experiments and Results)-28878(7)]TJ
10 0 0 10 107 612.12 Tm
[(3.1. Tuning of Backprop Learning Parameters)-22994.3(7)]TJ
0 -1.08 TD
[(3.2. Eliminating the "Flat Spot")-29133(8)]TJ
T*
[(3.3. Using a Non-Linear Error Function)-25772.2(9)]TJ
T*
[(3.4. The Quickprop Algorithm)-29271.2(10)]TJ
T*
[(3.5. Scaling Experiments)-31744.3(12)]TJ
T*
[(3.6. The Complement Encoder Problem)-25357.6(13)]TJ
T*
[(3.7. The Exclusive-Or Problem)-29107.1(14)]TJ
11 0 0 11 88.5 535.64 Tm
[(4. Conclusions and Future Work)-26070.8(15)]TJ
0 -1.0618 TD
(Acknowledgments)Tj
40.0455 0 TD
(15)Tj
-40.0455 -1.0618 TD
(References)Tj
40.0455 0 TD
(16)Tj
ET
endstream
endobj
66 0 obj
<<
/ProcSet [/PDF /Text ]
/Font <<
/TT2 4 0 R
/TT4 5 0 R
>>
/ExtGState <<
/GS1 6 0 R
>>
>>
endobj
6 0 obj
<<
/Type /ExtGState
/SA false
/SM 0.02
/OP false
/op false
/OPM 1
/BG2 /Default
/UCR2 /Default
/HT /Default
/TR2 /Default
>>
endobj
67 0 obj
<<
/Type /FontDescriptor
/Ascent 750
/CapHeight 676
/Descent -250
/Flags 262178
/FontBBox [-168 -218 1000 935]
/FontName /Times-Bold
/ItalicAngle 0
/StemV 133
/XHeight 461
/StemH 139
>>
endobj
68 0 obj
<<
/Type /FontDescriptor
/Ascent 750
/CapHeight 662
/Descent -250
/Flags 34
/FontBBox [-168 -218 1000 898]
/FontName /Times-Roman
/ItalicAngle 0
/StemV 84
/XHeight 450
/StemH 84
>>
endobj
69 0 obj
<<
/Type /FontDescriptor
/Ascent 750
/CapHeight 653
/Descent -250
/Flags 98
/FontBBox [-169 -217 1010 883]
/FontName /Times-Italic
/ItalicAngle -15
/StemV 76
/XHeight 441
/StemH 76
>>
endobj
70 0 obj
<<
/Type /FontDescriptor
/Ascent 701
/CapHeight 0
/Descent -298
/Flags 4
/FontBBox [-167 -299 1094 827]
/FontName /DOHJPA+Symbol
/ItalicAngle 0
/StemV 0
/FontFile2 71 0 R
>>
endobj
71 0 obj
<<
/Filter /FlateDecode
/Length 20699
/Length1 40112
>>
stream
Hâ‰W}t’ø3ª3;≥;ªY9û
UH8ájE®`B≈í“¿â©iÀ°fwﬁfGf?òô%Z%%Ræ#hä©•[´)•)•‘RH
M"bãÇ¥|T@•ÑbÈ}≥≥õz<µ˛ô˜ŒÃÔ˝ﬁΩÔÕΩ˜›7Û ∏oN?ÚËΩ˜ùûW2Äπ{g-
TÃVúk|O&"ﬂ¿é)F∞x¿=£È`,#B±®r˛ªKó P˝T(^iΩÁ≠≠ OlA}®‘™C÷-∏`ı GCò» ÒÌ´ö í8~t;§=Ïs ÛQÜÑ#frµªº˘;®_x¬0WÕC :QÆi±†LZïC œ7£MœD‰d\8"æâÛ=ÜÚÇ1ÂCâ/	 µ
Ä∞1*G»î¢!Äu®ÔàÎ$~±†_¿r‘.≈ıXºzﬂÃa ı›(ˇ*8úıÃJ‡¿…ngó‚åÀ”»É˚†r
ö¯Q3Î√[ã”<^≠ÄÌŒÆ·j øûUhsÜ‹¿ø@9˚·XKÿÒ0Z`&úcö‡Yfgñ†ÙÿM–Ü◊≥ ÂhÅn8€ö¡Ó≈!	ÿò®’ ,;êM`ˇj¶|Ïx∂ëYÏ|fLá|∏ÀŸó·1ÿ‚à@!ºÃŒÑ«ùYpéÚ3Z0ÅùÃ≤‚∏‚ì∏∞6C
"é3ÏLW	t2	√18	£`Ä`ó¿8»le3Ô2ÂÏÿ≈t3˚ôRé÷pz“¬mÖ€Ÿ9Ä)∑√xp:“ÚRîÁ√›h?ΩBË˚^n=˙?Ö9è£F¬ZXÖ˝k°î€ålçc8ZR@÷ªëØAVÀï¡.x · 7–¬'¯Yñl´£â)ƒV-◊∆Pæ∆–ﬁgÚca∞Û.v&øÁ⁄Ã=»f´†Æ∞µ‹Ê¨‡Rè~LWÀòtL&C9∑%∑bd#Œ¿…áÀ\9º∆g˝∏6ØdcÛ:wÜïÿIºÁ∏ÛL's/ˇæâÈ‰Ä≠Öfˇ t3Ö¸f,Ô„√¯‹ Fp·˜{Ê/ ‹~C—±ÒËÙt®Çeò#!Åãp7⁄X‹M'ÄÂJaêùÉ\Ueÿ§Ì*≈Sƒ≠oRA,nÃ	 *k!+cYºx†Àq¶ñL~W=Óì¥:ài–”GãÁx6"> ?æSº;†œF#]Ú_»∆§¶^+&ŒÑæì)˚≤1¡ù5äÊ…√}=&!-7¿¸>ãe}=&∆†lL˙|{=âV˜ıò‡G)œY˜cûLÈÎ1Iü$›ﬂ˘bü_?˝¶c(=Ö›ÂÃT#ûZÔ¥òzÊ§… 3ı⁄´ÃT˙Õ«‚∞N¯d∫∏2∏sq„;ﬂp¯ƒÔ!æÛÒÄŸãπú-W+>-∫Öıé/íI÷ºü≥ ì9k`©©·£1S!°⁄∏¿∆Zü≤q°ç?≤qëçãm\b„RóŸ∏‹∆6÷Ÿ¯¥ç+m\e„36>kcΩç?∂qµçœŸ∏∆∆Ám\k„:l|¡∆ı6æh„ÁƒÑ˚â05n®Z,:Ú%>¢F&˘©®√ s≤∂QÈr–Tc—ü	j4§FU≥:≈á¥òÆFÓjâ¿À.Eï#¯˜√tÛŒàÀ
yUîu=Vàô·∆tS#!Ûó.´ôàor[]≈„„Ø“r%V›Ã+§R'‰◊b\K‘£â7HßˇçÑŸ$∫e◊!í–L5ÆUˇV¢ø_1ù⁄(k[=q¥Aï5EÖ~«öFÃmº¢ŒUÚ{˝∂ÜøÊAPÁ â…<rÁHZíÌ—4b¸—cô5óË&IÓê,FøÁë‰N_©*W¢5fBè6s8S<‹"<Ñ—öç= Ì÷.OQânò∫lª}AUj$c˚ü›iNùmH$nVƒlì‘(˙ânSü⁄πD·uu{ÙFÃ=>ùÑ4íÃΩ"˙f$ÿ‹'eT)€/e){√EÒ$jp”X§€orr¥R#*uYQ±„Ø^ùT™ÜeÑÆÜ˙Ç¯Ék≠ï≈ﬂÚ°7
â»˙lãøÌ¬*â†y»E' ⁄aó3#≤~«≠≈*i>ÌH¶-Gïøâv;¶ˇ=Z%†—d9öe4_éπ3,?ÓÕ¥-KNdi‚t∏¥ÿ<≠$'EÀ:¯îîıBéÔz{ù@˙û∑◊§ÔãF"Çc§O”"Q:É?”KHÚl/	òx∏3f3≠¯a.%…èri¿<Á°î§Uˇ!eIDUŒ˜äÊ?]!…Ê§F.∫-o,´?hZ‡ió‹ôñøúmì‰ø≤ÌÄyE≤lµFöÒOrIvÊ∞Ä˘oüm™≠€u'…OØ„≥;mæ≠}’€À–õk9¬ÄŸ„˙Z˙MìyÉãks+Ä”Óa._/π±¶G:Å´Ñ⁄ΩíÓ‚π
ÆÇ2ßu9ulﬂTù:ˆ∑”Í˙≤–Ó©s7zÍƒ=b´4ZLyÍ∏
wÏÎÚ}›SÁ©ì <GÖ”bRlı∏]ºTÊ.ó∂â≠\Ö˜‘[å≥ï∏˚[3ùuÒ¬4æ^Lq%bäˆàE¬4Î)xS.^,ÚÑ›]Æ˜:Î⁄‰∏ﬂµ	˚?”√ˇ•zŒäÁóÖv∂ëFA≤ãø1oèò¢⁄˘zjá'åœùëˆ[h˜ÉòÚÉ{±wòªK⁄ñ∑G*ì º√Ë›Œ≠îK•çæoxá—àz¬b
GfÁwùÂJ®\™˜Ñ˘!ΩüeUi#W‚∏ªhΩYû^«ï÷!Ú‡pÚ è{ø˚Ï+áœ_Ú;˛Kg¡M=¯£T‹Àòœú≤ Ü⁄m˚ä€ú\‹≠Á]¯?LƒíE7vÁdï;l5Yü<M[Á(Õo¶∑€∆8pN?b˛©›≤ÙÕ›COñ./ì]jû¯‰•Äö‹©¢è‰2U:òº£sÚ€Ú;ª∑nÈ<Û‚•;w^lñÚèËÅì®0†ÊañŒŸüﬁ:¨Èã`q'8˙7;È¡Ã*Cæ£q◊€62˛]ˇ°æl`õ8œ8˛º∂/ÊüL »ŸπAÄÖ +Ì2rqú‡0Z2
*Ì:ëÑO≠kõÅÜ†•‰54à$Ñ
j eì:TF◊™*ùúè`@[Z	&¢…@W*–®C∑5∆ﬁsgõ§àêê–ﬁW˜·˜yÔy~ˇÁ˝∏ÛèÒŒúÓ†∑™3.…jäÒò¨R¥Ë•J•ªF,	z‹˘Â¢ãÏ^œ&˙	ˆ±L∂ıH·˜ø–}±‡2∑‡*˝„$π£u÷ë1ã¢{wkì•YØç7˙í4’`tÍaÓeX≠ ˘≠4ıR‡=©ÅDF∂k}∂`a¡!åTyøó’‰êπ»àÅÌ~éåø®QF £‚'¸:uî	´˘ù:#*ﬁPÓ…?NÆloÒ˚eæ“Ñtº—i0⁄5Ÿ`‰∫c«au+∞j?@J≈t∆∆ #=æy∏Ωı¯2G⁄ó€î/w¶°LvœÁ;≤	»XwUqªêQ≤∂ÂÆÀØ]ûBF>Úiı¡.U”FxÇ5ı°?.ì*v®ïÌØVÊèü;ã€≥‘P∞Qq!c÷ß’/N˙UG®€>È–ıml„á_î(TEd¨uPK(B»•‚EX£ŒËØÿÂ9s4ﬂS§t.ØõæF∞+;îmÚ,∫ÖzïƒdW∂*ÕÌcgí—!6Y◊ûk! £∏¢º∆“R®◊=£Éniœœ=∂9zØπ2¬gáπµ@›≤.‚”sö>ù=˛Õw ûúió.VƒÆŒŸîgw+ÖÄZ`Ô]Uª¢?Çcé¥jdføø_ éEt∆/≈œî≈ì\ﬁ.Uñv–‘F	˘6yÆùv"`◊À›[ÂÊ=»Ë¨Î–Àƒú“&+˝—r32:BNiÉˇêXÁRπØ&Ëoœó¸’PÀVÃ„AŒe5§i>}¨ÌRsπ£¬ÛËx´]Ë¨pUK»X¢‘{ñˆ´¢£FÆΩ6–~˝§
P∞∑ºéƒíÛqöyéøÊ°3ıUñDÅÄ,€«é4ù›Ìkd'⁄‚è'>¥˚.»ÕndTíåæghS1wlzN¢∏Ÿ!·R
5Äyˆ≈n≈Á/\Üår®•©2?ﬂ) #“az∆◊Àögz*&:?‡Ó†2Oaﬁ◊Æ·FJƒê1ÏÛ«ÛÆ5íDúπ®„óã'çdFFÏ`≈±Ÿ€ò·QÃ„o~Ê€™h‘»c_2è.—Í§A≥Cœ£l;Ö0è .B∆∏f6⁄˘Æ&}ÕH»8 ¿ìåÂdú`∑õºi∆∑®áEUÿ”õ¯Êz:èﬁ8TLb\Î˙ÜN«î÷£cL∞[5≤fΩË2jV{ûë«õ<-;ÄÎÛqAzÕTùt„…Mí†.,\ÿ:ÁDÊ€8UAYñüÔ$ˇ_GÚP„q‹{
|‹’\%ØªÙ˘xp◊Û„7xÊUÏyºl…O}Ì‹9vkx †Ø\ﬂ•˚k/ÃOÕG}ŸõìJ˚ÖR ÒûÛß¶∂_í	¬≤£Ác“x%êK¿*uÛÒe}ÔAgú}>…ò:Î6º∏´øäGü^m©∆u=;V◊[óË5-∂¢mÀ—˛û]+NüÃ ˙:qÆ+÷UÒ¥Û·≥ga(Ú∑Œ¢„ó_w˝∫˜ ™⁄>‹{"≈a¿°&◊µ«_G‚”$êùS´œZÿ∆>ÿíõ\◊uâ†%—E!l¨k ÂhHÆΩ5byÁ^˜H±‹Mm˜Ú‡õÏ∆ÕÕÖ$wD}∑ì
ÅräRj£CÉú°|€Å §Ú¶ß]8“EKXj#C¬Ë.s˚ëQÂ›d“‰twºππdaÙhT‘n0*rEíë2¬êÿ©oä4£≠H‚ña$„Ÿñ´ ˘#9bπî6ÑølH…“Oàÿ" Uß·„4π∂?	£2™ÉOß:‹6å·NhXõx—(€·™;@ªìB¿
vÅ9aLÅáëœãØÏôÃÖ'a,Ügªf%¨Å º!ÿM–
m∞ˆ¡ª>t√_‡úÖÛpÆ¿ø!N,ƒA y‰ád
yÑxIôCÊìÚYI^"	ëWHŸIﬁ ˚»€‰ ˘3È"«»«‰4È%»%rï¸áj°6Í§9‘M«–ÈCT•Ât6ùGü¢?ßUt…=®œ—ïÙ†Ë˙*m£ø£o—˜h˝Ñû°_––>⁄OÃÃ2Ÿ˜ÿLfcŸD6ï±Ãœû`ÿRˆ<[ÕÍŸ+¨Öµ±7Ÿ;Ï˚êùdßYÑ]dWÿ ß\‰√πõO‡ì˘˛^∆ÂOÒ•¸9˛"ØÁ˘fæùÔ·˚¯ª¸O¸ ˇÄwN~ò„'¯'¸ÔÂ˛w~â_ÊQ˛O˛/>¿ØÛ∏âöÃ¶S&V'·âõJ∫ﬂwPg&íA,T¿ô;åò):ß"Pf-bTz„ÃPVj∆cÕ0éå‘˙£Vb•ôxo5A∑b´Ö√Xz/∂öàïËë2t_8÷fÏÉ˛à	ØB≤¶,˙U∑Co›V¶∑ﬂâ¬€ËG.TâIu:åa≈(6íÖxË
çÿÇa∑§…òÕîAlƒ DT"Íπ√ÏâÿGW>‰@≈ÜŸE‘¢G—3a¯ßzÍ5Ïÿœ<X9∆‚X—f"‹l5£7j66˝L–Üœ¡4ÍÎ8Ÿx∫pY∑„ŸåmKÓ—q_ä7≈nºzπ$„ﬂË}/¬ÌªÌ+h©{≥-ÍB-ì;yn‰s[ÏˇNãs®°@F-Û:πT’"Ú˚§ÖÎ◊›hÒ—í%®®≈_äZÓçñ‹~òvkãê~Ÿﬂ\ÓRID“}—2\◊bø7xWZF˝o≥l©®a˝ú˙√4rkï)-)£¡…∫zæ€8ú÷“é_æ5Çƒ∆ËZ≤dø◊ó‘∂t0Õ¯„—“ûp^tÈ7=˛ZBÒG£§∫ÆÆƒÖògmøTkâÆ)*éèŒ˝*ëË€z≈≈	m¥‰0ÿ5#˝/Ûı€ƒu ¸˚ﬁªÿ\úsízgúÀR¥±Í£‘_BŸñ≠ ≠?&m’9AalÃ⁄çhäíKHúdrHR:…lêâ?∫µ[°l⁄⁄?∆968U”¡¶µb“TlEÇi ’l’ÕÌΩ≥B˘•mï∂g…≤œÔ~|¸ﬁ˜˚æOcÔÓ‹ZªAÜ’hû–ñ\}€~€≤}¸∆nÎ¨n}y«≥2Ê!√Ûÿ¿^îæ·¬ñEXnôX∞©¥˛(ä&ØiùÈ·Ï&õ^∫ÃãJ¨ ≠ñTQrHb\êÇ·zª_”,FûŸ 8oëtŸ∫óeı•áL≥öòn ‡‰“Û—£\L¿“∞KÑz|:l∂«ÜóI∏ê~˘òÖÜ•V?¥2À’â∂ÊöúÂÇ8·®ez˚¢≥›⁄e¬{∫5À‚ë:≈e[ZèE˝íﬁ‡qxú˝…8Á∑˘|†l¿ëÌ;ó,,≥(Ïø∞õ,^PTÄH-∫&xB÷Øf©d›m∑-›ôzj°7*Ãz’≤Tc≠ÀÊ¸™a»À,Cú1'¥˙tf9{Û¬íÂ“ı«z€¡ôÚ	V9%x%9ã¿,JÉ‚û“cSG‹–‡qï;!"ÈÙ*äœûí`Œ»Y–πDß≠1D6ÈÛB0˜N&Ì◊/Ó#g—Ö!jëO≈‘íÒn^»à"üûìd«1ü∑ÙÑ™Î˘F}≤¥»>Ê¯B≈|∂Ôÿ1}~∂Ê;;πänZá¯£mâ≠€fIö#KñŸJŸ—fo¨+=N-Ç°ƒ˜%ç⁄ãΩ60äµR¯züoLè9ø⁄].WÃπßªCY]ërÛsF¬∆,§ˆ‚·íÆWäœ›lLGbß˚NæuÏÊklZr‰‘¢~QÛIeÅ[2/“˝e»hÄáÕHˇT6gyv§s≥xæ*(—ùtmZÈrÙéJìŒ√PÈ‘b'W—od¥‘WµuªÊï¶m&ﬂ÷\.já,À£FK$ó€∞,ïóµΩZu¥◊m*´j+£=@-f4Ó°ñ⁄2f9≥+•)^Es/<◊?hYlÂÁ\ÓÙk£c£õ¥t$ò˛c‡¬≈w \Dm~ƒ,Ç•ÙLﬂô≥*'√àéŸ¶3ËLv•«P¬≤<a”6˙ÏÂAA<πt7—Sõu£Ç•/0´˙L~bGinôP®•uõ…Yñ—GM2Ô?"zô•L0‘mΩ≤m.•Å ¬I3æbçK4˛,´ufñ.PAV|4hÕ1õ'ÓR“ØNéMπuj	æ®d2ªâ´:—mYÃ¡£‚^Z€.ÍXeQƒëEê~∫.ô7XñßıêøŒŒÔª&/]D˝"µ√Ï!7z¸êq1Â-Ïd/#;<Ã"—qIZŸf©›?Ω∏˙µ[&µËÅÃz•˝JL"Cü‰”¬JŒ≤¬eµ‚akéÅN-b%/.g·„ÇÚ∆´;®%6Ò[ñ`ﬁb´ﬁØ
0«MÛ“M´XÁ˚ÃÑ*É÷,&ÕÃpñ%ﬁEıy£¡øa’òeÄZ˙ÃAò•såÆ/≥√ÊK[ÈÃÚÕóßÈ£Ÿa»äóÀ¬‚g¯=˚ØgÇçK:.ÕöBuˆ†‰W¬VÏG„Ó~Ï<'˛¡ä}‘¬∑`¡ñ∑H‘í∂,¡ÈH]ıeTs»†nîZ˙∆Ø∆•FóëŒ∆•$?«å˝)…ﬂê¨
ÆÏWW"»Y‘6B_?—A>KW…u˘qâû'€hÌ“_dYmÕBﬁ2†Ús°ÑK“u®‰g< j	Í-ß°?ZÁ#πúóbÑZˆ[9sä†¥œQãê≥î«%j©õr˙À∆<¡”€ON4USKèpÑ≈K;8B©q3`èÔaÒ¬~dr&È∞Ê2ÆI’“†>YF‘Ud‹¬‚•MÑÕ…K!^ÜDöìi∫gñ≥À,1¨Úë©ûz	¬êé&µΩ≠A°”KÛòÇPtßõ–µíZ∫§"ÜE≈Z+ÌYE®Ë1¡£u∞<∂ˆrπª„Â ±_¯÷Tå{¸œˇ˘çhlæ˙©où?w*Fsrh—1º◊Ktç•"∫t–°ˇB;¥gâ@K√”Îõ?:B◊ó∞∫"ömß@Ûò"ÉLoIûàâ˘<VÑy∫V&‹]+YNT-Bk˛ﬁvkÿÒ¯åH-©ÎÊc◊¡ò}ŒKÔÂ7öøòf<õœ6ˆö„ŸcY±1–Î≠œfˆ¨π®»jù¶hÊ/ç° H2VQ¸OﬁﬂháX,‚ûO˝ùÆïçt∂V"–ÿA_4ˆÌY.WèYµqK<ˆXh≥=—≠Ú›}Öı≈\êiì≥4â˘ıcﬁãR`ˇ»≤‡€^`ñ€{1,^V<,ˇ|ü¶ÉUÑ‰zu 2{Xûl·œwíı5Ö˛Ùëÿc›£Í∆÷èìÛ_P:u«	VÌSrãYNÄ≥≠õYƒ%≠«˛+Ká˙PÀ˝/u∑•ƒ§„eÍÚ=ªÍ8ƒµ)ÍQpé®ÇÆE¥ÂÓÆŒjˆ;-A˝^î“7Â-≈∏V¡“Ú–ªÊ[Ó¶‘0AÊ©•/K´Êˇwö=˜Æ{!ê ü≠ﬁm1Ù–BÓ£„◊*XÓ≥£ªª,Îd’≤É&,≥¸/€C˜ï˜nàV®‘ºÍ¯ˇ±¸áq|ﬁ"|ÇL√·¯4≠ÊÎ·Û–[ÈéÒIxönf:†ˆ¬–ÉÅó‡(úÄW‡4¸~g`.¿{>¸n¿"ZÅJë=äÍP=jB_F_Cª—˜– GG–	ÙKÙ[4ÉﬁBÔ¢KË*˙∫Ö	∂„RºØ≈
n¿Õ¯¯	¸˛Ó¬ﬂ∆ﬂ≈ΩxO‡)<çé_«o‚~ˇ	ßU|/í"≤í¨""ë>ÅW˘,yå‘ì“B⁄…3§É<Gê%”‰yì¸é$…;‰]Ú>˘+˘Ä‹ YÆàsp.ÆÜ[Àm‰öπ/qOrAn˜}nàõ‰~Ã˝å˚gposÔ˝ãˆ™äÍ∫¬Áﬁwﬂ€∑?∏oRVÀÃæ]ë∂©lPà⁄Ç—‘Œ»+ôN&¢6≠Nç,∞—›¢Ä:J"#¨öV:”hgÍƒ§-†®——ôLkgÃ@Zc“éç;Q∆ü◊s/¢∆®sœû˜ˆ›üsæ{ÓÔ«>aˇbóŸMYïìd]û&œîã‰9rôºH^"◊»A9"ø!Ôí;Â∑Â˜‰cÚGrø¸©|Aæ()_ëá‰õ
QdEQúJ≤í¢xØ2Iô¢LU≤ïßîô ”JëR¢ÃGydº¨‹ë@éø¡xà2UHòâÖÿ…8P	:&f∞PE®å%f#¸ã»TX∞çJ,ê@lh¡äœqƒä™ÄÁT8ˇ8‡j¡+÷Q1«Ü~lDë4dç6¥h·∂Ã¬ñä5y{ûcç+
Ê®¬*oa#fj&åb€áÈ·˙œ=ÒﬁH¿£0ééöHùòáJT|´!«/˛qL‘…xπM≤ìl·‰}ÂD¡æ!Öã8hƒÜV®ïZI"Æ+†b<ïxúàâë0Œ
ñ‡((xµ•60…¢L<Õºn
V√h‚Îÿ‡«öq#‰êw‡~l=ñ‚ˇO.≤)Í—¬°oÕ≈Éo∑ë¶?n$⁄òsÈH\8[ƒ˚∆6f∏7n|O˘˜∫æ#!k©| "ûæ8íú∆Ωª%ÕYØ›Ø¡Ìõ‡h
‘àì«ÒõÖ&ÃÍ#*@mí« πy‹ˆßÔ0R['^À¿È·ÔEçûpå#IÓYxq6T2Hûîìˇ√sH/<ˆÉΩµπ∆∫πÔÃqß|6‘◊eú≥√èk\à$Ã∏!~m˜5©Ó6Î|PT∞Ö©Õñ¯9…ÄÙmzÙ¶‚Q†‰÷Í5ó;ËHL<yyóomG$j,ï«§ß4+í“Ã¿ıyoM·¢y=¨4‰—¢æΩ`ºŸÈÙïµÑµÆ›~U !∑ëÿ48Æ´%uÌéûÑêÊiU“!k∏éﬁlB$«&,ÛóUeñ∆„Fi}Ëµ≈ûQ$ÂÅ¿‚ "ôz^ I¸Àµ¨(›÷Æˆ–æ‡.ÕÂRi®ƒ‚[[üñ·ÏçZííäµ≤Å ›ıàdÈº3:~ÒÉ™¥7wı˝ywq[öY◊TeJÛ·∂S∫âÏÙ§“}˘£Lñ∂.›¯LUÊÊ˜›}áØ\π⁄œäVe-0kAqñl]‡q5‰ˇW Ié=˚vÿ⁄µs{«¯YÉm™/ht√zK^K√˘∂`lÔ3ñ'>*‘ uø/ƒëlyΩh~U¥sΩo« uYÂ≠YïùÊâµaW÷öc˝˝µÜ·0¬ˆ€ÓT£˜òå∫úî’ÕÖÀ˜'•k∫Øê©>µhn†Ã≥‘-2ˆ-ç{: Æ∆˘˘á8íe…[JΩ'v⁄{!=¶x"ﬁÀAÚõÛÖ≥æñz_ƒ+ôly‚o!Õ´/q´|t¬ë‚=—ﬂ)æäÚŸ”5Å$£IUÊo√†ù«§Ñ@fa≈5ï—Yd{©˝DUñÀ•iÆb’nh´◊;“IyªÇ£S^q∞Ã<&…˙§-◊“¶KfÈªMüDºZı›‘[‚¨E$g…UÛjA¶	$·ëähg‰6áñØíÓ‰´}]M|ûL&–Ω´q˛-œ<€KÛŒTe$aUM◊3ÊfÅg…™ä=≠‡‡Hæû±jiZVÉäH~€;=Í’].P}•ÌZΩØŸ·4ÛÎ–ºf=§ü}1[√·ã£ù%>_˘—xLtµ¸…W6|?Ïj,Ö —XP„kß∂˙bu	ÆùÙ$]˜Öò©ﬂØÖB‡—Hƒ-F« ü'”ÜgÏ¯¬œOæ™Ç^]õ3ue 	ÄI7jﬂ3å‹cÎ^ËÍrá?À=ïü≥‚†±1ˇÏÚA®⁄?gF8≤2Û›‘Õ3O©À>‹±‚∏n⁄·Ó÷Í(ﬂ?pèµ‹R˘*VAø∞ÊË[n˚∂˜˚¯å‘mıjDïœXõ7/
…ƒ^é$æ«6çÏN˜Ÿ˝\˘¶QÁ'.UkíÁS=C4‘Óm¬‚Ü„[…h˛™ å…3ípèUcûH¸‹iæÓõÀHÍKIÂá}êˇÓ∂ 5i”°sjÉgh;åmç©“eÔÜzTD‚è#a˙±n˛÷Ôá‰Aâ8√º“”u¡á™ˇÌ%‚FR˜∏êº$@"|Ra2™)êπ0YU èÖdVe,∆ÿˇ~Àë[ΩøÇW°":lFÜÉNÿÔ ø:'°O0¨O·¯
nÔÕN‚"n2ëLF¶ïK∏qÕ%œí‰ß§äºHj»+d-©#Øë(2Ø≠d;i#ª»^Ú6˘#yóÙê£‰rÜ¸ïú#ˇ$ˇ&ó»W‰:1(PF-èAú‘ÖåŒCSÈd:Öf”ôÇ◊-†ï¥öÆ†´ÈZ⁄@√t#›J€êŸÌAn∑è†¢GËQzÜû•˝Ù§CEvó$•H>ÈI)[ Ã¨πY•T--GvV'E§&©9Zõì~/î∫§“idig•‰iÉ“UÈ:25`ŸöôŸòù9Y"KanÊeŸ$ˆ=6çe≤lñ√rŸèòü∞Ÿ(Û@∫áHèƒ•$ ¯}L»Ã»ùÇ¯bB%ºÈ+pó	Û≈≠ÀVè|√å≥HE;&T‚π¬0Û‚π*/ÁÃÜπ	CÜƒÎq{™‡ú∑q5·Ëâ˚Ê≤∞Õ[®ÂæóR—6ÔE6á,rXk3€∞_°ºüúg∆±õd¬:√¸œÇ<Gp,∏ãw∆˚œ≈ä5MˆAÙGÿ™`ºFX™|[ê«*T·åñèSÚ8ê%Q&ûºL¡ıkÂR»™å[ú∑Jo¢=æÒÂ=‹0Î ¸¸Ú,íˆµÂ∫®DE•‹À≠jb„Œß‹¡Ø≠zoíƒŸëˇ¬c€4z‘@Pﬁ—Ø˝ æ’1M◊Ñﬂ·7ˇ˘¨ÒÆ¥ËÊâÆ`œ¬økî'[Có.vÈN˝¯‰Pﬂ’¸2u”«ó.˝⁄¯¬¥Èe£AVÔ«ÜÈ ˚æS£≠◊≈R
·hPúëÿNáúö¡∞Cáw∆4éèÏ£°ƒ≥:–Ø3]¯çï.OGb;„≠yÕ»î»OV§7ábE>O„Ÿœˇè¸Úé¢∫„¯ÔΩ∑ŸΩ‡ö€Ñ ÕxG8“D—.A$˝c≤¡»$k§˛Èø%†¥ñqÇ0R‡	ê£°∆j"”D4Ä#0ÿ?∂3t/Á0a&#⁄ëVGZav¨Fmßâ∑˝Ω›Kr	vj˝3«˜nˇæ?øﬂÔÛˆ∑˜›'ÁT™«C{⁄öˆ-Á¶⁄¯±Ù•8wtXh≥∑À÷V˚*#."ÕòZTü„rÈ¶€r´v‘vï‰›Wvè&Ó->^ıÕãN-ºŒ∞ñ◊Õ∏Ï∆≥ãÍn€ü[ªW*:oÜ6g·‰›]Ø~ø§™Êö°e<\∞÷ËÒïf˛ÀõßÅ©=œ7µ QÉ…7^õs†∞∏Øº,P∂˛ùLˇ´{Î/„ô9Ô.x¯Í#ã™∫Jäá–Óùw,<˘k„q®è<QrKD'ØÎÕY’QV}Hÿ˝ˆoõøÊõ»$Ar€˛{~ı∑g^Æ)9ÿUhE7◊Ó;™©∑¥¨ÔÀA¡–¬vÆVKÉ)—m3Y]SáÃM9⁄éªˆ÷Á)Sáíﬂ;ºˇŸEï]%’¬n®ìw$j˙_?Zﬂ≤°§˝»GáfL«x´ã"´À≠›-e´∂îçvC˚Ôﬂ[s ∑∫~_W¡“„õÎ
˚⁄h{µH˜‰≠√úáCªÜ Ê⁄ë‚Ö≠z–|>Ãú‡é«æzWŒÆó{dËŒzÈÊÎ‹xá{‹x}µ5˝∆&ho´,i7OtB§hwUsAu(⁄y›ùèá≤s5OGª◊∂ﬂÛdÕ˛Ω’¯	r⁄≠ÌQ4ÿ˘◊7Œãx9–ù?Wí/jÏΩÓõRÈ»5√EÊ¶\ÛÓµ›4ûÒnªÍàâ¬6oΩ∑æ-∑ø4˜8≠ÿÓlÓﬂﬁaXx˝/Œ–ú{‡ıÉ«ﬂ~gÂ€•ßPéﬁîÄì≈]Ω7ﬂﬂ∞j˚ìÁfÌº~IkîñÉoû&÷7gO|ÿ¥
»è"ﬂP˜LÈä≥ìwπÎõlô¬Ó<\ﬂiR®äO€QíW\ŸSËÂof*?J!aˆThR4{2Kå»¶=0öß&±S}5ëfUû”D3âq•td¶…[°Ì“¢óˆjÓ9e7=ﬂG
ŒIöˇˆÑ]€≥ª"´Q¥©<‚cvGnñs2<a&—˙HŸTZu:vŒ˙Kü†xJ*Ú ÄZ™äa&ÃB55JQOï£ûö5p+,Ñ€QO˝ ñ¿R∏ñ√
‘S¬Fÿ?ÖvËÑ_¿∞¡x5’1WQùÇ3&j™˜!a‰íM.GU5uUπï’\r© ÛI˘™´;»˜àEÓ&˜¢æZM÷ëç§ô¥êVÚ0*¨«»n“Cû&G»oPa˝û%±œ†''»…ü…k‰2@ﬁ"Ô
•F3®üÊ“+Q_“´©AÁ–ØSìV“*z≠ß∑”Ô“%t)ΩèÆ¢ÎhòFh}î˛u÷”Ù˝ç“>zÇ˛âæF„tÄ˛ù˛É^`ÏR6µ÷Â,ÄjÎZ6’÷M¨Ü’±€ÿwXªó5≤ïljÆŸ:∂ïW3€ ZÿœP}u†˛ÍB÷Õv≥'ÿS¨á@-vò=ãı9rëíπC>ñí"(Òø=5Çœ=J®QÒäRwc$Ø&T!∆†Æ†ﬁ8oÏ»ôhuí{◊kë›ñWGE¬®BqsÁÛ¨g∏{Q%Ô(ÙRZãg…ÁıGªTÿ˛ÙJJh#ÙXózÂVÙ}∂‡F•Qèî4eö…Dªüèf∫æyU©ÓLT\"nö‚Üô Êûî≤.⁄ëï∆*C[¯‘dP¡ìIä$fîòÑæx˚—ÉÄ`–&ÚÿÅÀq˜&Ë»ÆÄ©ÓãhÀgÛä¯‚´™â3¸ﬂÒCµ¸%)A±Ò±Î∆ÚA÷™°T∏v¯ª‡$æ4,ja<ãCACœÖ|	Ô€˜Öb¡?≈ÿ«ÇA√'XHoCõˇø,¨ˇyˇ‰∆àPáXh™FÏã¶œì≈∆(6¯G≠Á_‘√N}†˛áb^|Àõå$¸Y«áß©c&æ ”XPè„Rê∑åg°~å…GÀ®í˛/Eµ €â©,56>⁄|*/±µâ#Ef•Î¯¬›Ω‚îÔJ›H^4r¨Ër˙óBF§±;ox,HgQÛæ‚‰ØQ8»n∑Èvì›HX"„CÃNyè⁄kÎôA-êÙG«Ô–%Ô&$Ö‚%Í$˝M…rT±8l;ßù¸ƒsB7@eQ‡ò1ÍﬁÄÅfYõË4˙5ÇE¨{ÖÊ±H#üb°ú¿≥af§˚"Ã@”Ö¶N9˝Î%S‚Ç‚∞¿+◊√RèÖ,!ãÊÉ0/HV∂©6~wâR∏¯°”	¶6:]≠F {∆bnk≤Ã¿Íg*5lK"<ﬂêt√¬_¨”ØŒÊúƒr^¥…6®£,Ù—gùxYuc¸kfûY¢”“¥Ä¶‡6œßÖ‹ã1zÇ´¨"Üb ã6L,\≤,´°∫I«"ıæ‡”q’gC»cÒœ≥í9∆‚<l
íC∑As±œú›≤2_ë Jıi)Ô7õó 4∑∏,Ï,bœ,ï›“-#⁄*´Ÿú≥p‹±∏[Mgë?ëÖ‰˘Áw∏àcM^'¶øùñß‚i,∆Ë	YSül∆–-≤eV+ƒGöhjzŸ>€∞çäõêÖ6ëEÿúO∂êMπu‹c1˘¬{±Ëµt;aD≈†s⁄JTøJ83^	„ÚΩÇ≈b(ïaân©L%zî{,,Í—cT’¢(€Æè∞HË≥õ≤î”É·√Ω“9‘“∫aƒ7æ•ñ*CˆúV›6bAd¡T√M$ÎJ0WÿñÎy•È≤@ê8ìæfãÿy…:\Ö~£è7¯ ê3g´„OñüIbVfÚ<Æß±|@é,$˙{skmüÀb™Û≤ÿ4¬b¿°AÚº©s9X!å›ë«ı
|ß∫,t∞î¸oR[∞X CLıXƒ ãX3gé∞à·ª7JpïÙ≈‹H±`Õkñ≈uÂù—ÁÚﬁ˜7…z„l›yKM.8ßU]]÷yÃD5‹á«*á¯Ö¯`jy≈˚C	‚ctirã”Á’∆~¶D∂Tµ4;‹Æõ*ÈòK∂ÍÉ†ï]ûô’RÒoÚÎ/6é£ ¯7ª€›u∫ˆÌ9¡ÃëKn„Ñ®HÃëí∏R™[ª!aD
©Î$J@BÌ•/QŸsæ”˘ê‹Êƒ_WH‘ï}-HHº ˆz©ØïL⁄æYjŒîÚPe£
r«ﬂÏÓ˝qÌ§mâyûÔvgg3˚}ﬂ|
-Y>h6p;¥pùßR:]ô≠z]ãçAã?ìLã¨˘)Pºo;±=öAãGñQ® @Cãœ¢ÖY‘§[^Ö›ƒõ>¨˘∫ó‚V≠Ü≥tŒ¯ª<{¿B˚ﬁ’SÍÚ±/◊ﬁûK˝<âll8ª4º8qı‹ÃíŒÙåyUZX¯:æˇ≠π)n∂L≤¥‡nRzw∫AZ(¡ö5±Ægﬁº∆”v„Z¥¡'À&√ªs\JÆ7ö*ßJéÏQZ8N…Æñ≤⁄¯hïEQ”‹Ç“µ¯ì¢π‰6ôW›O∫x≥e¥`Á’’@œPJdvAèyüÄ≤û*Í“bˆÏZ∞ÈÉ°Eu∫∆}¥∞.¬åbùœg°ÖÆÃÊØkü?Oèe˜:ÜNÈòï]J®yåmÀ*”ôµ∆—¬ƒ–+∏+»©∑≈{mW>∫Î‚Cz&>Œ°√ê∏Æ≈≥ãV˙mãm¥0«‡[xöU<—%Ätúçß ¨4ôD\mªLs^4Á≈Mm^N)ió≤0û¥ih±'#¸-8ZL)⁄ƒeHßUsAh·Õ÷·∫•'L™¢¿=0ΩúüÜ≤q-¬ZûXáÉhq†$-û_´sœÇáMf:#ó¿ÔY¨≥#Î÷©É“¬ïGÙÏAöéÁ≠?ÊΩ»"◊∆)≠V\øÒOq´„».'—5ÑO57¯æêËÄYâ-UüµÆYn~ˆ∏x…ﬁ°ÖûÌGSû¥–,∏ÏQÆ”©≈ñ--.DDZxh1©·∫P”ÈÄ≈p.j˙ﬁ•	¬=L∆<ã®›uA”m8P†K]ã∏ú∞•Ö⁄FËZ–#Ï;±÷©ÖK	“dûll‘#ã‡40¬WŒ8µµ-\ô€Òz[Æ])ò≈Ñê™ùè-›l§yh·PÃ2[,†SùiõX_8EÄ§ñŸ£¥–ú .ªÖ∂≠€Ò∫hûå-x◊B™nÁ¸œ=ﬁZ¯8#«t+[_óQù6:6.«1¥à„•◊BÁjÙé¯2™Si°ÄGx◊B]¶ﬁ%Î‘∏¥∑[¯hë¸Åxò+éSWtr»V âñ¥ Â=!D0)dˆƒ_∏Ú:–ÿy˘é∏=GZ——–b/Z»∫”+
ë‘∞`”#’˘Ò®\aâ„E+∂„Ö[…D;_XQ˝G0vö‚<≤ö,\2-6qX!Â8èP⁄ ∑f-9¢ÿÈêJòTÿbÅÎbTZ4iôÍ”€,fç?¥¢uQ°≈˚÷≈Ç¢À<¬¡à„Ö⁄‡*oÏcóeÏ‰[-"¨ÖTôñ§Æ‚,ËØãï_ÓA¶`⁄≈á¯YËYòúXuÓ´~Bã>”©Á±0^p`îMîQ}AiùªµaÀy8 ©æZHÙæ•yo÷:µGZ‰w∂HV¬ÿ9cW‹˙ª∑⁄ÛØãnº8tíjSHã~º∞’∫Oxc˛5Ã©_ƒh±%–¬ç,0{ZÖØ%ÃæÖ”èﬁÀáLÃ©8evdÒqZ¿¥†Øflaezâv)ZLñ`U÷ù“Ç’|^3,'[h“Ç®`FÎY®∆†≈+Su’ ñ$é„Õº4Ÿ,6£ÿ)sj*a⁄⁄f+wÂ…nô:ÊëfôŒÍ
A/ò∫—Ñ•∆àªÜµ÷5Ã"c +¯™ÂÓÄAd≠%-v°Ö;È‰ë¬I´¥ñ«)sÓlaˇ±ˆk,®f
◊∞TvNC_VÚ5±+H‹è†Ö…¸&üëQÓõZ§ÕB|å±=ãﬂ±7ú≥⁄I¥¯˙—4˚
-ñ≠ôüÓUôw!ª¸X∂ë∑∞¨âj-GÕ-då'∫ıE·ÑıE0GA◊zµ÷|'∏Œü_º6Y∆ºå’≈n¥«˙Çáı≈≤ kã~}ë$SX∫„ÉYÏñ±≥[Xj”A÷˚uº¢#˙Ãï,©u|¸˙¢¨;Ò^Æ≈.ÛW·¢YXûôc-·X—ﬁÃœ¥—"qΩ~C¯Œ>Yíc'œ¥Ûo∞µ±iìÓËà7ÉÈ∆Ê—2ıgsÈssÛÂıπ®óõ¥ÄÊxΩ∫SKYwb≠6‹T…u¡ìü1~&˜XÍÉ§…óu'ÎŒa9õÄ⁄ìöÏ’·˝∫s,»À§*;$-íëEIZàñ¥X^˛i´Ö®0¢6ƒÕ@Õ@¿2œ2XAC9—ÿƒ ﬁI‘ZπÑPã7≈¶x9◊“åZk.QrÌÃ©∑–´‰äh·u˘∑j’Ñ±y÷§Gç∫·[%ºÈ◊6g`#â˝#'2ı ÓÕ⁄2kûìªè~éıEM‡ÎGöÉ6ÜõéÆÓNC⁄v«{.1∞¢≠@ÜÀçTœ¢ø—† /ì≈–bó˝/ÒwBúzﬂS[áè–‚˘ìÒ"˙@Io`pî˝ÍÒ#ı/¿J‰πÂÓ?d^⁄ŒwàO9p}øƒWæ¢˙=ÙŒÀKã!`â°Ö≤eh∏ë∏/;>ˆ-T˘ïõæìÖ{è∞›bá∂£E‹Ö!Çq'òÊMŒIÖàbzÀ–læ≈`Õ][¯‡≤Xò¸næO˙©¡æ>¢ñ“¯Wt∂|9ó·∞ΩEﬂ%§Öc„Vé@h¡’≠C⁄y ∂©A¸°o°'¯],‹OUªˇçˆıÅáÔuîn|¥Dä¥®n≥ÿqM}¯÷çq–Çßqria»ªﬂ)^`Ú1∫u˝Ó⁄xÔS˙^GÈ∆«∞¯∂ã˚i¸˛ªp‚£Ì¨Ñö¥X˘oZ¸Ô4'>⁄é◊∑ˇ?-∫Õv¢w§ˇwZ®˙ˆ·FÈ8úÜ35¯&úá' èÅckŸg‡Gx~øáº	o¡_‡:¸˛F&#¬»£dö|É\"ﬂ≈Ç¯ádô¸ä¸ñºB÷…;‰ñb(c ∏ÚiÂÑÚ%Â´ Â)‹ΩVïü(/(ˇ¢ΩZ†¢:Œ?è˚ Å›ÖEQΩÀ√ YÖjä∂e¡≈æV≥kƒ∫%Ò4=—"Òë•÷jyhSkå=`L7ûH◊¶≠èhÎ„®®UcL|&ç«⁄¯à«ä∑ˇ\5új<«tˇ˝ÁŒ¸ˇÃ˜Õ?sÔùˇÆßõÈv∫üû§ü”´¥Éqf`}Y"KaCXÀ\ÃÕ~∆Ê±Ö¨û≠fÎŸ&∂ùµ±èŸvï˝áµ≥ ˝§èÁâ¸Iû∆Gq¸9˛ü≈=¸uﬁÃ7Ò]¸ ? œÒ/¯U~[
ózKödïûíFH„§bi™T!-í•ﬂIõ§•#“yÈ∫Ãe≥ú ß…Y≤Cû"ø(óÀã‰◊Âır´ºG>!ˇSæ ﬂTò°P~§‰*SîóîŸ \e±≤JyG˘@ŸÅiÌ^Â∞rD9©úU>W˛•\R:U¶™™Q5©}Uøí’AÍ`5CÕTG®π(Ö z¸∫-x:?ƒ?`UbÇ0ŒÃ$äˆ"*Q©h©ÄJÇ+ËEeó‡Æ*.ÑÙ"·`"F0ÅÅI$â@U¡ÄàF⁄Îë$J∑Ñ{G£Æ*è∆~FÅäÅ¶cÖÚGËÿXÍ!ÖD0a?√Jg˘0>¯G‘@edê9£ë,R◊~RÚ£‰•°˙<wX˜ºhåÑ~0r3⁄Õ<£ä"f=bS∑PΩ-|Ω©¬•pâ|ËGb0R°a∏‚∏FÖ˝CÑÄ.∆(°†wÔ #ÆZàB–ßàË”Íà@†N<«	‹‹ ±oo‚jâ∑C˙£ø‘¯∑A|bÆˆc°,^9◊	èÔˇv√}¡m˜a—ìmMgŸ‰n£?]∑/Û[YpTÂ›	“›˝Ô≤ÿ^µ›;≠≥¢t–JùÂ–∫µÎˆﬁU¥¢≤¨+ü<·ÅeO∑éoÏºπ™˛◊√∑Œ›⁄ÒıoœÏF≈‰<˙˜
©\NÒÜûï•¿∏”Y ó¨≈#éKj85˜Y¬˝≠Ø∏¸Zö˚PRº{ﬂ‡øüœ_YÈÎ'«ófÖLœ.∞oIÆœ3ûXñ_]⁄Úso≥m XOLñ®≠f˛§heq≤iÁ	ûóóÏ˘eü&l∫È˜*.œ=eûZ=+ Qsƒ7,å>üÊ>vT∞lˇä\iU¸+ÀÆ€ﬁ™Y_hX>&ππ ≥£™ûÆµèÇô%oóÓmÕùy‚bŸ‚c—)Í∏ˆµ¯^%z»ï9ÉêŒîWÓöf\Ë®Jåﬂ∏y÷d®_⁄3◊ZZèJ+ô≤<ﬁΩ˜ΩÊÏç}Ê`~Õ<˜€;ÈüœçZW®6\LÆ˝A~u]UΩe'∆≤¨Â˘¬ÆıbinŸ;ìè´©Å∂:Urÿ‚Ûøﬂ≠“Çê_ΩxYÍ›Œﬁ¯>œÎÃ´Ù8".µ∂Ö§πOÔ±Dù{ÈoYêﬂêWíò}ùDö∑®πˆZs~ubï∑¶f≤4î‰¶ºﬂln—Y¢È®∏#_ñ{nü±Â≈XÅè˜å-N∞T%WZ™∏]NƒXæjXh4!ãæbıEç√≤å˚;kWÙmV∑n˝c†qu˚ÍÀß?ò˚óœ∂<¥ föV’&âk∏•jŸƒåÂ§ié± b¨Sy÷ïõïÙÈùo–å°CöæX{˜Ât{‹n◊˜≈Yq&˜WLøì©Õ¯øo|≥od¢jµ+„™+ƒªá‡æ‹yp Ã¡ÁD Eú—Ôd—ÃLq∑%%Nﬂ'Fè¶w7ˆd)∆ˇˆ+0Ê.3-Öª	ﬁ]ùq~Æ‹˚ò·?eû/Èá”;l»øº«» {ù§Ω¸·£ìËh¯E—ﬂE—qj<$·SkÖA0ﬂ«O¡0¯	d√H»Éqêè˘÷dxû«•r√PÜô◊lx^É*®Ö•ò{≠Ñ’4√†3∞∞¿a8Üyÿ8‡ﬂp⁄°É °‚¨∆3"öÙ%q$û$ëÚ}íNÜí·$ìÿI.K&êId2)"”H))#/ìŸ§Ç,¿¨≠ö,$µd…w ‰7di"´…ZÃ7ÊÇ";»^rê'üê”‰3Ã/ë´‰i'_SB%<Û¬©ôF”«0SL¶©4ù£ô‘NÛËxÃ]¥àN£”ÈLÃÀÈ|ÃkËR∫åÆ¢kÙ,rıQ?›Fwa.yò•—SÙ3zë^¶◊ËWÙmß∑h'0¬Êò°,ÛÃH≈˙∞~,ñ≈1ãgIÏq6ê=Å2à≤ûB∑Ö>T>EëÜ)ï$≤$ )Cãh1*1¶óÌ!,ÿïÎ„‰.a¢¶‡Y∑©ìGUå=x∞áå„UΩü@êÉ∏<ÿbw,˜
bH¡QT∑<Tåå_`RåÑ
\¶tâûŸ»∫Ú 7Ú≤Óöà=Ñãª¢QXà>™k¨¸ÈäX¡æ*ï∏¥
|IWé“µJ≤^
”ïs}O®$KbﬁbD)|å„Û+6Eñ∏8Ä≠ö—Gìr›>€ß∂€eIµˆhjFEÛÅ√^°˘áì«J.ü‘ﬂ«íTOJ8{?ÁŸTÎháSÛ%«ÑÕ)∂£q¢´¢Öf¥ÁÿS˝†XG˚Av8ﬂ#d©ÀO’~∞?∂_3lJ∫U´¶ÂÃ∞˚H16B¨hH±`-‘™çD™ë˘ŒóÊ’ºπnØ6Rõ>’ç”ØË(Ò∫ûƒ)NtŒ¿≤¿iÒŸ\±w™%.◊0ƒÈ%p∏é„u!BY°LG@ÄNÏf≠˘ÿ ásÇ”Á±«˙lvW¨≈¢Â¯∂9úæmˆXãÀÖΩ¬ÔÃØÛgƒÁÅsO¡ä°◊¿Îó◊€’J∞¯<^o¨„∂˝∞≠áÅ@OÉ-hÉé»ír¸ƒ„–]ûK¨0$X,8OóXd£X˚ú©≈ï*é©©≥	OçI∑[:>Q+ª2≈{~ª˘~(’k∑ò≈mÅO9n…-P)g@1∂ãPãπ&£ﬂâZK3◊é∫ı«¡z:Í®®®˘®ch¨A≠¿Ò√)Æl	ÃGÃ°»ı%;oíï∞F\Y:T∞w·›ˇ≤]}°éce<Ω˜∂3®¯0ÏÉ≤é√ √∏ií6Ω∑√í¥˘”¥MoNísí√"¶I”¥I˙Á∂Ω˝Éóa||GÖyí≈QÑqÑEÙAáüóAd¿eÆ_Ô‹Öe±ÎwrŒ˜ˇ˚NŒ	ΩìÃd;ªórÓ¡CÊ¡˛Ê.ÃÎÖ`7dæ∂«ùkÖ[Ls=œ?}ˇÇ˘œò"úïﬂ…ÅL!( fv©+ª†Os◊Ø<cÊ;Ï∆W1Û–s˙~ºC˛lÂœòÔÁüúøÃcÜ;x 1oÌê¬Ûœ!?gj0~¯Æ_πÙ	s;ˇ3äAGÈ`xüπ27Ûﬂaﬁ,º…(∞ˆÂ¬;Ã|˘(‡«˜ˆuf|‘ ∑¡∆µÉ"‰¸ßÃCX˚‚.Fxfˆ?fn¬Xúè∂è±˜ÔùˇÂ‡·˘+X/√¸˘≥ÛW‡á≤´QÓ?Ã=–˚ÉBÅôn>ºÒ}p¿<˘7¿ø[;Ïr±ã˚≥ ;è 7çvc»œYæ¡Ëók?ºÚ˜ˇ»µ°«ª|‰nÔ_c~wâww ~lË–pëfÑ ˆk˘«Á/A«%xÔ_‚÷WûÂﬁﬁë˚‰Öæ¶„Ëzj˝tÔWôÛªór`3˜_¿áPÎﬁWn‰†⁄tÊêüoBÓwΩsb9Àø8ß≈«Ωˇ∂ÃÊr«|Ó‚ør± /,¯b››è^ØÌ√‹Wò?2ﬁÓ‰∏º=˝øﬂﬁÖ⁄‹'ÁÁøˇ4Õ}JÓÎ;n:M_èﬂÓ”Ë{Ø={Õ˜•KÊK´{iﬂÓnÁÎÆì¨1‚Q@,1Så4:˜'»∂˘lúMäı∏∑zÂtËóWìÊv0l'zÉ*XSkE7:ãHÎó{Â©s}+ú*ñÌ÷bõ‡5QBœˆpÏ; 6!¡<b	kÈ´AE„·…¥y⁄=nx∆P:A¨°V:	nôÒ¬“÷¬c2¶m‚r|†(KLñí9F•p2?—∂≈ä•˙:`ÁÅ»ÇrQâ~T?DuyŸ⁄"ˆT‘∑‰5ÁbÓ4ì%¯´3r‘»‰ı,‚“ö«6◊Í†πËm;cI§:«ç“âÜ8Iv√ñGx‰™Îù*ø<5ı&—l.÷≠ ê]òú™HB|3ú%M1m«ÿuI>ÓÛÇKd„lÀq÷·‘u±§·ƒËÈ)¢ÕeCeò¡”LjX¶<íÛÜÅ„cüéF.E…Ùƒ◊B◊Up+Â÷k „”9Ø{é,ˆ⁄Gß¶¨—ﬁXGºß’L—`K‹¬®”≠ëñ4â’Õ;5©_Âé—X9rg”;S’◊b‘”$ìE:L∏nÿØWÉZ{´πrOô=£‚L2Àc◊ò&q6w0r¥ZMQ,ÇûÿüØ˚I§¬¢Uoòu=w}.Ê©
uÂ8)–gñœ+¢œq'D(æVù“r‹∂˘5◊ìâ’V›KcöÈXr }Dá¢ØJd≤≤ƒµ‚zÌFΩr*ß©·)ãJê˙V*Sjè—Ãm-™éyèºÈ
;UHB:∞Ç’’|-a¡∆8Œºç7ÈéÇ&WNèg˘DØá§∂d‚•â—§™≤	S°ñ*·∆∆LÙ⁄–#æ;ÔmªÇõ[≠A∑Õ‚: b–ôûbôN([<G‚LKû?#¶õ6	´∑¸Üô‘√…âº(ùme [ïlAP«r©k©ÎAF˙®Ø™úèç#WGUTëÊq;n˘‘[6GÕLj-B3·Ü—6ëW&À∂q|±ã•√/Ü)‚tVÃ„äítÌ{º°¯8ÙV©”—`)o:#∫«±8Ø„‚ *´L¿kôs=ÃÕºrk®t‘s%É`Ω	!éàZ≈\\rÂXwC”]≠^1ù¿ÿNZëèQ¿Máïjô,Q;‰ISäSÃ…HÌÚ–åñ„ƒ]{∑{Dµ±·j&kx¸¬ÎßA’hTXq£L"ÕP`ˇñ°ÊÌ0S0&U◊%Ç:’„öC¶™ÕíïπR$ÛH<*Ÿ2q‘~%•˙pµË∑îno<ÏX]é`O@˝QPFCBö÷ó°ñˆ#µ∆áÏtÉ’ÎÖ#_ìû?û./˚§&Ÿ…,+Æ-Ûõ±≠[ÊL]ªÅú∂˚∏+ıí.EzË.^é©";
◊B$YXTà;˜ZÀK©‘¬Âé†ãcõG®u≠;gXÌó"’Õ¨Rﬁÿ–$õÛf•ﬁ§‘°e\ﬁ"=fá,í∆nπöÖŸ¥ó¶ínjáú¥jU„0°’$<oE›ê–i/Q±c©ï	€'.À©©”Qî¡ëﬂ¥Á˙f‘)ªt—í“j§)49;√SµivlU8ms™Å#à÷¿áŒ
±∫DËMY"πËR˙ ´J¶lTbë*5ä≥µÔ¶Q4Ì◊ßà=∫Ê‰8±•2´
£…LS≠‹+îX÷ç!Qhq∆MK}á
î◊èÊáq‰π‘ã0nõâQíM∑£c©FA1^›πÛ¯´^}£_ÎÍﬂ˛tÁ˘˝>¸˚œèˆ˘€_YüwøpwÙœÔŒÇ˛«{ôáEu^a¸‹πsgÄa@EáEÑA6ÉPï®¡®Q E1V%EJ’(AA¢à†h]£äöFåU@AÍâAcçm‹„2`\íêJÜû˚BÚ<ÒÔ>ôÔπø{æ˜€œ∑›)é=1‡Âíı>ñ„ØN≤Úˆf•ij˘∑EÌUGåq∑k¢F{¨€Òé≤≠.˘j≤„Ëf≠PÚÃ„L{c€“äQfµ°;≠ﬂ⁄9{’üÉ∫¶π∆
'©õ¢S∑Œ)ÕØ‹htagCÁ‚Y©—≥™óùM:îËr~€˝∑÷xÑ$]π£m©=ë=™ËpÀcø∆¡û’ˆ%ôÀ*üñ4Æ⁄◊©Ô¸‚§˘Û£kn¶W> jK
¥v=≥ÒìˆºyﬁYgzé>]’¸’z·3ã≤‰›Á3/Ön¨1∆:Êﬂ¯t—¸Á•-t‘g˝Ù®òáÈ◊ún≈ñ¥l±Ω•¶y^∆“j…ßÂ‡®˝/%Œi⁄qø‹¸EV‰kû>ﬁ?9TÏv∫~≤p©>∑cSn∆¸∏ãÀKœd¯µjS'úõqjÎÿ>ıÂ«k6WDd\wX4¨›ÁâèªÚ¡èÀB∂<j›S∞∫)9brîÆ6dÊã≈7≈>¡°≈ïè«ÊæŸt@·p≥∫%~R‡ﬁ·æﬂ∑πpg‰ÒÑª]oLR/Q}ô˚‡vΩÍHåL˚Ω†∂}Œß“JØø;]∞|ówN|€®”<>˙±Íáà]i9Zø»Í–Í•ÍÀ√ÁÓΩÛt{mdt‹◊ü'ŒáÖÑÌ+àK,5ŒsK*ﬁ€|ÏâsXŒÖ+πÔ¨5∆h™*Ú¥kVFO)VÁ5ﬂ8ÉPEj¡ÜÃB≠`K—4ûB»ãzÛ'àç°V∞£˚‘D{iEˆ(ˆ4Îï<Ω(åÜê;ı!mè‚@øQ®V–pAπ¢å_+≤§£¥ôq’ﬁ=äm¢ä°ø*÷¥‚≈Üfæ“ºñ¶¸F°*¡ë?ﬂ¨ïÇE˜?∫à.E¶ ëâDA	ä‹QP¿@íI]†	¸Ï_Çˇ;¿¡¿Ô¡‡s¯¸|∂Ém†|åOŒG∞“˙I¸ñc≤-“∑∞ÔÉ˜¿ª‡6x¸¸x¸º˛º~^•©∑vï‹8v⁄e∂ï‘ª©á≤Ú%ÏK`#xëŒ3?á}<ûœ¢ÔˇÑ› ÷Éu`-x
¨OÇ’`ù†I‹Ø™ÓXW∑/+"U“q≤ÊîJrD,ìSéS∫KHô·¨U‡£æú¸ô«hâ]º‘ˆöî¯=ù’£»WÜJ—◊:¬-J¸ñµ#tòñpL÷‰ò‹“·û‰ë)◊E±}eä¡‡ß®ÛÔ4ú˘	Ï˝–˜°ıΩPäzÍ+¢ëà…ı…äHªQÆê"òÛúàÙ7îﬁÖ÷v¬˛‹!Øc⁄NØ1∑AŸ
nAõë˙Wp⁄‹à‘^…"m@ûıHÕÛ0ßÎê'\ÀWò]ﬂqﬂr†¨W˜¨ù’º1Eÿ"e≥ßÂÒd#G6Êm∆ì≈˛ª¡)YH…¢”úíI+πU	)"SˆJﬁÏa¨…)"Sˆà¨»ªU.˘òNÀë+Ω;Üvñ#◊2(ÔÉK¡%ı‚àÓ}ù∆áM$óLÎé°§¨à¥êR1˚ëí Gé<˚©ò˝TÙ-Öﬁ£Å¨•`ˆS–7Y)eí¿y‡p68ú&Ä”¡iÙ6ˆ˙4ƒﬁF?„a«Å±`¯G0úäôö{28	úæ	N £¿q]7πßcQˇÎP∆`GèÜ=
å √1öë∞GÄ √¿Pp8¬•E^s≤=@p(k"˘¡ˆáÄ>‡`«4¿ˆΩ¿A‰…mH¸ñcûX°a{ÄÓd√t#”ïWºHzËÿG"ıG~(˝`˜u›Î†ÿm;#óË:ÙPÀﬁÈE∂‹¢ƒoY≥';ˆ¶ƒoπEªû4;ÏFŸπå	îÀ⁄`wYC±-AhöÉf†öÇò*>]E¬E∏£Hÿ[
‘,†ˇâ<˙DV3πˆç¸Ï·ßÑü”¸®I&¨^/§~ó˝>Õ¸?˝‰ÎñÿïƒNUÒr<ÕNüà8¡—Ïh%Oñ|©HÏP5Îfdf´·I3ÔØ·t÷5‘ÏpKŒÎÑ´«ôÉöWoNÈC}Ï4<M:∂˚R_OW?∂]»ÖÎÈO˝πû4º|ıú”ïÉÜóµÁtÁ Ò“˜`eâ7é€æà∑ò€Åà∑d€a8«Òf’–˛">ñﬁc˚/|H-Ê†·√P>Œˆ—®·ã¶àıZæÑ4<vyÙuà/Î3¨ú„´õ∫¯ˇÃ5mSŒïˆcë:Öõ+I%òI
%˘6ﬁhJ⁄÷∆÷F?{[Ω≠áﬁV?WIùi¢ÆÛûiõ⁄∫„Ÿ"ï°éï]˜ƒ©◊ÌE˛·Œñ**Û¥+s–y™ Ù:oKs—Ÿ›[€⁄¿µ5ÿÜpΩ≠∆ÌŸvmªmàü‡™rËÂ‡8–iÑ¿tsUππ
∑s–“/∂£¬r¡⁄¯©…˘ÒXYÜGZ‡Êµ,nœI”OßÛ]€ˇæ‚b«KVº<ïëΩ“wÛÇ∏πÑ∆òÆõL•¶1óèÍò—›_ë≤^JƒÍH∑§ i!ä*I!ê§Õîjm´∂¡Œâ;ÍkºÀ=åö>Ù≈r≈:ÖËHΩÖBœYıJ•§–	:Q%*$+ïù‘O5FäR-ñVKU'TBÇZ“çj≠—Ã($PB¸0Ωπ†N*:MŸäµ/ìA>”≈C/;∏+XØqB°“R·%G·fä| „Û@…^cu˝}ê^iŸ9A<&ÊaØwµ+Øà«ÿÔä	v÷9Zëì£ã∑ ^4ﬂeØwYzñËuiÉ£ï—f”)ÜwΩÊkTéN+“r&ï•ì™∑˚`mÎ›÷[ßûñ“ﬁı—`‰â±Â—€⁄Ò‹‹~ôA¥‘.[√z…3‰iÕ©H<7Nˆb¥œÜEÈ9Yã◊±Ç∏N¨˜QyÜGDNˆx´&≥ﬁÙ≥`QQPÓ1^xÿr‚V]ÛøÎe¶KìwD∆«•Ãﬂëõ.8¥÷
.Å›>Y–ıP¸üdÅ~4cHq∞’”U‘ePëñ‡—Zn†Î†œ†À¢k(4õáMbìÇäÅJÅ ï*¨*lãÕôÄ∫EÂÙ8’LDòMÃÆ\y"Ï⁄ë#Bñ±¿}Ú‰J,»ß«bÅæz_»R‡»øê§ı í˜·ÇpsqêˇAiì—8*aÀƒ˙ıöä
:‚“6&énÓ6í±‚r*J
ëRûπQÅÂ∫∆õ'oKc¸ ]‡ì—Ì§®Æ®m¨í(≠$≠†§öëXõ$]S5£:≤‰{%¡*,ºG»0‡9jBp˝¡¡êØIx°µ∞8≤y°l6 K
T±pE§ÄÂ<ÑÕ¨C°lPC≈ f≤3°l6 ªŸ≈ﬂ√+¿Q;∏27)?7ÿ ÛV
^¿6¢#∞ ∂çsíÄUB∞Ìò
lßñYâ¿÷/nu‰»Ä}yÿEacÄî¶¿f• ˚éœ@9F∞<´Ã´èÁ∑˘ (»V∏lGàæö∞ˇÔøUÁp`õ-H®1  ˇIúk
endstream
endobj
72 0 obj
<<
/Type /FontDescriptor
/Ascent 701
/CapHeight 0
/Descent -298
/Flags 32
/FontBBox [-167 -299 1094 827]
/FontName /DOHKLB+Symbol
/ItalicAngle 0
/StemV 0
/FontFile2 73 0 R
>>
endobj
73 0 obj
<<
/Filter /FlateDecode
/Length 20192
/Length1 39872
>>
stream
Hâ‰W}t’ø3ª3;≥;ªI©ßBŒ±ı*1°b°ú¿â©húñC9ÃÓæÕÃÓ3≥…Ü%5P¬WâÄ1¢µtk5•4EJ-Rh˘[PJÀG@TJA(¶1êﬁ7;ªY†«SÎüyÔÃ¸ÓÔΩ˚ﬁ‹{ﬂù7oÄÄ/„Õ	#¶<˛¿ÉgÊW0c±u∂ﬂP£@˚Åi¬[I "k–…6 <'Pù@•ôˇIòàÚi vl(h∆∫Nñè˙#B±h¬˜ó.A>†zaH´à¥›h¿áP*‘Í–ú9Ø« ÷„#¬Dﬂ∂™‡˘:?*å“v-¿‘áª¬3±∆]VÉº¿ë?'nòå´ÊQÄ⁄ÅÿØ™±Ä\–RÄ∂Ø5qæg"rBéào„|aÏœèS~7˛¥Ωv!Ä∞!*G»îq´É çx1-fò‡†˛6S}M'⁄•¸Ì ÀÔA˝Àö”™˜ÕÏù®8úÃJ‡¿…ncó‚àÂ)dé¡É–YM˛®àô˝·m„S\´÷@π´ók∫Ü±Â◊≥AÊ¨!˘7Ò/P.¿~x√vLáVòÁôXÕ,É·ÃÏ}6B¥„5Ü√lH:Z°¬Vºf≤{qD
`€
ì°µöÄe≥ql_√¥Çè«63Î°â]¿º3 Óv∂¬¯lvD†^ag¡Á"\ºå£r-ò¿NfYq\uéÜI‹Lÿõ 	«Yvñ´∫ò8Ñ·úÑë0öı√
≥K`;d∂0áô˜ò2ˆ Ïbzò˝L	Gkà∏=iÂ∂¿Ï<¿É;`8©˛ÏœÉ{—~zÖ–˜Ω‹zÙ*sG@#¨¬ˆF(·6![Áéñ¬Päı^‰Îê’r•∞ûÜ28»ÕÄ&¥	~∂’∑≈—¬¢TÀµ3îØ√ Ô3É˘10‘y7;ã_åsm‚a≥UPWŸZn3sVpIå« ¶â´e˝L*&ì°å[Ü=∑adÜ"Œƒ…É+\º¡gsqm^ÕƒÊMÓ,+±ì¯ úÁ.0]Ã¸◊¯¶ã∂Zôë¸#–√Ú€ô1ºè„s˝¡E?Ï]å ∏áNœÄ*XÜ…Q q\Ñc∏¡◊«‡€uXÆÜÿ9»UUÑM*WÕ0E‹L⁄”å˘ De5de,ãt9n«T¬“ã…Ô¬=C0˘«´#˛ò
Ω˝¥xég"‚»≈}»ª˙m4R%ÔÖLL`Íµa‚LËÔ1ô∫/|≥F“<y¨ø«$§fb‚òP ñˆ˜òC21¡Ûœw÷„iMè	~î“1ÒúıÊ…‘˛ì‘I“˝›/ˆ˘Õ•ﬂt•ß∞ßåôñ€åß÷Ø€L=}“dÄôv˝5f˝ÊcqX'|2]\|sÒ=∆=w8¸‚˜˜|¸`ˆb.g µÚOãæƒz«Ç…$kﬁœYÄIü5∞‘‘—ò$°Ÿ¯îçµ6.¥qëç?∂q±çu6.±q©çÀl\n„
Îm|⁄∆ï6Æ≤ÒW€ÿ`„≥6Æ±q≠çÎl|Œ∆Fü∑±…∆l\o„ã6æ‰ú◊c?¶iÜ¢∆¢/Û%7…OEïôó’BHó¶ã˛LP¢!%™ò’I>§∆t%˙sg@ç˚_q9Çtø‡∑L7_ÂMí◊DY◊cU˛ònNâ*	ôøtYb\€Ë∂]¡„„ØR˝¡XUt$:!ø55nPkåﬁ ú˛7v»&—-ª6ë∏j*öZ˝∫Døb:µQV∑x4¥Aë’†
˝ñ˜«Uïò[˘†R©…ÔÙ€˛ÜA©îUêﬂ{dÁHX=€¢™
ƒ¯É«2´íË&Iló,FøÁìƒ_ ©"W†5f\èÓ‰p&-‹*<ä—öã- lióßä(D7L]6å›æÄ¢Tí∂˝OÓßŒ∂	$¢ô’1€%%ä~¢€‘ß.Ex”G›ƒ÷8ΩsèO'!ï$“|Øàæq?ä˚§¥*e˚•¥"eoπ>ûDÕnãî¸6'G+TÚg°BóÉ
6¸≈´ì
≈∞å–ï–A_ p≠µ≤¯!z$YükÒw\¯¿`<`æÎ¢dı∞+3#≤˛´[çU–|⁄ë¥,GÉm9¶ˇ=⁄†_•…r4√hæsßY\;ÓMÀñ%'2ä4q:]jl>âVêì¢Â|J x!Gç˜º}N =ÌÌÛÈ˚¢è†¡È34ÖHîŒ`jg˚IúÎ#~ÛØﬂåπƒL)~òMI‚£lÍ7œ{(%)’HQÇ˙∫¸Ê?]!âãÊ§J.π-o,´?hZ‡©ó›i…‘Æddí¯WFˆõW%ÀVk§©}í≈H¢+ã˘Õ˚lSm›Ó8I|z˜õ=)ÛmÌkﬁ>Üﬁ\œÍÙõΩÆo§vöÙ.6fW ß›¬\π±ÁÊöÈÆ‹@ËñK∫ãÁ πr ú÷Â‘Qæ•:ulÔ†’ıU°√SÔnˆ‘ã{ƒ6iîòÙ‘sÂÓÅÿ÷Ì˚¶ßﬁS/ïzé
gƒÑÿÊqªx©‘]&m€∏rÔ[®Wá≥ªZ3ùsÒ¬tæALr≈bí∂àE¬tÎ)xì.^,ÚÑ››Æ”xùsmt<‰⁄àÌüÈ·ˇR=ÁƒÛé+B€L£ Ÿ%∑9gèò§:¯já'åœùôÚ[Ë»1ôÓ:Ô0w∑¥5gèT*ïzá—ª'ú])ó6H|ﬂÚ£ıÑ≈$éÃÃÔ:«”~©¡ÊÔÍ´¯,´J∏bßﬂ›MÎ≠˝©u\i¬ ◊ r∏wªÔ¿∂2¯¸%ØÛø4Êﬂ“Ç?J„˚ÛôSÊßÅ£√PªΩsﬂ¯éA'ÎzÙúãˇáâXr†ËÊ¶ÒŸYÂN€FGM∆∆'œPÈ„,•;È≠”∂QŒôãòwj∑,}{˜}'KñóJC.Ôú¯‰ÂÄö‹©¢ãSÚ>ã*ùLŒ—yyÌy]=Ö[á∂vù}ÒÚÅ;.ÌîÚéËÖì®0®Ê1ñŒ9êﬁ:≠Èã†Æ˛C}Ÿ¿6qûq¸y?l_Ã%>;òî5gÁV¥¥À»≈qÇ√h…(®¥ÎD>µÆmÇñ‚K0‘– íJ®ÅîMÍP]´j®tr>JÄmi%òà&]©@£Ünk2åΩÁŒ6I !U{_›áﬂÁΩÁ˘˝ü˜„ŒÃ·˙áôQ∆î¡É∏j’4#ë∫ÑwÊLΩUù^˛¢¨¶OÉ…*≈ä_™Rzj≈“ê«Ìm¨]d◊:6!@∞èe⁄≥máãæˇπÓãÖñ∫üP+…ùm3è^€≥Kõ$Õ¸Õ(p£/ISFßfø·^ÜU
êﬂj!£Q/ï ˛≥êHdd;◊Â
¬HÂ∞œÒ{YMçôÉåÿ‡»¯ÛZ≈`t 2*‚¡ØSGπëÍÌ“iHÒÖÛKÅ±rUGk †(Ûîf§„MNÉ—Æ…#◊„‡∞´[Åï˚ò R:æ†36EÈ±M√Ìm«ñ:2f∏‹Æ|±#√U`≤{>€ûªP@∆‡U≈ÌBF…⁄ûø÷[∑,/åå|‰”Í›™¶çÑj¬\*UnW´:^≠Úéõ3ì€s‘p®Iq!cŒ'5.L¸eg∏«>Ò‡ıÌÔoÂá^î*REd¨sÔWK)B»e·EXìŒ®‹È9}ƒÎ)VK><Oß≠Ï ve´<SÖ°ÅC1Ÿï-JKá¡ÿïbtàÕ÷5ß«XG»(.ØËüæ§™«ˆLÔ§õ;º˘G7E¬Ô∂Ty«zà[’Õk£~ù1Ø˘ìY„ﬁxª¸…vÈBe¸ÍÏçv∑RØPÌ}+Îñ˜GqÃëVBvÓ{˚§‹xTg¸B¸TY4—ÂÎVeŸaMmíêëoïÁh–e'Êqù‹≥EnŸçåŒ`ßŒX.Êï5[ÈÉÀÃ»ËpyeçÅÉb–•rm(–·ï‘pÎÃ„!Œe5¨i~}¨ÌRKÖ£“ÛË8´]Ë™t’H»X™4xñÙ´¢£VÆª6p˙;H†pOEêƒSÛq™yv†ˆ°”ı™,âı≤l3“tfóø}¿ìõlO<û¸¿Ó?/∑∏ëQI1˙ù·ç%tÏ—iy…íáÑK)‹XÊYz†h)2 ·÷Ê*Ø◊) #“az∆◊ [fx*'8Î¿›€Aeû¢Ç?$Ø]√ç8úå#cƒÔ/I\k"…sQ«/M… Çåÿ¡äc≤Ø)À=bò«_ˇ‘øE—®ë«+©<∫D´ìVÜÃ=è≤Ìl(∆<Ç∏Á„öŸ`Á;õı5#!„$Äzûbl¨p „xª›‰À0æI=,¶¬ÓæÇƒ¿◊◊3y$˙¡Á˙X74v9&∑mÇ]™¡X/k÷˚FFÕj/0Úx|£ßu;p}>Œœ¨ôÍêìn8±Q‘E⁄fœ~Á£*(KΩ^ü ˛:íáõé·ﬁSËÁÆñjy≠ÿ≠œ«;ü∑ﬁ3∑r˜„Âãr+Áé1["ı˙ö¡ı]∂ØÓ¸ºÙ|‘óΩ˘1©¨_(íË=wrJ€±≈Ÿ∞?";Jq>v"çOπ¨Rß1_÷˜t∆YÁRå˘Òê3∏˛≈e»XÛe"ˆÙ*KÆÎYÒ`_0Ÿ;jj|y˚Ê#˝Ω;óü(9ëìÛUÚlwºª$‚)Á√ΩfœÇpÙo]≈«.ΩÊ˙UﬂÂïuWpÔâñD ÑöZ◊û@ê$¶I #‘wM©9caÆ¿Ê¸‘∫&Cñ˙®Œ(
c]S.«äArÌ©+∫ˆ∏Gä∂Xzªóﬂd7nn.$µ#ÍªùTîSlê“‰,Âõ U7=Ì¬)ê)Z˙¿RFwôﬂèå*ÔÒ £êq g∫√‡ÕÕ%k£G£¢ŒXxÉQë+SådêÜƒNSdm˝@í∑#œ∂÷ZUŒ…+§å!Úm`CJé~≤@‘©&Õ7ß u˝)mêQ|:›π∂awB„ö¨à¿ãGŸUﬂ⁄ùV∞√ÃÒXò a2<å|>|eœÄ«`<	Ûa<Éﬂ5+`5‘√ÀÜç–m–{`/ºÔA¸N¬8ó‡2¸ƒB‰>R@~H&ìGàèTíŸd©%œê‰%¢ë0yÖ¥í‰u≤óºEê?ìnrî|DNë>rû\$W…(°j£NöG›t4}Ä>DUZAg—πÙ)˙3ZMﬂÉ˙]A_†ıt=›L_•ÌÙwÙM˙.Ì¶””Ùs˙zÖˆ”$3≥lˆ=vìŸ6ÅMa≈l:∞'ÿ|∂Ñ=œV±ˆ
keÌÏˆ6;»>`'ÿ)eÿe6¿)˘pÓÊ„˘$>ôˇòóÛG˘S|	éø»¯æâo„ª˘^˛ˇ?¿ﬂÁùºã‚G˘q˛1?…˚xîˇù_‰óxåˇìˇãÎ<a¢&≥)Àîç’IxÚ¶íi¡˜√¬ôâdpÊ#fäŒ©ÖîGãÖïﬁ83EÇïöÒF≥å#+}á˛®ïXi6ﬁ[çC–≠ÿj!√0ñﬁÀÑ≠&b%z§,›éµ˚†?b¬´ê™iã~’-√–õE˜ÉïÈÌw¢6˙ëU‚_RùÜcX1äç‰ ∫B#∂`ÿ-2f3e±2ïàzÓ0{"ˆ—ï9P±aGvµËQÙL˛©√Çz;ˆ3Vé±8V¥ô7[ÕËçöMÜM?¥·≥@0M˙:NB.û.É\÷mx6c€‚{¥G|'≈åõb^}\íCÇÒoÙ;/¬ÌªÌ+hÈ{≥-ÊB-ì∫x~Ù3[¸ˇNãs®°PF-sª∏T›*ÚÔHˇ∆Øª—‚¢%G(TQK†µ4ﬁ-˘˝0ı÷!Û≤øπ‹•íåf˙,¢e∏Æ≈_ËÆ¥å˙ﬂf?ÿ“Q#˙9˝Üi‰÷*”Z“FÉìu˜~ªˇH$£•ø|5jâè÷µ‰»ü?•%*lËdöÒ=∆ceΩëÇÿíØ{5¸µÑíGI¡ÓÓ‰˘∏gMøTgâ≠..I‹üˇeÚøÃ◊l◊ Ô{ÔbpqŒIÍù¡s.?H—∆™såBP3|	!d[∂Ç¥˛ò¥UÁÖ±Q0k7ÚGî\B‚$ìC¬Ë$g†A&˛®÷nÖ¢ikˇÁÿƒ©ñ6≠ì¶b+L©Æ`√®nnÔùÌ èj[•ÌY≤ÏÛ›Ω˚¯ΩÔ˜}ü˘ÅQñ4S˜ZùÃÅS∑∫¬{wÁø–⁄2¨F„ËÑÓ±‰Î€Œªñùì∑ˆZWıÍ+O,3À c1±pÄÕ±!ÏEÈ[.lYÑïñ©5õJÎ_ÄíhÚÜ÷ùÕÌc≤‡•´º®ƒáb—ZI%á$∆)n¥˚5ΩÈ–R‰πMÄIó≠æ,ã®/?dö’ƒtSgñüèÂÍ`
ñáá›"îÌÛÈ∞’]!A‡BJ\¯q‰c.ñ⁄˝–Œ,◊ß:ZÎÚñK‚î£ûYh˜%´Ê{µ´Ñ˜Ùjñ≈#uãM é¥ã˙%Ω…„8#í1Á∑˘|†l¬ëùªó-¨∞(Ïø∞õ,^PTÅH-∫&xB÷Øf©fß€ÓZz3ç‘B;*Œz’≤‘b≠«Ê¸∫a»+,#ú± ¥˚tfπp˚“≤Â Õ¨c£ÌlÂîkÜºÜºE`•IqOÎ±È„nhÚ∏*ùëtz≈gOI∞`‰-h.—mkë-˙¢åƒΩÅ3I˚Õ´_¯»YriÑZ‰≥F)µdº[≥Q‰·s¿síl„8f·ñæPm#ﬂ¨-/±É9ô≠ZÃú<©o√œ◊}Øha◊–MÎ¢#±}ß¡,IslŸ2_-;:ÏÕÂß®E0î¯Å§QπﬂFi¢^
û⁄ËÛMË±„◊BßÀÂ™Ä˜Lèb(k´Rn~¡HÿòÖ‘_>V÷ÛjÈ≈ë€ÕÈHÏ‹¿ô∑Oﬁ~ùMKé˝êZ‘/k> ©L p'+Û"›/PÜåÜxÿäÙœ‰ÚñÁ«∫∑äkÇ›M«–¶Eê.◊¿‡∏t‘Yg*ùZÏ‚˙Ôççó˚j∂Ô‘¬º“≤√‰;Z+EÌàey¸√hô‰rñ•˙™∂_´çˆªMemS}ïat®≈å∆=‘R_¡,Á˜§4≈´hÓÏÉ√ñ≈V9Árß_üﬂ¢•#¡Ùó.øS·"jßf¥X(•gŒòS9∆tÃ6ùAg≤'=ôE	ÀÚîM€Ï≥WÒÃ&–e‹BcLmEhx‹çäñÅ\¿¨0˘©]Â∏mJ°ñˆ&gY∆#I¥»ºˇ∏Ëeñ
¡P˚¥ç éÖîÇ€$Õ¯ö5.—¯Oa∏¢ﬁyåYz@UYÒehÿöc6O‹•§_;:1Ì÷©%¯#%ìŸK\µâ^ÀbFG˜”⁄vI«*ã"é,ÅÙ≥…®∏…≤<´á¸v˛¿˘T%Ë"1®mÊè∏—ìGåÀa®lc”xõ⁄ÂayÑéK“≤»÷∏0K˝¡ô•µØﬂ1©Ed6*ù◊b:$üVÚñuÆ®èYstj´yÅpyî7_€E-±ôàﬂ≤[ÌAUÄŸ∞8iöWn[≈:?`&T¥V1ifFs,Ò.©/M˛M{Ë∏®Ü¿,C‘2`√<ùct}ô5_ﬁHgñoø2CÁÕ#VºY/x4√Ô;hx=Sl\b†–qâhF–jsá%ø∂b?wbÁú¯+ˆPﬂÜ[¡"QK⁄≤g"¡7VXP›SÄÜqjòºVó:]F:ó≤¬3¶$S≤&∏zP]ç oQ;x}Ût˝˘]%7∆%zA<⁄Akó¡ÀíËh
ñ!ï_%\íÆC5?ÎQˆQKPóh9É—…Á‰∏#‘r– …òS•sÅZÑº•2.QKıƒ¥”_1·	û€yf™•ñZ˙Ñ„,^:¡JMö{|ãc €ê3ôHÅ5«êôΩ!’J√˙—
¢Æ! „6/"ÑhN.ZäÒ2"“úL”=≥\XaâaïèL˜5JÜt4π§Ìo
›^ö«Ñ¢ª›ÑÆï‘“#u1,*÷Ziœ)BUüπ$≠ãÂ±ıW+›]ØTO¸¬∑Æj“„Ò/oFcãµœ|Á‚‹ŸÕ…†E«Ë~/—5ñäËN–A#Ñ˛ùhÿû#-eOøoÒƒ]_¬Í™0h∂›Õcä2Ìí˛<y¨Ût≠L∏1∫V≤ú,®ZÑ÷¸1º„Œ®„…YëZR7Õk∆û√1˚ÇóˆÂ7õøòf<[Ã5˜õìπì9±9–Ô≠öÃÂˆúπ® i›¶hˆØõÕ°ÍH2VU˙Oﬁú‹láX,‚]L}@◊JÅF:[+hlâ†/˚ˆóØ«,ã⁄º-{"¥’ûËU˘ﬁÅ‚˙bfeZ√‰--ba}¡ò˜¢ÿ?≤,á¯éóòÂÓ^ãÄW+??§È`!˘3ç ô=,O∂NíıUu≈ÛÈ#±«z@+÷çÌ?&æ†tÍû¨⁄ßÏ≥úgG/≥àÀZè˝Wñ.ı-ø’˝ñ2ìéó©À<ΩX«!ÆCQOÄsLt-¢≠¥p˜ù¸®fø◊‘dA)}K¡R˙à{-müÿk°Â;•ñÄ12O-X∂X5ˇø”Ï˘w›ÅT·jı~ã°á≤˘èéG‹´hy»éÓ˛V¥lêUÀR
ö∞¬Úølü∏Ø|pC¥B•‡U«ˇèÂ?là„·S¥`è¡gi5ﬂ_ÑÿNwåO√≥t3”=∞^Ç>Üº'‡4º
Á‡Wk8Ûp	˛Ô¡ﬂ·,°U®y—„®5¢ÙUÙ¥˝ °ItùFøDøA≥ËmÙ.∫ÇÆ£†;ò`;.«Îz¨‡&‹äøÑü¬œ‡o·¸]¸}‹èGÒû∆3¯Á¯¸N‡ﬂ·?·æéo‚%RBVì5D$“ß™!ü'OêF“D⁄H'yétë»!2F¢dÜú%oëﬂí$yáºKﬁ##Ôì[$«ïpŒ≈’qÎπÕ\+˜Ói.»¯ÌU’uÖœΩÔæ∑wpﬂ.§¨ñô}ª,
6*¢m!RŸ£µ£;#C¨d:ôà⁄¥⁄Y`£ªQ@•ëP”JgÌ¥NL⁄äçù…§v∆§5&Ìÿà±e¸y=˜≤(jå⁄1˜Ïyoﬂ˝9ÁªÁ˛~ÏóÏU÷ƒZŸˆG÷Õﬁc≤èŸøÿ%vCÂjÇj®S‘jÅ:[-Q™ã’*5§F’_´;’NıMıı®˙Å⁄Ø~¢ûW/®_®ó’!ıÜF4U”4∑ñ®%i>ÕØM–&iìµÈ⁄S⁄Ìi≠@+“Ê°<*^VÓH†∆ﬁ`>Dïj$¨ƒFúdpÇéâlTì™bâFâ¯"*U¡6l√â‚à-ÿÒ9ÜÿQ5∞„úä˛Bmòc«:sË«A4EG÷Ë@ã6paÀ*mq¨)⁄ã{LQ0áK´¢ÖÉX©ï0äm¶áËø$z£Äà¬:Fj<uc*·¯Ê°¿/ˇ	L‘ÕDπCqí8l·}D¡æ)E|À8Ëƒ	6ÜV®ù⁄I<Æ;†b<µXú‚àEHQ0Œ

ñ‡(hxµ•∞®≤L>≠¢n
v”lÎÿ«öy#‰R∑„ql<ñ‚ˇO≤)Í”#·oÃ≈Éo∑ë‰N}‹HÙQÁ“êxp∂»˜ıilHÛnX?¯(ûrÓu}GB÷R˛@$D>1$ôyFwsä+∫Nø_É€7¡[)P%Oƒn∫4kåT(ﬁ®éBr„:xùÎO›a§∫FæñÇ€'ﬁÚ|ëÅ$±g¡ÖYPŒ qBfŒ˜œ Ω9ÙVgôkÁº5€õÙÈP_óy÷	?¨ íÜƒµEﬁ◊§≤€≤?hàA·¡P3„M∂ÿ9…ÄÙm|	å∆¬[@…ÕU´/µ”ëò¯≤≥/›‹ÜHxG≤àIOqF4©âÅÁ≥ﬁ™¸Á1Ds{Xqÿß◊äåº±V∑;P“—ªvµπDBn#qËpÃ‡E5mÆû∏∞ÓiU¬A{§ÜﬁhD$G«-ñT§«‚Fim¯’Eæ[HJssÂ"í…Á$í¯ø^Õ®ß[{¡”ﬁ⁄©{<úÜãlÅ5µ)i¡‹=ı∂ÑÑBΩd†|Wm"YÚˇ¥ˆü}Ø"Âı}ŸUÿöb5tÆMj:‘z“p„`!ëùöPº7ábî…íñ%û©HﬂÙÆ∑Ô–ÂÀW˙Y¡ å˘>ÅB= Ó“-Û}ûÜºúˇJ$âœ˛á≤wÌÿ÷>vÊ`Î∏ Û:]øŒñ›\wÆ5‘±Á€‰Î•F0H6øV0Ø¢æs]`˚äµ•-Âù÷Ò’O∆Í£˝˝’¶È2#Œ«Æd≥˜:XÃöÃ§UM˘Àˆ%§ÍF üÒ /òì[‚[bôªÕÊÜ›Ì%.O√ºúÉ…“ƒÕ≈˛„;úΩê⁄°˘¢˛K!Úõs˘≥\ÅÊ⁄@4ÿQ4—ˆƒﬂ¬∫ﬂXÏÂbt"—¬›eıø”e•≥¶ÍIZ#◊Êm≈†Mƒ§à@z~ŸUNƒË,tºÿvº"√„—uO!wöz⁄ ‹kÌ©§¥M√—)-;–Vâd]¬Ê´)S+É‘]ñè£~ΩÚºaÈ-r˘÷ üªhé◊˝zàÈI$r∏¨æ3zâKœ·§;ÒJ_W£ò'	tÔlÿósG«7◊Ò‚‹””%íÁ©F⁄úç YY∂ª\…√3ñßd‘qDÚ€ﬁ©ı~√„(nk◊kM.∑UÃXóÓ∑a„Ã!ÿâº_XﬂYîâ≈ƒ‡ÖLºº˛ªë`HgIàŒB∫X;’ï*ãpÌ§&F Ã,˝A=üN¢^9:˛‹\Ûd åYŸâW8ï’ôìW$òpΩ˙”Ã:∫ˆ˘Æ.o‰”¨ì9ôÀòrŒ,Ñä}≥ßE¢+“ﬂNﬁ4„$_˙˛ˆÂ«Àvo∑^C≈˛Å{¨s‡&´ò7Äq~ıë7ºŒ≠Ô∆˜â;®õ˙™Uà¢\ÃXá?;Ú…¯^Å$∂«6éÏN˜ŸÉB≈¶Q$n»5)Ú©ë&Í˜6a1√±≠‰V˛ å 3pèÂyæhÏ‹iîæÓõ$ÀìHjãI˘ásP¸Ó∂*4a„¡krùohånµ©‹„ÏÜZ8"	∆ê0„h∑x˜CÚ†D‹ë\¸ ”5°á™ˇÕ%‚FRÛ∏êºƒA<|ía2™IêY0YU.E»¨J‡9XÑ±ˇ	¸ñ!∑z~Ø@D·5ÿÑ´:aºÖ¸Í0úÄ>…∞>Åœ·K∏ATº7ªâáx…x2ôV…≈çkyñÃ'?&‰RE^&kHyï‘#Û⁄B∂ëV≤ìÏ!oí?ë∑I9Bﬁ#ß…á‰,˘'˘7πHæ$◊àIÅ2j{‚¶dt>öL'“It:ù!y›|ZN+Èr∫äÆ°u4B7–-¥ô›n‰v{È~˙gzò°ßÈ⁄Oœ”A:§Pdw	JíPûT¶+Ÿíôï"7+W*ïe»Œjî®“®4!GkU:îﬂ+î.Â∏r
Y⁄e y⁄†rEπÜLC∂feÊdnœíòó˘Ÿx6Å}áMaÈl:ÀdYÏ,»Úÿ,îπ†‹√îG‚R
e‚>d	V‰NA~1©
ﬁÙ5∏KàÇ˘ÚVèex´Gæa≈Yƒ—éïÅ¸FÆ0ÃºD.ÂÇô¡07a»êD=aèK˛ xõP∞>ÅûÑèa~°J€¢óò®˝∏G€¢7Ÿ≤»aë¨ÕB√~•ä~
û√FäÎÛ?Ú…±‡.ﬁÎø;÷¥(ÿŸi_™ÜÒa©ÍmA´QM0Z1"å3‰q†*≤L>EôÜÎ◊rãK!´2o
ﬁ™ºéˆƒ∆ó˝p[¿ÃØ)äÀ≥L˙Wñ≤ïï≤.µ¯ÜOyC_Yıﬁ§»≥#=ˆÖ«∂Â÷Q3 !ux√GøŒ˝¯Ê£ZÆJø“oŒè2∆zRÍ7ç˜Ñz¸]«†<Ÿæx°HwÚG'Ü˙Æ¸„Á…?∫xÒWÊÁñç/ôu∞⁄ 6L@ÿ˚≠*~§÷êˇI1DÍCÚå‘¡qÍdVéÄ:¥£C¯»^é_?≥˝∫S•ﬂé‚e©HlßΩÒ?ÚÀ8äÍé„ø˜ﬁf˜Ç+∑	A6öÒép§EªîÙèdÉÅI"‘$@A˚o	ˇZ 8AE≠é»ŸPÜö¿¥‡ˆèÌ›À9LòfD;⁄ÍHÎ!`«j‘vzëx€ﬂ€Ω$ó`ßVÌtﬂª∑ˇﬁüﬂÔ˜y˚ˆæoŒ6‹)ëªVá[*õüöÿÚ˚iwÓøπB=⁄K–nD–ƒ@∞m7’Æ¿’†/Õπ≠ÕBõ3∞\uïØ"*∞‡$“¨1°uπ.óN˙p^Âéöéí¸ª√Qa˜xreÒ…:®kZpzﬁ4√ZU;˘™€ŒŒ◊Œ?òW≥O
_4CõrÇÚ≤éWø[RY}Cˇä∂ﬁot˘J≥‡çUM¡‘KòZêø†¡∏€nÃ=TT‹S6#0„°w≤˝ØÓ´ªägÁjº≥p€ı«Tvî˜£›…:oõ˜Ú/ç«°.˙D…‹‡©v^€ùª∂mF’a˜õøn˙äov õ…¸ÉÀÒ◊ß_™.9‹Qd≈6’8Æ©sõ/<‘ìãÇ°ô◊Ô∫Wq,∆«û¸ËΩ’·`$dnÃ’v‹µØ.óèü–ü:¸ﬁ—Éœ,®Ë(©vCÌº-Y›€˜˙Ò∫Êı%€œÇ|ºÚ$å∑*]ª3Øfèî£⁄R⁄ºg_ı°º™∫ÖKOn™-Íi•€´ƒrO›1¿y$¥ª aû->T‘¢ÕÁ"Ã	Óÿ˘Âªrwø‘%CÁÿoüÊ∆;–Â∆Î´©Ó56¬ˆ÷äíÌÊ©vàÜ˜T6VÖ¢°]”Ó|<îìw§i⁄Ωq˚Ú˝’˜U·‰:¥[”•h∞5˙œØ]Òr†ª~™§^–ÿ{ﬂ?0æ"⁄ñgF¬Ê∆<sŸ˝ù4ÍüÒ>|›1Öm˛Cﬁ¸6/|q˙IZæ’Ÿ‘ªµÕ∞~˜˙üù˛ÁÔ;ﬂÎáOæ˝Œö∑KO£ùïÑóã;∫oøß~Ì÷˝Áo⁄uÀíñ.(-ﬂLMÃoÓﬁƒÄiíDæµˆÈ“’Á∆ÌvÁ7U2Ö›ô8ø•P%ü∏£$ø8›[‰≠ﬂÏÙZ¸(ÖÑ´è†Bì¬–‰…,—#áv¡–:5âùn´âeVÈ	1MTì8WJG∑ –.ø∏∑Hsü»iªôÎ}0·ò§Q±ÒoOÿµ=ª´«6à:#Ωé¯∞›¡áeúåI‘>:c≠<?o}Ñ•Oê<%ï˘@-U≈0nB55JQOï°ûö’pÃÉÖ®ßæK`),áU∞ı‘∞6√èa;¥√œ‡	8 G‡<ãöÍÑ´®N√Yx5’˚êá0r…!W£™öà∫*Ln@e5ù‹J …lRMæÅÍj˘±»2≤ı’Ω‰A≤Å4ëf“B∂°¬⁄Iˆê.Ú9F~Ö
Î∑‰8â˘$9E˛@˛D^#oê‰-ÚÆPj4ã˙iΩıUΩûÙf˙Uj“
ZIÁ“:∫ê~õ.°KÈ›t-}êFhî∂“«ËœQg=Eè—ﬂ–Ì°ßËÈk4A/–ø—ø”K,ã]…∆°÷∫öPm›»¶£⁄ö≈™Y-õœæ≈ÍŸJ÷¿÷∞µ®π`≤ı®ºöÿ÷Ã~ÇÍ´ıW*∞N∂á=¡ûd]Ïj±£ÏÃœíÀî‘‡Ú±îAàˇÌY®|ÓYBçäwî∫Öë,ºï±áËÉ∫Çz˝ºæÉW¢N‰1ÓSØFvk≤\!	£
≈‚éÁYœrè"KﬁYË•åœíœkèv©∞˝Èïî–FË±.Ù ÕË˚¨¡B•!èîeöÕDΩüèfªæyYÃÓHT\"nöÊÜ+Aå=&m]‘#*gÜ∂≠…¢Ç'ìIå(1	}ÒéY¢¡†U¨cÆ∆√õ†C ß&∏¢ÕüÕ'‚ÛóÃ™&Æƒ’Ú$E·√˜e}¨EC%®p;‚w¡I~aX‘¿HGÇÜ$ﬁ˘
ﬁ!<
∂Ôs≈ÇäæÕ0Ç1ÇÜO∞ê4ﬁä,6˝YXˇu˛…ç°G±–TçÿiçˇKí†ÿ‡≤^pY;ΩA˝7…º¸ë7I˙«~û°éôÿAf∞†∆• o…B˝É•!%˝íjAé=”åtﬂƒPıÈtºƒ÷F˜Aò	îÆ#wèäS∂;˝ uYœ·§Àô;Ö¨,»`·wﬁX(ê…¢f~…)Xßpê›fìÏFªÅ∞d¡ÜGòùˆµ◊ñ≥}Z Âè9éﬂ°KﬁMJ
$JÃI˘Se®<‚	XÔwŒ$78I¯ëÁÑnÄ6ƒ¢–;1c»Ωu œ+ÕäV—hh7ÇIÃ{πÊ±» üf°úΩÄW0¿HÁeò3‡Å∆KçÌrÊÓ%[‚Çb∑¿+∑¿RèÖ,!ã¶A}03H÷¥™6ÓªD*Z¸»ô$S˙úé#Ì=c1∑5Yf`ı2ï∂%^`H∫a·/ﬁÓWßrN‚è8/XGd‘!˙–ªNºì¨πá1˛%ª/¬ñh¥4#†ÒXf˙¥ê{3LO∞†ïï«Q`í¡Ü—)ãKñe’W5Í#X§ø|v£˙Ty,˛qN2áY\ÑçARoË6h.ˆ)Sõ◊(ƒ»ë-Ì=„ÜaÛRô÷óÖ=ñÿSJ%C∑tÀàµ»jÁ,“ãw™ô,
F≥ê<ˇ¸q¨Ào«Âo$D£UÖxãaz¬A÷ÿ#õqtãlæ©ÉU4=<¯lümÿF˘,d°çf1gìÕdc^-˜XåªÙﬁe,∫-›N1—iŒÕ≠%™_%úØÑq˘ön¡b1î ∞D∑T¶=∆=ñäıÿ	™j1îmÇ◊Y$ı©çcï3}ë£›“y‘“∫a$6º•ñ*˝vú›6‚Ad¡T√]H÷µ`Æ∂-◊Û
”eÅ q$}›Òãíu∏
ΩF	Æ˜A `Œ«ü*;õ¬UôÕÛπû¡¬G,8≤òìÏmËŒ´±}.ã	Œ»b„ ãíÁLùÀ¡ralQ>7‘kõÍ≤–¡R
æNm¡béq’cá±ƒö2eêEøΩ1rgI_Ãç4÷¥nE\Q◊‹ªt>ˇ}£¨7L’ù∑‘T‡ís¶H’’Ì'Ld!Q√}y¨2H\JÙ•ßW|/0î æFW¶F∞8sQmËeJts•QCs"Õ/ÚÎ/6é£ ¯7ª€›u∫ˆÌ9¡å…5∑˘CT§HåII\)’≠”ÇdDûPH]'Qj/º`Deœ˘NóCr∏B¢y@ÙµEB‚±◊Ks≠d“ˆÕR˚p∆†îá*Uêã8n¯æ›=ﬂπq“6Ñƒ<xœwª≥≥øô˝æo\ú7«‡¯.Öé^ê-ç‘¶>áïÜ“ç-|o¡´UN‘éé.=ãµAã?≥|õ≠Ñ„†ﬂÒR~0èè/£p]ÄÅ_@ªlêÖXæ
€Y0ΩﬂÕ`\:ı:Œ“iÎÔtˆÄÖÒÉ+«ıÂC_≠ø;7˛ã,Zà±·â≈·ãìWNœ,ö¬Ã€W»¬¡á0Ò˝oÿMu≥m„ê…B˙-|HÚÓˆ2Yh—ä3πjÊﬂæ&Ñ0∂„Zt!dÀ∂¿ªK\J~0:^ØXx‘#Yx^≈]™L{FóDbëWu√/i=ã?iÜœnÀëy›ˇ¨è7[FqFøôyŒe¥D®ö„eì,fO≠†Öòﬁ[,M◊eàŒ9ò—ú3Ñ"∂0µŸ‚˛·U„Kg¯°âùûer>ÊL,fÙ"∆∂e]ò¬Yëha„¯%‹Ù€ÍÉéûOèÓ˚¯êÅçèsCô0§Æ©≈ÏE'˜Æ#¢Zÿcm<Õá%<—gÄtRÏØä T-pµm≥Ìy’öW7çyyï¨[ôÄ=Yó«;Ú*¨»ÿbA¢≈QÕ¯≤∫ πúb.à-ÇŸ\wÃåÕMKïd vPsPµn£EF9Àì´∞-vW»‚≈ïÜxÃ6a¶;r¬ãUq`r’9æó,|≤8`N\R¨Â¡ÍìA9H,
ú“•öÓ≤˛©nu=ä·4âæ•Bn¯—ïÇLÏZjÒÑ˛ºsÕÒã≥á’ÀÓx-<\$ZxòÚ»¬∞ê‘#≠€´•éKgFZL∏.Ù‹9>`1\ÜsÜπsqíBx‡É-D‡0Ω∑.xÆªK|±g—ﬂÅ#.YË¥Äû? æõZ»aì;h±òa-—⁄Z#±àNÄ`ÚÚIØærC°ÖOπØwi]òZ….g®n1µx
Lªôì±Ö«1Àl≤–ÄÕõ¬X«˙¬+dç,Pèdaxµmn-\◊t”u—:ñZ»ûÖ˘ΩÜ[ø¯TGEh‚å2ùâ∆*EuﬁÏ∫∏«–"çú_ã-º+…;RTÁd°A¿dœB_Êì¡yÁ¯≤Ô¥—"˚#ı:.{^„}’U»A≠RPm≤`%Ì!3£T4•({‚/R{,h>EzG¸è,îÍh±-®Ó Je,ÿÃƒB˜~:JÎ"#iºhßqºkÖàgJóıqåùåèKôX-Ø	˘∂ò‹Ø±jöG8oÇ_wΩ≥IÏêÖg6Y‡∫%ãØrs˙ãôã÷⁄…∫h®ÿ‚CÎbA3)èH∞“x°7•.õèà;Âfº»gaº +dÅ+Çyª˚Î‚ÚØv†S∞›GSã}rÅ-lXÿí9Í·ƒ%|8dÚ qºê ∏ò‹KI}¡yC˙ıa«{,…©°[êfﬂÇÛb0ÎﬂA≈≠-≤µ8vŒ∏5øÒ˛≠Œ¸õ™/ˆ√á‰∆—á»¢/\Ω2Ÿ‹U|sÍW0ZÏ@	¥ÃûNÈ»ªo·ı„EÍ>s*NôõX|ZEÉp-¯k≤ïZ8Kv)ZLU‡*’ùd!Í°¨[éóK-≤‡jò—6,tk–bRTπ~’ô®<Ã</òyy™Un%±ìrÍx∆íıïıv·“3Ω<rÙDúGv€U>kjLEIº˙Zõ#˛
÷Z◊0ãåUK~íG§£Zã,∂°Ö?Â‰ë“1ß≤Rƒ)ÛÓn·˛±˛
Tx3ák9∫8m†EHΩ&é%≤¿˝Zÿ"l…≤Hj–6–"górbå›∞¯ùxÀ;eCãoÃâØÒr’ô˘ŸN]g'ñüúh,kíZÀ¬QKáEyÎÈ^}Q:	q}Õq0ççZkæ]ó/^º6U≈ºä’≈v¥«˙B∆ı≈≤X@5¯ÄEøæ»≤£X∫„ÉüXlßÿYI-ΩÂ°Öh´]&^QÜsÊ“?ı…>~„"’ùx/ﬂ‰ÎpŒL,ú º ±ñúdoÊ;hëπﬁ∏°¬ gü-“ÿŸsù‚[bel⁄Êá_88Ãﬂ`eæ∂~∞ √ŸBÓ•¬‹|uu.©¡i≥Ç–Å`£Ó§`Iu'÷zq√M≠àûY˚ºıs⁄cÈ≥dKíÍN◊ù√4õÄ: ÉzıdøÓ‹t©ä}dëM,*d°⁄dqQ≈gù6v†√àﬁT7#=ë»?oQ∞Ç¶ä
™πéAºõ©∑•óo™uıj°mXıˆ\¶:ò”o°˛ÆÆ≤K™ç◊ﬂ©/e¨ıS6?h5¨‡–U&[a}}÷1íŸ5r$ﬂà
Òﬁ¨CYì…Ì>˙58÷uÖØkZ∏n∫¶æ=Ë∏]‹πÃ¿~D©éyI©ã˛~ƒÄ]F≈ÿbõ˚Å¨»˜bú6æß67	ü†•ÛGÒ"˘¿Ÿﬁ6¬‡H˝öÈ#ı/¿J‰ÖÂﬁ?l^∆÷wHO9∞˜Cø§◊æ‚Ê=lúW$ã!`QF±Ö∂ih∏ëx 7=ˆ-t˙ œ›Õ¬øO∏”bã∂•E⁄Ö•¢=^!≤#ÃõR≤SÂ‹¶°πrì+¿4˜lÒÉS∞∞ÂΩ,¬êı-∆˚˙ÑXJ„_’›ÙÂ\^¬ù-˘.Cûã[9±Ö‘7mhÎ|‹¶GÈáæÖôë˜∞»»03¥‘˚ot∞Øè¥ÿø£Ù”£ª;±–»bÈã-◊‘«oΩmx°$ãÓ~∑xÅ…oƒÍ]‘Ûªgìür˜;J?=~Ñ≈∞X<HìﬁÖó]Ôrlaê≈Âˇ¶≈ˇNÛ“£Î}ãˇ”¢◊\/yGÄ,‰ø”B«–˜nî√	8	_áo¡xä8Ê∞ñ}~øÑó‡7{h¬€¸Æ√_·oå±aˆ)ñgÇ=¡¶Ÿ7Ÿyˆ},àÃñŸØŸoŸklïΩ«niñ6¶Ì—˛E{µ@Ey\·ô;Û?@`waQCˇÂaê,âb5E”≤‡b_+ÇŸ5	b]àíxö6Z$æñPX´Â°iTlç1ˆÄÒt„âtm⁄˙à∂>éäZMåâœ§ÒX[‘x¨Hp{ÁgQ√©∆sLˇªwvÊﬁ;˜õ;Ûœ?wûÑg`‰É~Ü∑◊ZX	o¡Fÿ
;· úÇ/‡*t0Œ¨/Kd)l(Àd„òãπŸOÿ|∂ò’±µl#€¬v≤Vˆ	ª»Æ≤ˇ∞v‡°ﬂı·Ò<ë?≈”¯H>ûø¿_‚≥πáø¡õ¯æá‚«˘y˛%ø oK·RoIì¨“”“HiºT$Mì •%RÉÙñ¥E˙P:&]êÆÀ\6À	röú);‰©ÚÀrôºD~Cﬁ(∑»˚‰ìÚ?‰ãÚMÖ)  Â%Gô™º¢ÃQÊ)Kï5 ª  .Lk˜+Gïc )ÂúÚÖÚOÂ≤“©2UUç™IÌ´jx√KV©É’t5C©Ê 9–„Èñ‡È¸^`Uj"a4úôiÙ¢*UA¥TÇLäó‘"ã≤ãpU.àˆ¢·ƒDçƒD`§ë4Y%ÙhÑﬁXè§Q∫‘ dhEç:´<Ìå¬+jÑß0›W(5 ~ÑÓK]#»@BIz0°ù	¬pÑ√Q>LÑ~® #ÇÑ˚ "Y§Œ˝§ƒG¶à°˙8vX˜∏ FB=1r3 Õ<£ä¢f=bS7íﬁ∫ﬁ`&·R8çD<ÈGc0R¡a8„8G$
ÌCù$åQBBÆæFúµ%Ñ¢N®”Íà@†VÏ„ 		‹‹ b›~É≥%æC˝Û†>@7®®ïƒ'Êhˇ7`Ò⁄˘Ví¯¡oG1‹◊πÌ>(z≤≠È([‹≠„2æ{U‹ ÍæPÙøãb{›vÔ∞Œâ“: ëÎ7ºßpUE-±ÆZzÍ§á¨x∂eBCÁÕ5uø±}ﬁˆéàØ{v/a Á—Ô+¥b%¸¸M=+K!„≥mLG)[∂Å(q\Ç·Ùºøg
ı«PWﬁ∂(Õ}$)ﬁ}`ﬂ.‰≠.'C6Nâ/…ôëïoﬂñ\ók<π"Ø™§˘ßﬁ&€h¬+b=1ô6∂˛yì£ï•…¶›'ÖÛ‹‹dœ/˙T´dR¡¿∆ãü}ØºmﬁiÛ¥™ŸYÑ$jé¯˙≈—“‹'éîm^ìÀIZÂ&ˇÍ“Î∂∑´7VéMn œ€Í®¨Éıˆ—dVÒ;%˚[r∆dúºT∫ÙDtä:æ˝PG~◊&\âzeÓ Ñ3Âñπ¶;*„7où=Ö‘-\Ù‹µÊñ„G“äßÆåwÔØ)kÛ_û;úW=ﬂ˝Œn¯”˘—
‘˙K…5ﬂœ´™≠¨≥Ï∆XV4øXê‚œ±^*…)}wr‚GjÍ§@†µVï∂¯‹¿Ôˆ™êÚÀóo!JÌí€YõﬂÁπùπGƒÂñ÷ê4˜ôÉ"ñ®ÛØ¸5ì‰’Á'f]ßëÊmj}éΩ∆úWïXÈ≠ÆﬁÅ(ı≈9)Ô7ôõuîhwÏﬂeÅ¿æ€gmŸ√0C‡ì}„ ^(ï…ñJnó1ñØÍMà¢œX]a√L„¡ŒöU}õ‘Ì€ˇhX€æ∂ÌL√Û˛|Ë˘ˆ√GíY¶55I«‚n´\1)}%múk¨¥kUûyÂf<ª˚MH6¥ÒÀım∏.g⁄„Êrªæ.ñ»Ú≥	∏.8c˙õ6„ˇ~Ò5ÃæmÑ'SUÛ®]WmhÇÎrg„bÓ·(‚¨˛&ãfFäª5)q∆—ìz4›‹ÿ•;Øê±wQòi9πõ‡›E—Á„uÂﬁmÜøî˘æ§ÃË∞ëG~£ÏµíˆÍáè¢{√E/¸EìXá°∆ì$‹µV2à∆ÔÒ”d8˘…"£H.OÚ0ﬂöBû'/‚Tπ…K§3Ø9‰u≤àTí≤sØ’d-yõ4ëﬂìÃ¿vëΩ‰9JN`vñ\ …ø»u“N:(° Œj<#¢i_G„iM°O“!tA3®ùÊ–qt"ùLß–B:ùñ–R˙*ùCÀÈBÃ⁄™ËbZCó}TOMW—F∫ñÆ«<pıa.¯G∫ãÓßáÈGÙSzÜ~éy·ezïﬁ†ÌÙk† ·ôfàÜ«0SLÜT√!Ïê0gtA!Lá0s«2XÄ˘c5,á∞÷ÈY‰&Åv¿Ã%è¬q¯N√Áp	⁄‡|7†nA'esÃPéyf$ãb}X?À‚òÖ≈≥$ˆ8»û@¨gÜ–-Åá ß aòÇQI"K%¢≈@bL/%ËA,hÉÃı~r1QS∞è¨ÀT¿‰Q˜*z	¥ê±ø™€	r–/∂ÿ…ΩƒÉ>§`/–%„„>#·ó)]§g6≤Œ<àç∏¨ª&b·"¬Æh¢˜ÍÍ+É∫"V–VâKA©/ÈÃë∫fI÷ÎAb:sÆØ	H≤$z·!÷GîB«8Ó_)∞)≤ƒ≈l’å>H q˚lù⁄^ó%’⁄£©ÕGærÕ8ú<Vr˘§˛>ñ§˙xR¬π˚)œ•Z«8úöü*Ÿˆ†€Ï";
'9±*Z(Fy∂=’OÎ?ëŒ˜(]ÓÚ”@ïüÿ€Üü6µ’™U”≤g⁄}¥!V§X∞j’F!‘®<gÇKÛjﬁ∑W•ÕòÊ∆ÅÈˇ®(ˆ∫û¬!NrŒƒ2ﬂiÒŸ\±w™≈.◊pÙ”K¯·∫Ø=î=îÍ–A'ÖY«h>6¿·úËÙyÏ±>õ›k±hŸæßoá=÷‚r°U¯ùë‚ˇÇô1¡1G‡ò√S∞bËÚÇs`ãıó◊€’J∞¯<^o¨„∂˝dG%=∂†¿Otè,)€O=]ÂI∞ƒ
AÇ%¡Ç„tâI6äπœ∆ëZ\©‚Ëë;Ò‘ò|ªπ„Sµ¢+SºÁŸÀíΩvèY¸ÉfÚ'Å[r3©ê”I∂ëã∏õLAΩπ“◊Åº˘á¡˙‰'êÛë”ëÛê«B:Yá\é˝Öß¯gÀ»Ùô/±˛ÀvıÑ∏çùqy<vB=Ñ=¥l”JHS,…ñg|ãdÎèe[=IÔIè•Tñ,Àñ‰?c{¸áÀzË!ÙPöñJ)K•Ïaz*•{h√ñBYz^B/%á%ÑP
”œìYXñ~˛ûﬁ˚˛ﬂ”{zûˇî˘UÓóÃØ˜4_a∂˘òÄÔù¬c&€€Ωís1Û/òwa^/>ª!ÛùÓB+ﬁa"ò˚«axÒË˚7Ãﬂ:|∆î‡¨¸Á^dä9»@1Û+]Ÿ%}öªyÌ≥ÿc?æéôﬂÉû#–˜ã=
Á®pŒ¸§‰‚e3‹·SF8¸åykè¬¶Tx˘¯SáÒ€¿wÛ⁄M†OòªÖ∑ò9P: á¿üò sª#ÊÕ‚õåkﬂ,æ√¿óè~¸8Ø3‡£∏6nñ Áøa¡⁄◊˜1¬3ìˇúπc	p<Z˛>cÁÔ_¸˝—≈+XØ¿¸ÖÛãW‡á≤ØQÓ?Ã}–˚”bëY n>ºÒ}|»>˘7¿ø;{Ïs±è˚´ ;Bnö{Ï«êüÛBì—Ø÷~x‰ˇïÎ@=Nˆ˘*>Ã›Õﬂ`˛|Ö˜ˆ ~lË–pëfÑ= ˆÖ«/A«=%x\·Œ◊ûÂﬁŒø»˝ÚB_”ÀqÙ=|	µ~zΩÎÃ≈ªWr`3˜_¿'PÎ˛◊nÂ†⁄%tÊ#»œ˜!˜˚ﬁπ±ú^\|ß≈ÁóΩˇwÃˆj«|ÌÚøzπ /,¯b›ﬂè^ØÂaÓ[Ã_or\›û˛ﬂÔ‡RmÓéããø|ôÊæ$˜›=∑?õ•Ø«≈ˆGiÙ˝◊ûΩÊ˚∆Ûï’Ét`˜vãMœI6òÒ8 ñL(F]¯Sd€|6…¶•F\ã€˝J:Ú+Îik7uΩI¨©ı°¢›e§*˝ ÃäπÅŒÀvÎ±MÜ(°g{8ˆeóâ`±Ñ5éıı∞*áÒËt÷:Îù4=c$ù"÷–+ùÜ∑ xaek·	ô–q9>Pî&+…ú†r8]úÍ€f≈rc∞ã@dAπ(Ü√D?n°Üºälm{*XÚÜs1wñ…Çã|Ñ’≠9jdÚzqi›c[uÿZˆw›â$Sù„∆iäDCú&MªiÀc<vUâçnç_ùôzãhxæVuƒ.MNU§&!æŒìñã∂cÌÜ$üx¡%≤q∂èÂ8ÎhÊ∫X“pbÙFGÙ—÷™)ä2Ã‡Y&5-SÀÕE”¿Òâœè∆có£dvÍk°Î*∏ùrõÂÒŸÇ◊=G˚ù„3S÷h¢#ﬁ”Í¶h∞enÈ	‘È’I[ö∆Ívâù∫4®q'h¢ªÛYÇùôÍk1ÍOhí…À%\/ÏN6ÎaΩ≥”\π/åÕæQu¶ôÂ±Lì8[89ZΩèf(	ñA_,6É$RaŸn4Õ≠∫Y∏>ÛTÖ∫rúËsÀÁ—Á∏S"î_´Õh%Óÿ¸ÜÎÀàƒjª·•1Õt,9ï¢#—W%2][‚FqΩN≥Q=ì”‘îe5H}+ï)µ'hÓ∂ó5Gè=éN∆ﬁlçù$°X≈Íz±ë∞`cgﬁ÷õˆ∆Aã´§'é≥ç|¢7BRﬂ2Ò“ƒhQUŸÜ©POïpk„-&z}‰ë.ﬂ[t˘é]≈≠ùãK÷∞◊aqe1ËLœ∞Lßî≠û#q&ã%œü”M[Ñ’€~”L·ÙTﬁïÓÆ:„≠j∂$®kπ‘µ‘Õ0#4PUŒ«∆±´£™Jã∏∑}Í≠Z„V&µó°ôp£hó»kìe;8>äÿÂ ·ó£Äîp:´fÇÇI’I∫Ò=ﬁP|˙	´4Ëx∏í∑›•ΩìX\4piU‘%&‡µÃπÊÊ^•Ω
T:ÓªíA∞ﬁåÑGD≠a..ªr¨ª°ÈÆ◊ØòN`Ï¶Ì»«(‡çñ√JıLñJ®Ú§%≈)Êd§ˆxhFÀq‚û=è;}¢Zâÿt5ì5<~È“†f4´¨∏U¶ëf(∞+PÛNò)ìöÎAùÈq›!3’Çf…*\9íÖE$WNFlÖ8Í†Üírc¥^⁄JØ?u-áÆ∆∞'†˛(®†°À*M´PKëZÁCv∂≈jùı¬±ØICœüÃVâWä}ZóÏÆdV◊ñ˘ÌêÜÿ÷-sÆn‹@N;‹ì˙Iè"=tØ&TëÖk#í,,´ƒ]xmâÂ•Tj„JW–≈âÕ#‘á∫6å3¨ ëéfV)oli¯≠E´⁄ËRÍ–
ÆÏê≥#I∑RÀ¬l÷OSI75ÅCNZ≥™ÀIò–Zí	û∑¶nHË¨ü®ÿ±‘ÍîóÂ‘‘È* ÿoŸ};ÓñO\∫ÏIy=÷öâ›—ô⁄2ª∂Ç™ú∂=Sç¿Dkhã#gçX
]"tâ¶¨ê\r©}ê’$S6™1ÑHï:≈ŸFáw”8öéS¬›prúÿRÖUÖÒtáÜ¶©6ÜnÑç5J,Î∆à(¥4ÁfÂÅC Î«ã£8Ú\ÍEwÃƒ(À¶€UG±ÖTÅ„†ØÔ›{¸ÌáØ>˛˘~p˝”ø›{˛‡gœáˇcøÃÉ¢∫≤0~Ø7hÑÓf”VPêM[Y4≠D√∏`BF¿uT4¢åuà q!Ó„Ü‘å¡Ãh@&J$#öå±&Óqåª3A»π_wY´2§¨TM’ÙÂ˝ﬁwœ]ﬂπ+˜Í˜…?XØM∂üˇˆÕEG%ÏˆcÊÚ ˚°Á„Á_Y£hã/˚æ∞•≤∏)˘JıÿÿA>LT4M;üÊ:ËåN*πÔs¨•æ9´<ZS”ì√Ô◊∆LZvx˚hØ$”÷8uCB˙∫…•À*6Ö¬Î6’∂Œûêû0°jŒg©KÒ8π˛∆≈Àã|Ã°©ÁÆÍŒ÷Ãã.‹{ˆNp˝{}¸™úJrÊT‹+©_∞≥’≥ıãC∂ˆ/∫4Ø‚vnsjòÉ◊±Uµ,ù⁄3◊wºﬂ†#Gcœ|Ω\˙ƒn_⁄∂ì9ß˚Ø™nJr]vq˜¨iJﬂ/úÈ}€∏|Ll‚≠yﬂ∏]N*iŸb∑·lıô©ŸYU Ä≥{¢wù:ù2π°‡FôÌ√‹òW¸z˛€•|õ€ÖC[≥<ÛüÆŒœûñ|jnÈ±Ï‡F]˙„Øº"‡xŸÅÍ5ÂQŸºoˆm	∏‡≠∏˘déyÌÌ∆Ì+6§EΩ€π∆º+Á·bõÔä˙Ù/™∏38ˇıÜèm\.Uù∂£_–£–éuW{≠}Xú:Sıe˛Õ+«U≈â!â-◊√õwv<úQz·≠1ä–πõ{.ŸΩb¥œ∆'ïè£6g,÷«P˜Ø ™;^9e«’{jbíø˝<eí‹◊±serJi”‘Ó©E;Œ|z∑cƒ‚∫s˘ó4%j+Àó’ÍÕOQ§^ZÀˇ∆ô§JRKédkíj$=%–P2SÍƒWGSçd†‘@;h≈X-N4·Ö<ŒAÅ‰MF“Y-.˙3’HZ.(* ~^ë=Ìß54ã´Óiµt†’4Éi¿sãΩ˜Ç≈ë∆ø–ºéF¸ÃBïí+_ﬂLíùÂ?∫®võIIm$K
PÊn»í¥í µÉm‡3∞¸¸¯|>Å¡‡}¯xlõ¡&Æú∑°o—M˙#)˘-bBÀÙ=Ù:xº
^/ÉﬂÅˇ/ÇﬂÇ¿ÇﬂÄ_ÉÁÈ}rÁ÷ŒSwé}€W¨t∫¡Ja˘˙4Xû¢ìÃœ°Î¿ì‡	3Ù˝–µ‡q(X´¡C`XI)é˚Uiâµ7p˚¬"S N© Wƒr8Â ïÛ’]âô…∂r\ÍÀ(Ñ˘)- 9™ùß⁄vûìJ~èaÎ~‰€áJ—◊*Êï¸∂b⁄Kô6-Ìµ÷ Rd¶®·äeΩeä¿è¡›®ÛØ‘è˘Ù.ÿw¢ı∞ZÎ+§Åàâ˙ÑE¶m(∑ï¢ò[xLd˙•7£µM–¡1èiΩ¬\À:p-jXÉ‘øÄ´—Ê*§Æ‰ô,”
‰Yé‘e‡RåÈ»ì.·WòÌ?pﬂ√≤\hù;ya –2Â±ß≈˜‰!G∆mæ'ó˝wëSrëíKG8%áÊs´J§»L··˘ºÿ#ÿ&Rd¶à∞à’*J˛úGsëkû%ÜvÊ"◊Xﬁ≥¿L|ıÏ(À∫Œ‡Õ&ÜKfXb(),2Õ§tå˛L§§Ûñ#F?£üéæÕ†w»óm30˙3–7aë)eR¡©‡dp8é«Ä£i÷˙hƒF°ü#°ì¡$0¸ò ∆c§F@ø	∆ÅoÄØÉ√¡XpH˚%ÓÈ`‘ˇ;X^√äFÅë¯öÅ–¿W¡∞?ÿ4siôÁú–}¡00{≥M¶`Ë 0 {—¶	∫'ÿÙ'?nC…oÛ√ıÖˆΩ…ëŸù:3Ωx∆À‰	{7ˆëL]ëﬂwË.`Á(À<0Çù–vG‰r]A+uÏg“sãJ~õÿõJ~ã÷4V£–2óiEYG¨.X:Äˆ†¥mA®¶p¶äwWôpFŒ(íA¨-‘,°ˇ)¸ı)lÕ·⁄WÒ≥ùü~é£&Iæpπîa˙M~Ù€4Û2Ó‚∏%v%±SU<è∞”ﬂ@ú‡h;v¥ÇK*Jv®öÌ“Ëµ<h∂]µún«v-5;‹ûÛ∫·ËÈ»AÕì´ß…h–Ú0uf›Ö∫xhy∏‹Y{ê◊”ï∫r=›8hy˙zrN/Zû÷›9ß7%O}∂¯sPÚ¬È¡:àÒf∆ÅxIÜ≥é¿f8Ñ´ñ¶Û=àx[záıüxì"öÕAÀõ°ÿŒ∂Û÷®ÂÉ¶êÌ5|i˘€≈◊Â@|XcÀ	>∫©ùœî∂ıä) ]ò§në∂
RI•çÇÇÍ/÷˜&]c}c}∞ìﬁSÔ„©˜ú¢†÷πsÎı∂ıjáß˜g©z¸ºæ-Aë%pœì"©∑ ®P;⁄vìuˆÚnùª[Fà∫¿?,–´∑O7/£ﬁŸ—®Q9:´zi®óOÔ0]cmScSc≠ﬁ`~˛gÊvkØÖ<å®m
—ªônÊ`…Ÿ54ƒÆ’—ÁU)‘CrqVu˜ÚısêU.Œã!<¨OxògïÃ√27U˚˙õLí4!yj™¥Â›Qôn6yÅ˛A=4™Cíy’$ihõ§}ñ2Üˇ˜Î¢q2Ã ..-ﬂìñÍ‰ó€v‘›?p–pC›àaí}ÑÙ”„¯ÚC¡ˇ√ØÑYlY€Œ<ãYIF~TÙø˘3Ωƒ∫ºˇˆ:·ü2¨xf{¶’_ñ}Úøˇ44Â˘ŒŸÅ»™¨;XµäïQx^aÀ#Ôkm√{fîUãÉ9Œ™¨ﬂ≤jÎº◊‚Üƒè6≈œô>q∆¥_é≠&é7¡Xæ˚D≥õ‚˘.8ù&Ú8çÔJì˘^6õ’æÌ˝ræ_ìÇØ<…WrÀå«t$_	H}SÛÄ”$§ÛcÙ-1èwåx$È5»X¯≈íÒ>óRÛ¨mœ≥5GT[·-´Ô` GŸˇ“
endstream
endobj
4 0 obj
<<
/Type /Font
/Subtype /TrueType
/FirstChar 32
/LastChar 121
/Widths [250 0 555 0 0 0 0 0 0 0 0 0 0 333 250 278 
500 500 500 500 500 500 500 500 500 500 0 0 0 0 0 500 
0 722 667 722 722 667 611 778 778 389 0 0 667 944 722 778 
611 778 722 556 667 722 0 1000 0 0 0 0 0 0 0 0 
0 500 556 444 556 444 333 500 556 278 0 556 278 833 556 500 
556 0 444 389 333 556 500 722 500 500 ]
/Encoding /WinAnsiEncoding
/BaseFont /Times-Bold
/FontDescriptor 67 0 R
>>
endobj
5 0 obj
<<
/Type /Font
/Subtype /TrueType
/FirstChar 32
/LastChar 146
/Widths [250 0 408 0 0 833 0 0 333 333 500 564 250 333 250 278 
500 500 500 500 500 500 500 500 500 500 278 278 0 564 0 444 
0 722 667 667 722 611 556 722 722 333 389 722 611 889 722 722 
556 722 667 556 611 722 722 944 722 722 0 333 0 333 0 0 
0 444 500 444 500 444 333 500 500 278 278 500 278 778 500 500 
500 500 333 389 278 500 500 722 500 500 444 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 333 ]
/Encoding /WinAnsiEncoding
/BaseFont /Times-Roman
/FontDescriptor 68 0 R
>>
endobj
11 0 obj
<<
/Type /Font
/Subtype /TrueType
/FirstChar 32
/LastChar 121
/Widths [250 0 420 0 0 0 0 0 0 0 0 0 250 333 250 0 
500 500 500 500 0 500 0 0 500 500 333 0 0 0 0 0 
0 611 611 667 722 611 0 0 0 333 0 0 556 833 667 0 
611 0 611 500 556 0 0 833 0 0 0 389 0 389 0 0 
0 500 500 444 500 444 278 500 500 278 278 444 278 722 500 500 
500 0 389 389 278 500 444 667 444 444 ]
/Encoding /WinAnsiEncoding
/BaseFont /Times-Italic
/FontDescriptor 69 0 R
>>
endobj
30 0 obj
<<
/Type /Font
/Subtype /Type0
/BaseFont /DOHJPA+Symbol
/Encoding /Identity-H
/DescendantFonts [ 74 0 R ]
/ToUnicode 75 0 R
>>
endobj
74 0 obj
<<
/Type /Font
/Subtype /CIDFontType2
/BaseFont /DOHJPA+Symbol
/FontDescriptor 70 0 R
/CIDSystemInfo<<
/Registry (Adobe)
/Ordering (Identity)
/Supplement 0
>>
/DW 1000
/W [
44 [494]
54 [611]
60 [548]
95 [630]
161 [438]
]
>>
endobj
45 0 obj
<<
/Type /Font
/Subtype /TrueType
/FirstChar 181
/LastChar 181
/Widths [576 ]
/Encoding /WinAnsiEncoding
/BaseFont /DOHKLB+Symbol
/FontDescriptor 72 0 R
>>
endobj
75 0 obj
<<
/Filter /FlateDecode
/Length 245
>>
stream
HâTêªRƒ Ü{û‚îÓXê∞fm2ÆMäU«D{N"3BäºΩ\‚:¿|ÁÁ?7zÓû;£–7oeèFmî«≈Æ^"\q“jJÀ∞SæÂ,–hÓ∑%‡‹ô—B€˙≈%¯ÓÜ·Òæ: }ı
Ω6Så<∞èœÈWÁæqF†ŒA·HË˘"‹ãòh6˛áÕ!∞Ãı^€*\úêËÖô⁄™bíßG‘–®ˇ:iäÎ: /·…Ôo∆*∆OÖNÖäV≠Sﬁ„Sù)•O‘p´Ï˘RΩ¥ã[ˇrı>éññH≠kÉ∑ù:ÎRóÈê K†vl
endstream
endobj
1 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 3 0 R
/Contents 2 0 R
>>
endobj
8 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 10 0 R
/Contents 9 0 R
>>
endobj
12 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 14 0 R
/Contents 13 0 R
>>
endobj
15 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 17 0 R
/Contents 16 0 R
>>
endobj
18 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 20 0 R
/Contents 19 0 R
>>
endobj
21 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 23 0 R
/Contents 22 0 R
>>
endobj
24 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 26 0 R
/Contents 25 0 R
>>
endobj
27 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 29 0 R
/Contents 28 0 R
>>
endobj
31 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 33 0 R
/Contents 32 0 R
>>
endobj
34 0 obj
<<
/Type /Page
/Parent 7 0 R
/Resources 36 0 R
/Contents 35 0 R
>>
endobj
37 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 39 0 R
/Contents 38 0 R
>>
endobj
42 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 44 0 R
/Contents 43 0 R
>>
endobj
46 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 48 0 R
/Contents 47 0 R
>>
endobj
49 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 51 0 R
/Contents 50 0 R
>>
endobj
52 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 54 0 R
/Contents 53 0 R
>>
endobj
55 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 57 0 R
/Contents 56 0 R
>>
endobj
58 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 60 0 R
/Contents 59 0 R
>>
endobj
61 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 63 0 R
/Contents 62 0 R
>>
endobj
64 0 obj
<<
/Type /Page
/Parent 41 0 R
/Resources 66 0 R
/Contents 65 0 R
>>
endobj
76 0 obj
<<
/S /D
>>
endobj
77 0 obj
<<
/Nums [0 76 0 R ]
>>
endobj
7 0 obj
<<
/Type /Pages
/Kids [1 0 R 8 0 R 12 0 R 15 0 R 18 0 R 21 0 R 24 0 R 27 0 R 31 0 R 34 0 R]
/Count 10
/Parent 40 0 R
>>
endobj
41 0 obj
<<
/Type /Pages
/Kids [37 0 R 42 0 R 46 0 R 49 0 R 52 0 R 55 0 R 58 0 R 61 0 R 64 0 R]
/Count 9
/Parent 40 0 R
>>
endobj
40 0 obj
<<
/Type /Pages
/Kids [7 0 R 41 0 R ]
/Count 19
/MediaBox [0 0 612 792]
>>
endobj
78 0 obj
<<
/CreationDate (D:20210131150047+08'00')
/ModDate (D:20210131150047+08'00')
/Producer (PSNormalizer.framework)
>>
endobj
79 0 obj
<<
/Type /Catalog
/Pages 40 0 R
/PageLabels 77 0 R
>>
endobj
xref
0 80
0000000000 65535 f 
0000146132 00000 n 
0000000016 00000 n 
0000002330 00000 n 
0000143776 00000 n 
0000144243 00000 n 
0000101585 00000 n 
0000147781 00000 n 
0000146212 00000 n 
0000002436 00000 n 
0000007141 00000 n 
0000144802 00000 n 
0000146293 00000 n 
0000007260 00000 n 
0000012478 00000 n 
0000146376 00000 n 
0000012585 00000 n 
0000018299 00000 n 
0000146459 00000 n 
0000018418 00000 n 
0000024280 00000 n 
0000146542 00000 n 
0000024399 00000 n 
0000029962 00000 n 
0000146625 00000 n 
0000030081 00000 n 
0000033143 00000 n 
0000146708 00000 n 
0000033250 00000 n 
0000041937 00000 n 
0000145259 00000 n 
0000146791 00000 n 
0000042068 00000 n 
0000049576 00000 n 
0000146874 00000 n 
0000049707 00000 n 
0000056592 00000 n 
0000146957 00000 n 
0000056723 00000 n 
0000063937 00000 n 
0000148046 00000 n 
0000147916 00000 n 
0000147041 00000 n 
0000064068 00000 n 
0000070809 00000 n 
0000145642 00000 n 
0000147125 00000 n 
0000070941 00000 n 
0000077746 00000 n 
0000147209 00000 n 
0000077889 00000 n 
0000084138 00000 n 
0000147293 00000 n 
0000084281 00000 n 
0000090822 00000 n 
0000147377 00000 n 
0000090965 00000 n 
0000093882 00000 n 
0000147461 00000 n 
0000093989 00000 n 
0000098032 00000 n 
0000147545 00000 n 
0000098151 00000 n 
0000100118 00000 n 
0000147629 00000 n 
0000100226 00000 n 
0000101478 00000 n 
0000101725 00000 n 
0000101927 00000 n 
0000102124 00000 n 
0000102324 00000 n 
0000102514 00000 n 
0000123303 00000 n 
0000123494 00000 n 
0000145402 00000 n 
0000145814 00000 n 
0000147713 00000 n 
0000147741 00000 n 
0000148137 00000 n 
0000148269 00000 n 
trailer
<<
/Size 80
/Root 79 0 R
/Info 78 0 R
/ID [<7096a3c229b4c9cba12d307324541e3c><7096a3c229b4c9cba12d307324541e3c>]
>>
startxref
148339
%%EOF
